{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "published-superior",
   "metadata": {},
   "source": [
    "## Using sklearn for basic data transformation, cross-validation\n",
    "\n",
    "Many common machine learning operations and procedures are already encoded in various `sklearn` libraries.  Here, we see one way of handling a couple basic tasksâ€”data transformation and cross-validation testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(suppress=True, precision=2)\n",
    "plt.style.use('seaborn') # pretty matplotlib plots\n",
    "sns.set(font_scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-tribune",
   "metadata": {},
   "source": [
    "### A built-in data set\n",
    "\n",
    "There are a number of existing data-sets in `sklearn`, many drawn from real-world data-sources. Here, we use the Wisconsin breast cancer set, which allows high-accuracy classification with pretty simple models:  \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\n",
    "\n",
    "Here, the data is loaded into a dataframe; in basic form, it consists of 569 data-points, each characterized by 30 real-valued features in a number of different units. For more information about the data, see the UCI Machine Learning Repository:\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_breast_cancer(as_frame = True)\n",
    "frame = dataset.frame\n",
    "X = frame.iloc[:,:-1]\n",
    "y = frame.iloc[:,-1]\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-burns",
   "metadata": {},
   "source": [
    "### A basic perceptron model: one test\n",
    "\n",
    "A simple perceptron model will often do an OK job on this data.  Performance can vary quite a bit, however, depending upon the exact test/train split we get, which by default is randomized across runs.  Performance can also be hampered somewhat by the fact that the original data is not scaled, and displays different orders of magnitude.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------------\\nClassify with base data, 1 split\\n-----------------\")\n",
    "\n",
    "acc_train = 0\n",
    "acc_test = 0\n",
    "print(\"Train accuracy: \", acc_train)\n",
    "print(\"Test accuracy: \", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-norfolk",
   "metadata": {},
   "source": [
    "### Scaling data features\n",
    "\n",
    "We can use the exact same test/train split, but scale all our features to the $[0,1]$ range, independently (i.e., each is scaled according to its own maximum/minimum values).  This tends to give significantly better performance, since coefficient-weights on large-magnitude features are less likely to exert undue influence in the solution process.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.minmax_scale.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------------\\nClassify with scaled data, 1 split\\n-----------------\")\n",
    "\n",
    "acc_train = 0\n",
    "acc_test = 0\n",
    "print(\"Train accuracy: \", acc_train)\n",
    "print(\"Test accuracy: \", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-lloyd",
   "metadata": {},
   "source": [
    "### Cross-validation testing\n",
    "\n",
    "Rather than a single randomized test/train split, we can automate the process somewhat by using $k$-fold cross validation techniques.  Like most things, there are a number of ways of handling this; this is one that is pretty basic, using a `KFold` object to generate splits of our data automatically.  Here, we do this for our basic, non-scaled data, using 5 folds.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------------\\nClassify with base data, 5 folds\\n-----------------\")\n",
    "    \n",
    "print(\"\\nAverage train accuracy: \", 0)\n",
    "print(\"Average test accuracy: \", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-premiere",
   "metadata": {},
   "source": [
    "### Combining data transformation and cross-validation\n",
    "\n",
    "We can also do our $k$-fold validation of data after scaling each feature to $[0,1]$.  This gives us the best accuracy and most robust expected performance.\n",
    "\n",
    "This code is almost identical to that above, but differs in how we access data.\n",
    "    In the above, the `X.iloc[index,:]` notation is used, because the data is still in\n",
    "    data-frame form from pandas.  Here, we index more directly as `X[index,:]`, because\n",
    "    we have run `minmax_scale()` on it, which converts it from pandas frame to more basic\n",
    "    array-based structure.\n",
    "    \n",
    "**NB**: the `KFold.split()` function can handle data in either pandas data-frame or \n",
    "    basic array-based format, and several more.  In general, most of sklearn is pretty good at handling all\n",
    "    manner of basic linear data.  See the entry for 'array-like' at:\n",
    "    \n",
    "    https://scikit-learn.org/stable/glossary.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-outreach",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------------\\nClassify with scaled data, 5 folds\\n-----------------\")\n",
    "\n",
    "\"\"\" \n",
    "\"\"\"\n",
    "\n",
    "X_scaled = minmax_scale(X)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for train_idx, test_idx in kfold.split(X_scaled):\n",
    "    X_train, X_test = X_scaled[train_idx,:], X_scaled[test_idx,:]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "\n",
    "    acc_train = accuracy_score(pred_train, y_train)\n",
    "    acc_test = accuracy_score(pred_test, y_test)\n",
    "    print(\"Train accuracy: \", acc_train)\n",
    "    print(\"Test accuracy: \", acc_test)\n",
    "    \n",
    "    train_scores.append(acc_train)\n",
    "    test_scores.append(acc_test)\n",
    "    \n",
    "print(\"\\nAverage train accuracy: \", np.average(acc_train))\n",
    "print(\"Average test accuracy: \", np.average(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-factor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
