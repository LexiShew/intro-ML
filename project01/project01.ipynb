{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('ml_env': conda)"
  },
  "interpreter": {
   "hash": "5488065bb2d6ca6a9de5ec6734160292494259bd2c5abc9e8432ae79ff9e5079"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Project 1\n",
    "### Author: Lexi Shewchuk\n",
    "### Date: Saturday, 17 July"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lm\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import random"
   ]
  },
  {
   "source": [
    "## Part 1: Logistic Regression for Digit Classification\n",
    "The goal of this part is to train a logistic regression model that can take in 28x28 pixel greyscale images of handwritten 8s and 9s, and identiies each image as an 8 or a 9. A logistic regression model is trained on the given train data-digits_8_vs_9_noisy dataset. Several model hyperparameters were altered and used to generate different models in order to find a model that correctly classifies the training data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1.1: Fit logistic regression models and explore different max_iter values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the datasets and turn into ndarrays\n",
    "# make y data into 1D arrays using ravel()\n",
    "x_train = pd.read_csv('data_digits_8_vs_9_noisy/x_train.csv').to_numpy()\n",
    "y_train = pd.read_csv('data_digits_8_vs_9_noisy/y_train.csv').to_numpy().ravel()\n",
    "x_test = pd.read_csv('data_digits_8_vs_9_noisy/x_test.csv').to_numpy()\n",
    "y_test = pd.read_csv('data_digits_8_vs_9_noisy/y_test.csv').to_numpy().ravel()\n",
    "\n",
    "# explore what happens with varying max_iter values\n",
    "# keep track of accuracy and log loss\n",
    "accuracy = list()\n",
    "log_losses = list()\n",
    "models = list()\n",
    "\n",
    "# create logistic regression models and train on x_train data\n",
    "# all params are default except solver = 'liblinear' and max_iter\n",
    "# for small max_iter (<10), liblinear will fail to converge\n",
    "iter_values = list(range(1, 41, 1))\n",
    "for i in iter_values:\n",
    "    log_model = lm.LogisticRegression(solver='liblinear', max_iter=i)\n",
    "    log_model.fit(x_train, y_train)\n",
    "    y_test_pred = log_model.predict(x_test)\n",
    "    accuracy.append(log_model.score(x_test, y_test))\n",
    "    log_losses.append(metrics.log_loss(y_test, y_test_pred))\n",
    "    models.append(log_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy\n",
    "plt.title('Model Accuracy vs Maximum Iterations')\n",
    "plt.xlabel('maximum iterations')\n",
    "plt.ylabel('accuracy')\n",
    "plt.scatter(iter_values, accuracy)\n",
    "plt.show()\n",
    "\n",
    "# plot error\n",
    "plt.title('Logistic Loss vs Maximum Iterations')\n",
    "plt.xlabel('maximum iterations')\n",
    "plt.ylabel('logistic loss')\n",
    "plt.scatter(iter_values, log_losses, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "### Discussion:\n",
    "The above plots demonstrate that at the maximum interations increase, accuracy increases and loss decreases. For logistic regression, a model is trained by adjusting weights to minimize error on a given datum. When the number of iterations that the model is allowed to undergo is low (making the step size alpha larger), the amount of adjustement -- and therefore fitting -- decreases. This often prevents the loss from reaching a minimum, meaning that the model isn't as accurate as it could be.\n",
    "Log loss and accuracy are very closely related. Log loss is reduced by labeling positive examples as closer to 1, and negative examples as closer to 0 -- in other words, an accurately labeled datum reduces loss."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 1.2: \n",
    "# get the first weight for all 40 models\n",
    "coeffs = list()\n",
    "for model in models:\n",
    "    coeffs.append(model.coef_[0, 0])\n",
    "print(coeffs[38])\n",
    "# plot first weight vs iter value\n",
    "plt.title('First Weight vs Maximum Iterations')\n",
    "plt.xlabel('maximum iterations')\n",
    "plt.ylabel('first weight')\n",
    "plt.scatter(iter_values, coeffs);    "
   ]
  },
  {
   "source": [
    "### Discussion:\n",
    "This plot demonstrates that increasing the number of iterations allows for the weights to adjust more, and the ideal weight applied to pixel000 should be about -0.44 for the most accurate model. Initally, because max_iter is set so low, the model isn't able to sufficiently adjust weight and actually fit the training data, so the inital weight on the first pixel is nearer to zero.\n",
    "It's likely that the model determines that the first pixel, which would lie in the top left corner of the image, shouldn't have a great deal of sway in the determination of the digit, so if there is a relatively high value for that pixel, it shouldn't contribute greatly to the classification. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3\n",
    "c_grid = np.logspace(-9, 6, 31)\n",
    "min_loss = 10_000_000\n",
    "best_model = lm.LogisticRegression()\n",
    "\n",
    "# smaller C means larger penalty (stronger regularization)\n",
    "for c in c_grid:\n",
    "    model = lm.LogisticRegression(solver='liblinear', C=c)\n",
    "    model.fit(x_train, y_train)\n",
    "    loss = metrics.log_loss(y_test, model.predict(x_test))\n",
    "    if loss < min_loss:\n",
    "        print(min_loss, '-->', loss)\n",
    "        best_model = model\n",
    "        min_loss = loss\n",
    "y_test_pred = best_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Least loss:\", min_loss)\n",
    "print(\"Best model's accuracy:\", best_model.score(x_test, y_test))\n",
    "\n",
    "# generate confusion matrix for best model\n",
    "conf_matrix = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "sb.heatmap(conf_matrix, annot=True, cmap='Purples', fmt='d')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Value')\n",
    "plt.xlabel('Predicted Value')\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4\n",
    "\n",
    "# get lists of false positives and false negatives\n",
    "false_pos = list()\n",
    "false_neg = list()\n",
    "for i in range(len(x_test)):\n",
    "    if y_test[i] == 1 and y_test_pred[i] == 0:\n",
    "        false_neg.append(x_test[i])\n",
    "    elif y_test[i] == 0 and y_test_pred[i] == 1:\n",
    "        false_pos.append(x_test[i])\n",
    "\n",
    "# plot 9 random false positives\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(1, 10):\n",
    "    plt.subplot(3, 3, i)\n",
    "    rand_num = random.randrange(0, len(false_pos))\n",
    "    plt.imshow(false_pos[rand_num].reshape(28, -1), cmap='Greys', vmin=0.0, vmax=1.0)\n",
    "    plt.title('Predicted: 1, Actual: 0')\n",
    "plt.show()\n",
    "\n",
    "# plot 9 random false negatives\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(1, 10):\n",
    "    plt.subplot(3, 3, i)\n",
    "    rand_num = random.randrange(0, len(false_neg))\n",
    "    plt.imshow(false_neg[rand_num].reshape(28, -1), cmap='Greys', vmin=0.0, vmax=1.0)\n",
    "    plt.title('Predicted: 0, Actual: 1')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot all the fase negative examples\n",
    "# plt.figure(figsize=(20, 20))\n",
    "# for i in range(len(false_neg)):\n",
    "#     plt.subplot(len(false_neg) // 4, 5, i + 1)\n",
    "#     plt.imshow(false_neg[i].reshape(28, -1), cmap='Greys', vmin=0.0, vmax=1.0)\n",
    "#     plt.title('Predicted: 0, Actual: 1')\n",
    "# plt.show()"
   ]
  },
  {
   "source": [
    "For false positives, which in my case means 8s that have been mislabeled as 9s, many have a bottom loop that's very skinny or they're tilted right a bit. This makes sense because most 9s can be identified by having a stick at the base rather than a loop, so a skinny bottom would resemble a 9.\n",
    "False negatives -- 9s that have been mislabelled as 8s -- often have very loopy or wide tails, similar to the base of an 8. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5\n",
    "coeffs = np.array(best_model.coef_).reshape(28,28)\n",
    "\n",
    "plt.title('Pixel Influence')\n",
    "plt.imshow(coeffs, cmap='RdYlBu', vmin=-0.5, vmax=0.5)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "This colormap shows that the pixels most commonly found in 8s -- and thus have the most influence in classifying the example as an 8 -- are centered around (13,20), and follow a curvy crescent shape that you would see from at the base of a typical handwritten 8.\n",
    "The pixels that are most influential in determining a 9 are around (17,12), where the joint between the circle and stick of a 9 would be.\n",
    "This colormap provides some insight into the shapes of numbers that are misclassified. False negatives -- 9s that are labeled 8s -- often have loopy bottoms that go through the very negative-pixel region. False postives -- true 8s labeled as 9s -- often have a dense area where the joint of a 9 would be, and thus have more positive weights at those pixel locations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Part 2: Shirt Classification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the datasets\n",
    "x_train = pd.read_csv('data_shirts/train_shirt_x.csv').to_numpy()\n",
    "y_train = pd.read_csv('data_shirts/train_shirt_y.csv').to_numpy().ravel()\n",
    "x_test = pd.read_csv('data_shirts/test_shirt_x.csv').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# look at some of the images\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(1, 10):\n",
    "    plt.subplot(3, 3, i)\n",
    "    rand_num = random.randrange(0, len(x_train))\n",
    "    datum = x_train[rand_num].reshape(28, -1)\n",
    "    plt.imshow(datum, cmap='Blues', vmin=0.0, vmax=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(model, x, y):\n",
    "    return model.score(x, y)\n",
    "\n",
    "def loss(y, y_pred):\n",
    "    return metrics.log_loss(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a basic logistic regression model\n",
    "model = lm.LogisticRegression(solver='liblinear')\n",
    "model.fit(x_train, y_train)\n",
    "y_train_pred = model.predict(x_train)\n",
    "print('acc: ', acc(model, x_train, y_train))\n",
    "print('loss:', loss(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at what mistakes are being made\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do 5-fold cross validation to get more accurate loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change C\n",
    "model = lm.LogisticRegression()\n",
    "c_grid = np.logspace(-9, 6, 31)\n",
    "min_loss = 10_000_000\n",
    "\n",
    "for c in c_grid:\n",
    "    c_model = lm.LogisticRegression(solver='liblinear', C=c)\n",
    "    c_model.fit(x_train, y_train)\n",
    "    c_loss = loss(y_train, c_model.predict(x_train))\n",
    "    if c_loss < min_loss:\n",
    "        model = c_model\n",
    "        min_loss = c_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use L1 penalty instead of L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn every pixel to 'on' or 'off' by some threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore clustering"
   ]
  }
 ]
}