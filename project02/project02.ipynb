{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Project 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Process\r\n",
    "* create a small data set (only 12 elements) to look at text processing and see how BoW vectorization works\r\n",
    "* create vectors for the data\r\n",
    "    * default\r\n",
    "    * exclude common words\r\n",
    "    * exclude rare words\r\n",
    "    * count # of times word is used in single review? or just present at all\r\n",
    "    * look at word pairs? word triples? expensive to generate, could produce better results\r\n",
    "* train a simple logistic regression on the data\r\n",
    "    * look at all different vectors, see which set is best\r\n",
    "* tweak logistic hyperparams\r\n",
    "* train MLP\r\n",
    "* train decision tree"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "# imports\r\n",
    "\r\n",
    "import re\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "from sklearn.model_selection import KFold\r\n",
    "from sklearn.metrics import log_loss\r\n",
    "from sklearn.metrics import roc_auc_score\r\n",
    "from sklearn.preprocessing import minmax_scale\r\n",
    "\r\n",
    "from sklearn import linear_model\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "from sklearn.model_selection import RandomizedSearchCV\r\n",
    "from sklearn.neural_network import MLPClassifier\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn import tree"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data processing\r\n",
    "* Removing whitespace and punctuation\r\n",
    "* Converting to all lowercase\r\n",
    "* Generating vectors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Default BoW"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# big\r\n",
    "raw_reviews = pd.read_csv('data/data_reviews/x_train.csv')['text'].values.tolist()\r\n",
    "reviews = list()\r\n",
    "\r\n",
    "pattern = re.compile('[^a-z ]')\r\n",
    "for review in raw_reviews:\r\n",
    "    review = review.lower()\r\n",
    "    review = pattern.sub('', review)\r\n",
    "    reviews.append(review)\r\n",
    "\r\n",
    "vectorizer = CountVectorizer()\r\n",
    "X = vectorizer.fit_transform(reviews)\r\n",
    "y = pd.read_csv('data/data_reviews/y_train.csv').to_numpy().ravel()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# # small\r\n",
    "# raw_reviews_short = pd.read_csv('data/data_reviews/x_train_small.csv')['text'].values.tolist()\r\n",
    "# reviews_short = list()\r\n",
    "\r\n",
    "# pattern = re.compile('[^a-z ]')\r\n",
    "# for review_short in raw_reviews_short:\r\n",
    "#     review_short = review_short.lower()\r\n",
    "#     review_short = pattern.sub('', review_short)\r\n",
    "#     reviews_short.append(review_short)\r\n",
    "\r\n",
    "# vectorizer_short = CountVectorizer()\r\n",
    "# X_short = vectorizer_short.fit_transform(reviews_short)\r\n",
    "# y_short = pd.read_csv('data/data_reviews/y_train_small.csv').to_numpy().ravel()\r\n",
    "# # vectorizer_short.get_feature_names()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### BOW w "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Default"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# kf = KFold()\r\n",
    "# log_model = linear_model.LogisticRegression()\r\n",
    "\r\n",
    "# k_train_losses = list()\r\n",
    "# k_train_scores = list()\r\n",
    "# k_train_aucs = list()\r\n",
    "\r\n",
    "# k_val_losses = list()\r\n",
    "# k_val_scores = list()\r\n",
    "# k_val_aucs = list()\r\n",
    "\r\n",
    "# for train_index, val_index in kf.split(X):\r\n",
    "#     x_train, x_val = X[train_index], X[val_index]\r\n",
    "#     y_train, y_val = y[train_index], y[val_index]\r\n",
    "#     log_model.fit(x_train, y_train)\r\n",
    "    \r\n",
    "#     train_probas = minmax_scale(log_model.decision_function(x_train).reshape(-1,1))\r\n",
    "#     val_probas = minmax_scale(log_model.decision_function(x_val).reshape(-1, 1))\r\n",
    "#     # for i in train_probas:\r\n",
    "#     #     print(i, end=' ') \r\n",
    "#     # print()\r\n",
    "#     k_train_losses.append(log_loss(y_train, train_probas))\r\n",
    "#     k_train_scores.append(log_model.score(x_train, y_train))\r\n",
    "#     k_train_aucs.append(roc_auc_score(y_train, train_probas))\r\n",
    "    \r\n",
    "#     k_val_losses.append(log_loss(y_val, val_probas))\r\n",
    "#     k_val_scores.append(log_model.score(x_val, y_val))\r\n",
    "#     k_val_aucs.append(roc_auc_score(y_val, val_probas))\r\n",
    "    \r\n",
    "#     print('   TRAIN -- auc: {:5f}   loss: {:5f}   score: {:5f}'.format(\r\n",
    "#         k_train_aucs[-1],\r\n",
    "#         k_train_losses[-1],\r\n",
    "#         k_train_scores[-1]))\r\n",
    "\r\n",
    "#     print('   TEST --  auc: {:5f}   loss: {:5f}   score: {:5f}'.format(\r\n",
    "#         k_val_aucs[-1],\r\n",
    "#         k_val_losses[-1],\r\n",
    "#         k_val_scores[-1]))\r\n",
    "\r\n",
    "# print('TRAIN -- auc: {:5f}   loss: {:5f}   score: {:5f}'.format(\r\n",
    "#     np.mean(k_train_aucs),\r\n",
    "#     np.mean(k_train_losses),\r\n",
    "#     np.mean(k_train_scores)))\r\n",
    "\r\n",
    "# print('TEST --  auc: {:5f}   loss: {:5f}   score: {:5f}'.format(\r\n",
    "#     np.mean(k_val_aucs),\r\n",
    "#     np.mean(k_val_losses),\r\n",
    "#     np.mean(k_val_scores)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Max iters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# # find max_iter\r\n",
    "# train_losses = list()\r\n",
    "# train_scores = list()\r\n",
    "# val_scores = list()\r\n",
    "# val_losses = list()\r\n",
    "\r\n",
    "# for i in np.logspace(0, 2, 20):\r\n",
    "#     kf = KFold()\r\n",
    "#     log_model = linear_model.LogisticRegression(max_iter=i)\r\n",
    "\r\n",
    "#     k_train_losses = list()\r\n",
    "#     k_train_scores = list()\r\n",
    "#     k_val_losses   = list()\r\n",
    "#     k_val_scores   = list()\r\n",
    "\r\n",
    "#     for train_index, val_index in kf.split(X):\r\n",
    "#         x_train, x_val = X[train_index], X[val_index]\r\n",
    "#         y_train, y_val = y[train_index], y[val_index]\r\n",
    "#         log_model.fit(x_train, y_train)\r\n",
    "#         k_train_losses.append(logloss(x_train, y_train, log_model))\r\n",
    "#         k_train_scores.append(score(x_train, y_train, log_model))\r\n",
    "#         k_val_losses .append(logloss(x_val, y_val, log_model))\r\n",
    "#         k_val_scores .append(score(x_val, y_val, log_model))\r\n",
    "\r\n",
    "#     train_losses.append(np.mean(k_train_losses))\r\n",
    "#     train_scores.append(np.mean(k_train_scores))\r\n",
    "#     val_losses.append(np.mean(k_val_losses))\r\n",
    "#     val_scores.append(np.mean(k_val_scores))\r\n",
    "\r\n",
    "# # plot accuracy\r\n",
    "# plt.title('Model Accuracy vs Maximum Iterations')\r\n",
    "# plt.xlabel('maximum iterations')\r\n",
    "# plt.ylabel('accuracy')\r\n",
    "# plt.scatter(np.logspace(0, 2, 20), val_scores)\r\n",
    "# plt.show()\r\n",
    "\r\n",
    "# # plot error\r\n",
    "# plt.title('Logistic Loss vs Maximum Iterations')\r\n",
    "# plt.xlabel('maximum iterations')\r\n",
    "# plt.ylabel('logistic loss')\r\n",
    "# plt.scatter(np.logspace(0, 2, 20), val_losses, c='r')\r\n",
    "# plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LBFGS L2\r\n",
    "best C: 8.68511373751352"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# # find best C\r\n",
    "# train_scores = list()\r\n",
    "# train_aucs = list()\r\n",
    "# train_losses = list()\r\n",
    "\r\n",
    "# val_scores = list()\r\n",
    "# val_aucs = list()\r\n",
    "# val_losses = list()\r\n",
    "\r\n",
    "# best_c = 10**-2\r\n",
    "# best_loss = 1000\r\n",
    "\r\n",
    "# kf = KFold(shuffle=True)\r\n",
    "# for c in np.logspace(-2, 2, 50):\r\n",
    "#     log_model = linear_model.LogisticRegression(solver='lbfgs', penalty='l2', max_iter=500, C=c)\r\n",
    "    \r\n",
    "#     k_train_scores = list()\r\n",
    "#     k_train_aucs = list()\r\n",
    "#     k_train_losses = list()\r\n",
    "    \r\n",
    "#     k_val_scores = list()\r\n",
    "#     k_val_aucs = list()\r\n",
    "#     k_val_losses = list()\r\n",
    "    \r\n",
    "#     for train_index, val_index in kf.split(X):\r\n",
    "#         x_train, x_val = X[train_index], X[val_index]\r\n",
    "#         y_train, y_val = y[train_index], y[val_index]\r\n",
    "#         log_model.fit(x_train, y_train)\r\n",
    "        \r\n",
    "#         train_probas = minmax_scale(log_model.decision_function(x_train))\r\n",
    "#         val_probas = minmax_scale(log_model.decision_function(x_val))\r\n",
    "                \r\n",
    "#         k_train_scores.append(log_model.score(x_train, y_train))\r\n",
    "#         k_train_aucs.append(roc_auc_score(y_train, train_probas))\r\n",
    "#         k_train_losses.append(log_loss(y_train, train_probas))\r\n",
    "\r\n",
    "#         k_val_scores.append(log_model.score(x_val, y_val))\r\n",
    "#         k_val_aucs.append(roc_auc_score(y_val, val_probas))\r\n",
    "#         k_val_losses.append(log_loss(y_val, val_probas))\r\n",
    "\r\n",
    "#     if np.mean(k_val_losses) < best_loss:\r\n",
    "#         best_c = c\r\n",
    "#         best_loss = np.mean(k_val_losses)\r\n",
    "\r\n",
    "#     train_scores.append(np.mean(k_train_scores))\r\n",
    "#     train_aucs.append(np.mean(k_train_aucs))\r\n",
    "#     train_losses.append(np.mean(k_train_losses))\r\n",
    "\r\n",
    "#     val_scores.append(np.mean(k_val_scores))\r\n",
    "#     val_aucs.append(np.mean(k_val_aucs))\r\n",
    "#     val_losses.append(np.mean(k_val_losses))\r\n",
    "\r\n",
    "#     print('C: {:2f}   score: {:5f}   auc: {:5f}   loss: {:5f}'.format(\r\n",
    "#         c,\r\n",
    "#         np.mean(k_val_scores), \r\n",
    "#         np.mean(k_val_aucs),\r\n",
    "#         np.mean(k_val_losses)))\r\n",
    "\r\n",
    "# print('best C:', best_c)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# # plots\r\n",
    "\r\n",
    "# # plot accuracy\r\n",
    "# plt.title('Model Accuracy vs Inverse of Regularization Penalty')\r\n",
    "# plt.xlabel('inverse of regularization penalty, C')\r\n",
    "# plt.ylabel('accuracy')\r\n",
    "# plt.scatter(np.logspace(-2, 2, 50), val_scores, label='validation')\r\n",
    "# plt.scatter(np.logspace(-2, 2, 50), train_scores, c='r', label='train')\r\n",
    "# plt.xscale('log')\r\n",
    "# plt.legend()\r\n",
    "# plt.show()\r\n",
    "\r\n",
    "# # plot loss\r\n",
    "# plt.title('Logistic Loss vs Inverse of Regularization Penalty')\r\n",
    "# plt.xlabel('inverse of regularization penalty, C')\r\n",
    "# plt.ylabel('logistic loss')\r\n",
    "# plt.scatter(np.logspace(-2, 2, 50), val_losses, label='validation')\r\n",
    "# plt.scatter(np.logspace(-2, 2, 50), train_losses, c='r', label='train')\r\n",
    "# plt.xscale('log')\r\n",
    "# plt.legend()\r\n",
    "# plt.show()\r\n",
    "\r\n",
    "# # plot auc\r\n",
    "# plt.title('AUC of ROC Curve vs Inverse of Regularization Penalty')\r\n",
    "# plt.xlabel('inverse of regularization penalty, C')\r\n",
    "# plt.ylabel('auc')\r\n",
    "# plt.scatter(np.logspace(-2, 2, 50), val_aucs, label='validation')\r\n",
    "# plt.scatter(np.logspace(-2, 2, 50), train_aucs, c='r', label='train')\r\n",
    "# plt.xscale('log')\r\n",
    "# plt.legend()\r\n",
    "# plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# # best model details\r\n",
    "\r\n",
    "# kf = KFold()\r\n",
    "# log_model = linear_model.LogisticRegression(solver='lbfgs', penalty='l2', max_iter=500, C=c)\r\n",
    "\r\n",
    "# k_train_scores = list()\r\n",
    "# k_train_aucs = list()\r\n",
    "# k_train_losses = list()\r\n",
    "\r\n",
    "# k_val_scores = list()\r\n",
    "# k_val_aucs = list()\r\n",
    "# k_val_losses = list()\r\n",
    "\r\n",
    "# for train_index, val_index in kf.split(X):\r\n",
    "#     x_train, x_val = X[train_index], X[val_index]\r\n",
    "#     y_train, y_val = y[train_index], y[val_index]\r\n",
    "#     log_model.fit(x_train, y_train)\r\n",
    "\r\n",
    "#     train_probas = minmax_scale(log_model.decision_function(x_train))\r\n",
    "#     val_probas = minmax_scale(log_model.decision_function(x_val))\r\n",
    "            \r\n",
    "#     k_train_scores.append(log_model.score(x_train, y_train))\r\n",
    "#     k_train_aucs.append(roc_auc_score(y_train, train_probas))\r\n",
    "#     k_train_losses.append(log_loss(y_train, train_probas))\r\n",
    "\r\n",
    "#     k_val_scores.append(log_model.score(x_val, y_val))\r\n",
    "#     k_val_aucs.append(roc_auc_score(y_val, val_probas))\r\n",
    "#     k_val_losses.append(log_loss(y_val, val_probas))\r\n",
    "\r\n",
    "# print('TRAIN -- score: {:5f}   auc: {:5f}   loss: {:5f}'.format(\r\n",
    "#         np.mean(k_train_scores), \r\n",
    "#         np.mean(k_train_aucs),\r\n",
    "#         np.mean(k_train_losses)))\r\n",
    "\r\n",
    "# print('VALID -- score: {:5f}   auc: {:5f}   loss: {:5f}'.format(\r\n",
    "#         np.mean(k_val_scores), \r\n",
    "#         np.mean(k_val_aucs),\r\n",
    "#         np.mean(k_val_losses)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LBFGS none"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# # details\r\n",
    "\r\n",
    "# kf = KFold()\r\n",
    "# log_model = linear_model.LogisticRegression(solver='lbfgs', penalty='none', max_iter=500)\r\n",
    "\r\n",
    "# k_train_scores = list()\r\n",
    "# k_train_aucs = list()\r\n",
    "# k_train_losses = list()\r\n",
    "\r\n",
    "# k_val_scores = list()\r\n",
    "# k_val_aucs = list()\r\n",
    "# k_val_losses = list()\r\n",
    "\r\n",
    "# for train_index, val_index in kf.split(X):\r\n",
    "#     x_train, x_val = X[train_index], X[val_index]\r\n",
    "#     y_train, y_val = y[train_index], y[val_index]\r\n",
    "#     log_model.fit(x_train, y_train)\r\n",
    "\r\n",
    "#     train_probas = minmax_scale(log_model.decision_function(x_train))\r\n",
    "#     val_probas = minmax_scale(log_model.decision_function(x_val))\r\n",
    "            \r\n",
    "#     k_train_scores.append(log_model.score(x_train, y_train))\r\n",
    "#     k_train_aucs.append(roc_auc_score(y_train, train_probas))\r\n",
    "#     k_train_losses.append(log_loss(y_train, train_probas))\r\n",
    "\r\n",
    "#     k_val_scores.append(log_model.score(x_val, y_val))\r\n",
    "#     k_val_aucs.append(roc_auc_score(y_val, val_probas))\r\n",
    "#     k_val_losses.append(log_loss(y_val, val_probas))\r\n",
    "\r\n",
    "# print('TRAIN -- score: {:5f}   auc: {:5f}   loss: {:5f}'.format(\r\n",
    "#         np.mean(k_train_scores), \r\n",
    "#         np.mean(k_train_aucs),\r\n",
    "#         np.mean(k_train_losses)))\r\n",
    "\r\n",
    "# print('VALID -- score: {:5f}   auc: {:5f}   loss: {:5f}'.format(\r\n",
    "#         np.mean(k_val_scores), \r\n",
    "#         np.mean(k_val_aucs),\r\n",
    "#         np.mean(k_val_losses)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### liblinear L2\r\n",
    "best C: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# # find best C\r\n",
    "# train_scores = list()\r\n",
    "# train_aucs = list()\r\n",
    "# train_losses = list()\r\n",
    "\r\n",
    "# val_scores = list()\r\n",
    "# val_aucs = list()\r\n",
    "# val_losses = list()\r\n",
    "\r\n",
    "# best_c = 10**-2\r\n",
    "# best_loss = 1000\r\n",
    "\r\n",
    "# kf = KFold(shuffle=True)\r\n",
    "# for c in np.logspace(-2, 2, 50):\r\n",
    "#     log_model = linear_model.LogisticRegression(solver='liblinear', penalty='l2', max_iter=500, C=c)\r\n",
    "    \r\n",
    "#     k_train_scores = list()\r\n",
    "#     k_train_aucs = list()\r\n",
    "#     k_train_losses = list()\r\n",
    "    \r\n",
    "#     k_val_scores = list()\r\n",
    "#     k_val_aucs = list()\r\n",
    "#     k_val_losses = list()\r\n",
    "    \r\n",
    "#     for train_index, val_index in kf.split(X):\r\n",
    "#         x_train, x_val = X[train_index], X[val_index]\r\n",
    "#         y_train, y_val = y[train_index], y[val_index]\r\n",
    "#         log_model.fit(x_train, y_train)\r\n",
    "        \r\n",
    "#         train_probas = minmax_scale(log_model.decision_function(x_train))\r\n",
    "#         val_probas = minmax_scale(log_model.decision_function(x_val))\r\n",
    "                \r\n",
    "#         k_train_scores.append(log_model.score(x_train, y_train))\r\n",
    "#         k_train_aucs.append(roc_auc_score(y_train, train_probas))\r\n",
    "#         k_train_losses.append(log_loss(y_train, train_probas))\r\n",
    "\r\n",
    "#         k_val_scores.append(log_model.score(x_val, y_val))\r\n",
    "#         k_val_aucs.append(roc_auc_score(y_val, val_probas))\r\n",
    "#         k_val_losses.append(log_loss(y_val, val_probas))\r\n",
    "\r\n",
    "#     if np.mean(k_val_losses) < best_loss:\r\n",
    "#         best_c = c\r\n",
    "#         best_loss = np.mean(k_val_losses)\r\n",
    "\r\n",
    "#     train_scores.append(np.mean(k_train_scores))\r\n",
    "#     train_aucs.append(np.mean(k_train_aucs))\r\n",
    "#     train_losses.append(np.mean(k_train_losses))\r\n",
    "\r\n",
    "#     val_scores.append(np.mean(k_val_scores))\r\n",
    "#     val_aucs.append(np.mean(k_val_aucs))\r\n",
    "#     val_losses.append(np.mean(k_val_losses))\r\n",
    "\r\n",
    "#     print('C: {:2f}   score: {:5f}   auc: {:5f}   loss: {:5f}'.format(\r\n",
    "#         c,\r\n",
    "#         np.mean(k_val_scores), \r\n",
    "#         np.mean(k_val_aucs),\r\n",
    "#         np.mean(k_val_losses)))\r\n",
    "\r\n",
    "# print('best C:', best_c)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# # plots\r\n",
    "\r\n",
    "# # plot accuracy\r\n",
    "# plt.title('Model Accuracy vs Inverse of Regularization Penalty')\r\n",
    "# plt.xlabel('inverse of regularization penalty, C')\r\n",
    "# plt.ylabel('accuracy')\r\n",
    "# plt.scatter(np.logspace(-2, 2, 50), val_scores, label='validation')\r\n",
    "# plt.scatter(np.logspace(-2, 2, 50), train_scores, c='r', label='train')\r\n",
    "# plt.xscale('log')\r\n",
    "# plt.legend()\r\n",
    "# plt.show()\r\n",
    "\r\n",
    "# # plot loss\r\n",
    "# plt.title('Logistic Loss vs Inverse of Regularization Penalty')\r\n",
    "# plt.xlabel('inverse of regularization penalty, C')\r\n",
    "# plt.ylabel('logistic loss')\r\n",
    "# plt.scatter(np.logspace(-2, 2, 50), val_losses, label='validation')\r\n",
    "# plt.scatter(np.logspace(-2, 2, 50), train_losses, c='r', label='train')\r\n",
    "# plt.xscale('log')\r\n",
    "# plt.legend()\r\n",
    "# plt.show()\r\n",
    "\r\n",
    "# # plot auc\r\n",
    "# plt.title('AUC of ROC Curve vs Inverse of Regularization Penalty')\r\n",
    "# plt.xlabel('inverse of regularization penalty, C')\r\n",
    "# plt.ylabel('auc')\r\n",
    "# plt.scatter(np.logspace(-2, 2, 50), val_aucs, label='validation')\r\n",
    "# plt.scatter(np.logspace(-2, 2, 50), train_aucs, c='r', label='train')\r\n",
    "# plt.xscale('log')\r\n",
    "# plt.legend()\r\n",
    "# plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# # best model details\r\n",
    "\r\n",
    "# kf = KFold()\r\n",
    "# log_model = linear_model.LogisticRegression(solver='liblinear', penalty='l2', max_iter=500, C=c)\r\n",
    "\r\n",
    "# k_train_scores = list()\r\n",
    "# k_train_aucs = list()\r\n",
    "# k_train_losses = list()\r\n",
    "\r\n",
    "# k_val_scores = list()\r\n",
    "# k_val_aucs = list()\r\n",
    "# k_val_losses = list()\r\n",
    "\r\n",
    "# for train_index, val_index in kf.split(X):\r\n",
    "#     x_train, x_val = X[train_index], X[val_index]\r\n",
    "#     y_train, y_val = y[train_index], y[val_index]\r\n",
    "#     log_model.fit(x_train, y_train)\r\n",
    "\r\n",
    "#     train_probas = minmax_scale(log_model.decision_function(x_train))\r\n",
    "#     val_probas = minmax_scale(log_model.decision_function(x_val))\r\n",
    "            \r\n",
    "#     k_train_scores.append(log_model.score(x_train, y_train))\r\n",
    "#     k_train_aucs.append(roc_auc_score(y_train, train_probas))\r\n",
    "#     k_train_losses.append(log_loss(y_train, train_probas))\r\n",
    "\r\n",
    "#     k_val_scores.append(log_model.score(x_val, y_val))\r\n",
    "#     k_val_aucs.append(roc_auc_score(y_val, val_probas))\r\n",
    "#     k_val_losses.append(log_loss(y_val, val_probas))\r\n",
    "\r\n",
    "# print('TRAIN -- score: {:5f}   auc: {:5f}   loss: {:5f}'.format(\r\n",
    "#         np.mean(k_train_scores), \r\n",
    "#         np.mean(k_train_aucs),\r\n",
    "#         np.mean(k_train_losses)))\r\n",
    "\r\n",
    "# print('VALID -- score: {:5f}   auc: {:5f}   loss: {:5f}'.format(\r\n",
    "#         np.mean(k_val_scores), \r\n",
    "#         np.mean(k_val_aucs),\r\n",
    "#         np.mean(k_val_losses)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### liblinear L1\r\n",
    "best C: 8.68511373751352"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# # find best C\r\n",
    "# train_scores = list()\r\n",
    "# train_aucs = list()\r\n",
    "# train_losses = list()\r\n",
    "\r\n",
    "# val_scores = list()\r\n",
    "# val_aucs = list()\r\n",
    "# val_losses = list()\r\n",
    "\r\n",
    "# best_c = 10**-2\r\n",
    "# best_loss = 1000\r\n",
    "\r\n",
    "# kf = KFold(shuffle=True)\r\n",
    "# for c in np.logspace(-2, 2, 50):\r\n",
    "#     log_model = linear_model.LogisticRegression(solver='liblinear', penalty='l1', max_iter=500, C=c)\r\n",
    "    \r\n",
    "#     k_train_scores = list()\r\n",
    "#     k_train_aucs = list()\r\n",
    "#     k_train_losses = list()\r\n",
    "    \r\n",
    "#     k_val_scores = list()\r\n",
    "#     k_val_aucs = list()\r\n",
    "#     k_val_losses = list()\r\n",
    "    \r\n",
    "#     for train_index, val_index in kf.split(X):\r\n",
    "#         x_train, x_val = X[train_index], X[val_index]\r\n",
    "#         y_train, y_val = y[train_index], y[val_index]\r\n",
    "#         log_model.fit(x_train, y_train)\r\n",
    "        \r\n",
    "#         train_probas = minmax_scale(log_model.decision_function(x_train))\r\n",
    "#         val_probas = minmax_scale(log_model.decision_function(x_val))\r\n",
    "                \r\n",
    "#         k_train_scores.append(log_model.score(x_train, y_train))\r\n",
    "#         k_train_aucs.append(roc_auc_score(y_train, train_probas))\r\n",
    "#         k_train_losses.append(log_loss(y_train, train_probas))\r\n",
    "\r\n",
    "#         k_val_scores.append(log_model.score(x_val, y_val))\r\n",
    "#         k_val_aucs.append(roc_auc_score(y_val, val_probas))\r\n",
    "#         k_val_losses.append(log_loss(y_val, val_probas))\r\n",
    "\r\n",
    "#     if np.mean(k_val_losses) < best_loss:\r\n",
    "#         best_c = c\r\n",
    "#         best_loss = np.mean(k_val_losses)\r\n",
    "\r\n",
    "#     train_scores.append(np.mean(k_train_scores))\r\n",
    "#     train_aucs.append(np.mean(k_train_aucs))\r\n",
    "#     train_losses.append(np.mean(k_train_losses))\r\n",
    "\r\n",
    "#     val_scores.append(np.mean(k_val_scores))\r\n",
    "#     val_aucs.append(np.mean(k_val_aucs))\r\n",
    "#     val_losses.append(np.mean(k_val_losses))\r\n",
    "\r\n",
    "#     print('C: {:2f}   score: {:5f}   auc: {:5f}   loss: {:5f}'.format(\r\n",
    "#         c,\r\n",
    "#         np.mean(k_val_scores), \r\n",
    "#         np.mean(k_val_aucs),\r\n",
    "#         np.mean(k_val_losses)))\r\n",
    "\r\n",
    "# print('best C:', best_c)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# # plots\r\n",
    "\r\n",
    "# # plot accuracy\r\n",
    "# plt.title('Model Accuracy vs Inverse of Regularization Penalty')\r\n",
    "# plt.xlabel('inverse of regularization penalty, C')\r\n",
    "# plt.ylabel('accuracy')\r\n",
    "# plt.scatter(np.logspace(-2, 2, 50), val_scores, label='validation')\r\n",
    "# plt.scatter(np.logspace(-2, 2, 50), train_scores, c='r', label='train')\r\n",
    "# plt.xscale('log')\r\n",
    "# plt.legend()\r\n",
    "# plt.show()\r\n",
    "\r\n",
    "# # plot loss\r\n",
    "# plt.title('Logistic Loss vs Inverse of Regularization Penalty')\r\n",
    "# plt.xlabel('inverse of regularization penalty, C')\r\n",
    "# plt.ylabel('logistic loss')\r\n",
    "# plt.scatter(np.logspace(-2, 2, 50), val_losses, label='validation')\r\n",
    "# plt.scatter(np.logspace(-2, 2, 50), train_losses, c='r', label='train')\r\n",
    "# plt.xscale('log')\r\n",
    "# plt.legend()\r\n",
    "# plt.show()\r\n",
    "\r\n",
    "# # plot auc\r\n",
    "# plt.title('AUC of ROC Curve vs Inverse of Regularization Penalty')\r\n",
    "# plt.xlabel('inverse of regularization penalty, C')\r\n",
    "# plt.ylabel('auc')\r\n",
    "# plt.scatter(np.logspace(-2, 2, 50), val_aucs, label='validation')\r\n",
    "# plt.scatter(np.logspace(-2, 2, 50), train_aucs, c='r', label='train')\r\n",
    "# plt.xscale('log')\r\n",
    "# plt.legend()\r\n",
    "# plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# # best model details\r\n",
    "\r\n",
    "# kf = KFold()\r\n",
    "# log_model = linear_model.LogisticRegression(solver='liblinear', penalty='l1', max_iter=500, C=c)\r\n",
    "\r\n",
    "# k_train_scores = list()\r\n",
    "# k_train_aucs = list()\r\n",
    "# k_train_losses = list()\r\n",
    "\r\n",
    "# k_val_scores = list()\r\n",
    "# k_val_aucs = list()\r\n",
    "# k_val_losses = list()\r\n",
    "\r\n",
    "# for train_index, val_index in kf.split(X):\r\n",
    "#     x_train, x_val = X[train_index], X[val_index]\r\n",
    "#     y_train, y_val = y[train_index], y[val_index]\r\n",
    "#     log_model.fit(x_train, y_train)\r\n",
    "\r\n",
    "#     train_probas = minmax_scale(log_model.decision_function(x_train))\r\n",
    "#     val_probas = minmax_scale(log_model.decision_function(x_val))\r\n",
    "            \r\n",
    "#     k_train_scores.append(log_model.score(x_train, y_train))\r\n",
    "#     k_train_aucs.append(roc_auc_score(y_train, train_probas))\r\n",
    "#     k_train_losses.append(log_loss(y_train, train_probas))\r\n",
    "\r\n",
    "#     k_val_scores.append(log_model.score(x_val, y_val))\r\n",
    "#     k_val_aucs.append(roc_auc_score(y_val, val_probas))\r\n",
    "#     k_val_losses.append(log_loss(y_val, val_probas))\r\n",
    "\r\n",
    "# print('TRAIN -- score: {:5f}   auc: {:5f}   loss: {:5f}'.format(\r\n",
    "#         np.mean(k_train_scores), \r\n",
    "#         np.mean(k_train_aucs),\r\n",
    "#         np.mean(k_train_losses)))\r\n",
    "\r\n",
    "# print('VALID -- score: {:5f}   auc: {:5f}   loss: {:5f}'.format(\r\n",
    "#         np.mean(k_val_scores), \r\n",
    "#         np.mean(k_val_aucs),\r\n",
    "#         np.mean(k_val_losses)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MLP"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# random grid search best model\r\n",
    "# generally, lower alpha and lower tolerance work best, with only small changes between solver and activation\r\n",
    "parameters = {\r\n",
    "    'solver': ['adam', 'lbfgs'],\r\n",
    "    'activation': ['relu', 'identity'],\r\n",
    "    'alpha': list(np.logspace(-3, 3, 25)),\r\n",
    "    'tol': [10**i for i in range(-8, 3, 1)]\r\n",
    "}\r\n",
    "\r\n",
    "mlp = RandomizedSearchCV(MLPClassifier(), parameters, n_iter=50)\r\n",
    "mlp.fit(X, y)\r\n",
    "\r\n",
    "pd.DataFrame(mlp.cv_results_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\lexis\\miniconda3\\envs\\ml_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\lexis\\miniconda3\\envs\\ml_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\lexis\\miniconda3\\envs\\ml_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_tol  \\\n",
       "0       1.773048      0.240800         0.002393        0.001010       100   \n",
       "1       3.319957      0.400572         0.001400        0.001354     0.001   \n",
       "2      20.148663      0.733699         0.001793        0.000398   0.00001   \n",
       "3      29.673104      1.563069         0.001795        0.000744       0.0   \n",
       "4       3.962802      0.110580         0.002439        0.001086         1   \n",
       "5      25.225149      3.102389         0.001398        0.000486       0.0   \n",
       "6      13.213224      2.198493         0.001195        0.000400    0.0001   \n",
       "7      13.539051      4.003281         0.001797        0.000399    0.0001   \n",
       "8      17.305067      1.399279         0.002282        0.001143    0.0001   \n",
       "9       3.878847      0.427348         0.001398        0.000488     0.001   \n",
       "\n",
       "  param_solver param_alpha param_activation  \\\n",
       "0        lbfgs  562.341325         identity   \n",
       "1        lbfgs    0.017783         identity   \n",
       "2         adam   56.234133             relu   \n",
       "3         adam       100.0             relu   \n",
       "4         adam    5.623413         identity   \n",
       "5         adam    3.162278             relu   \n",
       "6         adam    5.623413         identity   \n",
       "7        lbfgs   56.234133             relu   \n",
       "8        lbfgs    0.562341             relu   \n",
       "9        lbfgs         1.0         identity   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'tol': 100, 'solver': 'lbfgs', 'alpha': 562.3...           0.493750   \n",
       "1  {'tol': 0.001, 'solver': 'lbfgs', 'alpha': 0.0...           0.827083   \n",
       "2  {'tol': 1e-05, 'solver': 'adam', 'alpha': 56.2...           0.500000   \n",
       "3  {'tol': 1e-08, 'solver': 'adam', 'alpha': 100....           0.500000   \n",
       "4  {'tol': 1, 'solver': 'adam', 'alpha': 5.623413...           0.795833   \n",
       "5  {'tol': 1e-08, 'solver': 'adam', 'alpha': 3.16...           0.814583   \n",
       "6  {'tol': 0.0001, 'solver': 'adam', 'alpha': 5.6...           0.797917   \n",
       "7  {'tol': 0.0001, 'solver': 'lbfgs', 'alpha': 56...           0.795833   \n",
       "8  {'tol': 0.0001, 'solver': 'lbfgs', 'alpha': 0....           0.810417   \n",
       "9  {'tol': 0.001, 'solver': 'lbfgs', 'alpha': 1.0...           0.802083   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0           0.497917           0.504167           0.475000           0.497917   \n",
       "1           0.802083           0.747917           0.764583           0.806250   \n",
       "2           0.500000           0.500000           0.500000           0.500000   \n",
       "3           0.500000           0.500000           0.500000           0.500000   \n",
       "4           0.766667           0.741667           0.747917           0.781250   \n",
       "5           0.800000           0.762500           0.777083           0.816667   \n",
       "6           0.779167           0.741667           0.745833           0.802083   \n",
       "7           0.775000           0.745833           0.750000           0.808333   \n",
       "8           0.800000           0.762500           0.772917           0.839583   \n",
       "9           0.820833           0.733333           0.762500           0.818750   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0         0.493750        0.009948               10  \n",
       "1         0.789583        0.028988                3  \n",
       "2         0.500000        0.000000                8  \n",
       "3         0.500000        0.000000                8  \n",
       "4         0.766667        0.020199                7  \n",
       "5         0.794167        0.021221                2  \n",
       "6         0.773333        0.025393                6  \n",
       "7         0.775000        0.024580                5  \n",
       "8         0.797083        0.027468                1  \n",
       "9         0.787500        0.034233                4  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_tol</th>\n",
       "      <th>param_solver</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>param_activation</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.773048</td>\n",
       "      <td>0.240800</td>\n",
       "      <td>0.002393</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>100</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>562.341325</td>\n",
       "      <td>identity</td>\n",
       "      <td>{'tol': 100, 'solver': 'lbfgs', 'alpha': 562.3...</td>\n",
       "      <td>0.493750</td>\n",
       "      <td>0.497917</td>\n",
       "      <td>0.504167</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.497917</td>\n",
       "      <td>0.493750</td>\n",
       "      <td>0.009948</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.319957</td>\n",
       "      <td>0.400572</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>0.001</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.017783</td>\n",
       "      <td>identity</td>\n",
       "      <td>{'tol': 0.001, 'solver': 'lbfgs', 'alpha': 0.0...</td>\n",
       "      <td>0.827083</td>\n",
       "      <td>0.802083</td>\n",
       "      <td>0.747917</td>\n",
       "      <td>0.764583</td>\n",
       "      <td>0.806250</td>\n",
       "      <td>0.789583</td>\n",
       "      <td>0.028988</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.148663</td>\n",
       "      <td>0.733699</td>\n",
       "      <td>0.001793</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>adam</td>\n",
       "      <td>56.234133</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'tol': 1e-05, 'solver': 'adam', 'alpha': 56.2...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.673104</td>\n",
       "      <td>1.563069</td>\n",
       "      <td>0.001795</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.0</td>\n",
       "      <td>adam</td>\n",
       "      <td>100.0</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'tol': 1e-08, 'solver': 'adam', 'alpha': 100....</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.962802</td>\n",
       "      <td>0.110580</td>\n",
       "      <td>0.002439</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>1</td>\n",
       "      <td>adam</td>\n",
       "      <td>5.623413</td>\n",
       "      <td>identity</td>\n",
       "      <td>{'tol': 1, 'solver': 'adam', 'alpha': 5.623413...</td>\n",
       "      <td>0.795833</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.741667</td>\n",
       "      <td>0.747917</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.020199</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25.225149</td>\n",
       "      <td>3.102389</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.0</td>\n",
       "      <td>adam</td>\n",
       "      <td>3.162278</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'tol': 1e-08, 'solver': 'adam', 'alpha': 3.16...</td>\n",
       "      <td>0.814583</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.777083</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.794167</td>\n",
       "      <td>0.021221</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13.213224</td>\n",
       "      <td>2.198493</td>\n",
       "      <td>0.001195</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>adam</td>\n",
       "      <td>5.623413</td>\n",
       "      <td>identity</td>\n",
       "      <td>{'tol': 0.0001, 'solver': 'adam', 'alpha': 5.6...</td>\n",
       "      <td>0.797917</td>\n",
       "      <td>0.779167</td>\n",
       "      <td>0.741667</td>\n",
       "      <td>0.745833</td>\n",
       "      <td>0.802083</td>\n",
       "      <td>0.773333</td>\n",
       "      <td>0.025393</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.539051</td>\n",
       "      <td>4.003281</td>\n",
       "      <td>0.001797</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>56.234133</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'tol': 0.0001, 'solver': 'lbfgs', 'alpha': 56...</td>\n",
       "      <td>0.795833</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.745833</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.024580</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17.305067</td>\n",
       "      <td>1.399279</td>\n",
       "      <td>0.002282</td>\n",
       "      <td>0.001143</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.562341</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'tol': 0.0001, 'solver': 'lbfgs', 'alpha': 0....</td>\n",
       "      <td>0.810417</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.772917</td>\n",
       "      <td>0.839583</td>\n",
       "      <td>0.797083</td>\n",
       "      <td>0.027468</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.878847</td>\n",
       "      <td>0.427348</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.001</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>1.0</td>\n",
       "      <td>identity</td>\n",
       "      <td>{'tol': 0.001, 'solver': 'lbfgs', 'alpha': 1.0...</td>\n",
       "      <td>0.802083</td>\n",
       "      <td>0.820833</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.818750</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.034233</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Logistic\r\n",
    "pretty trash -- ~15% worse than ReLU or ID"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# kf = KFold()\r\n",
    "# log_MLP = MLPClassifier(activation='logistic')\r\n",
    "\r\n",
    "# k_train_losses = list()\r\n",
    "# k_train_scores = list()\r\n",
    "# k_train_aucs = list()\r\n",
    "\r\n",
    "# k_val_losses = list()\r\n",
    "# k_val_scores = list()\r\n",
    "# k_val_aucs = list()\r\n",
    "\r\n",
    "# for train_index, val_index in kf.split(X):\r\n",
    "#     x_train, x_val = X[train_index], X[val_index]\r\n",
    "#     y_train, y_val = y[train_index], y[val_index]\r\n",
    "#     log_MLP.fit(x_train, y_train)\r\n",
    "    \r\n",
    "#     train_probas = log_MLP.predict_proba(x_train)[:,1]\r\n",
    "#     val_probas = log_MLP.predict_proba(x_val)[:,1]\r\n",
    "#     # for i in train_probas:\r\n",
    "#     #     print(i, end=' ') \r\n",
    "#     # print()\r\n",
    "#     k_train_losses.append(log_loss(y_train, train_probas))\r\n",
    "#     k_train_scores.append(log_MLP.score(x_train, y_train))\r\n",
    "#     k_train_aucs.append(roc_auc_score(y_train, train_probas))\r\n",
    "    \r\n",
    "#     k_val_losses.append(log_loss(y_val, val_probas))\r\n",
    "#     k_val_scores.append(log_MLP.score(x_val, y_val))\r\n",
    "#     k_val_aucs.append(roc_auc_score(y_val, val_probas))\r\n",
    "    \r\n",
    "#     print('   TRAIN -- auc: {:5f}   loss: {:5f}   score: {:5f}'.format(\r\n",
    "#         k_train_aucs[-1],\r\n",
    "#         k_train_losses[-1],\r\n",
    "#         k_train_scores[-1]))\r\n",
    "\r\n",
    "#     print('   TEST --  auc: {:5f}   loss: {:5f}   score: {:5f}'.format(\r\n",
    "#         k_val_aucs[-1],\r\n",
    "#         k_val_losses[-1],\r\n",
    "#         k_val_scores[-1]))\r\n",
    "\r\n",
    "# print('TRAIN -- auc: {:5f}   loss: {:5f}   score: {:5f}'.format(\r\n",
    "#     np.mean(k_train_aucs),\r\n",
    "#     np.mean(k_train_losses),\r\n",
    "#     np.mean(k_train_scores)))\r\n",
    "\r\n",
    "# print('TEST --  auc: {:5f}   loss: {:5f}   score: {:5f}'.format(\r\n",
    "#     np.mean(k_val_aucs),\r\n",
    "#     np.mean(k_val_losses),\r\n",
    "#     np.mean(k_val_scores)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ADAM ReLU"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# # find best alpha\r\n",
    "# train_scores = list()\r\n",
    "# train_aucs = list()\r\n",
    "# train_losses = list()\r\n",
    "\r\n",
    "# val_scores = list()\r\n",
    "# val_aucs = list()\r\n",
    "# val_losses = list()\r\n",
    "\r\n",
    "# best_alpha = 10**-3\r\n",
    "# best_loss = 1000\r\n",
    "\r\n",
    "# kf = KFold(shuffle=True)\r\n",
    "# for a in np.logspace(-3, 3, 15):\r\n",
    "#     model = MLPClassifier(activation='relu', solver='adam', alpha=a)\r\n",
    "    \r\n",
    "#     k_train_scores = list()\r\n",
    "#     k_train_aucs = list()\r\n",
    "#     k_train_losses = list()\r\n",
    "    \r\n",
    "#     k_val_scores = list()\r\n",
    "#     k_val_aucs = list()\r\n",
    "#     k_val_losses = list()\r\n",
    "    \r\n",
    "#     for train_index, val_index in kf.split(X):\r\n",
    "#         x_train, x_val = X[train_index], X[val_index]\r\n",
    "#         y_train, y_val = y[train_index], y[val_index]\r\n",
    "#         model.fit(x_train, y_train)\r\n",
    "        \r\n",
    "#         train_probas = model.predict_proba(x_train)[:,1]\r\n",
    "#         val_probas = model.predict_proba(x_val)[:,1]\r\n",
    "                \r\n",
    "#         k_train_scores.append(model.score(x_train, y_train))\r\n",
    "#         k_train_aucs.append(roc_auc_score(y_train, train_probas))\r\n",
    "#         k_train_losses.append(log_loss(y_train, train_probas))\r\n",
    "\r\n",
    "#         k_val_scores.append(model.score(x_val, y_val))\r\n",
    "#         k_val_aucs.append(roc_auc_score(y_val, val_probas))\r\n",
    "#         k_val_losses.append(log_loss(y_val, val_probas))\r\n",
    "\r\n",
    "#     if np.mean(k_val_losses) < best_loss:\r\n",
    "#         best_alpha = a\r\n",
    "#         best_loss = np.mean(k_val_losses)\r\n",
    "\r\n",
    "#     train_scores.append(np.mean(k_train_scores))\r\n",
    "#     train_aucs.append(np.mean(k_train_aucs))\r\n",
    "#     train_losses.append(np.mean(k_train_losses))\r\n",
    "\r\n",
    "#     val_scores.append(np.mean(k_val_scores))\r\n",
    "#     val_aucs.append(np.mean(k_val_aucs))\r\n",
    "#     val_losses.append(np.mean(k_val_losses))\r\n",
    "\r\n",
    "#     print('alpha: {:4f}   score: {:5f}   auc: {:5f}   loss: {:5f}'.format(\r\n",
    "#         a,\r\n",
    "#         np.mean(k_val_scores), \r\n",
    "#         np.mean(k_val_aucs),\r\n",
    "#         np.mean(k_val_losses)))\r\n",
    "\r\n",
    "# print('best alpha:', best_alpha)\r\n",
    "\r\n",
    "# # plot accuracy vs alpha\r\n",
    "# plt.title('Model Accuracy vs Regularization Penalty')\r\n",
    "# plt.xlabel('regularization penalty (alpha)')\r\n",
    "# plt.ylabel('accuracy')\r\n",
    "# plt.scatter(np.logspace(-3, 3, 15), train_scores, c='r', label='train')\r\n",
    "# plt.scatter(np.logspace(-3, 3, 15), val_scores, label='validation')\r\n",
    "# plt.xscale('log')\r\n",
    "# plt.legend()\r\n",
    "# plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# best_alpha = 1.0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# # find best tolerance\r\n",
    "# train_scores = list()\r\n",
    "# train_aucs = list()\r\n",
    "# train_losses = list()\r\n",
    "\r\n",
    "# val_scores = list()\r\n",
    "# val_aucs = list()\r\n",
    "# val_losses = list()\r\n",
    "\r\n",
    "# best_tol = 10**-8\r\n",
    "# best_loss = 1000\r\n",
    "\r\n",
    "# tols = list()\r\n",
    "# for i in range(-12, 4, 1):\r\n",
    "#     tols.append(10**i)\r\n",
    "\r\n",
    "\r\n",
    "# kf = KFold(shuffle=True)\r\n",
    "# for t in tols:\r\n",
    "#     model = MLPClassifier(activation='relu', solver='adam', alpha=best_alpha, tol=t)\r\n",
    "    \r\n",
    "#     k_train_scores = list()\r\n",
    "#     k_train_aucs = list()\r\n",
    "#     k_train_losses = list()\r\n",
    "    \r\n",
    "#     k_val_scores = list()\r\n",
    "#     k_val_aucs = list()\r\n",
    "#     k_val_losses = list()\r\n",
    "    \r\n",
    "#     for train_index, val_index in kf.split(X):\r\n",
    "#         x_train, x_val = X[train_index], X[val_index]\r\n",
    "#         y_train, y_val = y[train_index], y[val_index]\r\n",
    "#         model.fit(x_train, y_train)\r\n",
    "        \r\n",
    "#         train_probas = model.predict_proba(x_train)[:,1]\r\n",
    "#         val_probas = model.predict_proba(x_val)[:,1]\r\n",
    "                \r\n",
    "#         k_train_scores.append(model.score(x_train, y_train))\r\n",
    "#         k_train_aucs.append(roc_auc_score(y_train, train_probas))\r\n",
    "#         k_train_losses.append(log_loss(y_train, train_probas))\r\n",
    "\r\n",
    "#         k_val_scores.append(model.score(x_val, y_val))\r\n",
    "#         k_val_aucs.append(roc_auc_score(y_val, val_probas))\r\n",
    "#         k_val_losses.append(log_loss(y_val, val_probas))\r\n",
    "\r\n",
    "#     if np.mean(k_val_losses) < best_loss:\r\n",
    "#         best_tol = t\r\n",
    "#         best_loss = np.mean(k_val_losses)\r\n",
    "\r\n",
    "#     train_scores.append(np.mean(k_train_scores))\r\n",
    "#     train_aucs.append(np.mean(k_train_aucs))\r\n",
    "#     train_losses.append(np.mean(k_train_losses))\r\n",
    "\r\n",
    "#     val_scores.append(np.mean(k_val_scores))\r\n",
    "#     val_aucs.append(np.mean(k_val_aucs))\r\n",
    "#     val_losses.append(np.mean(k_val_losses))\r\n",
    "\r\n",
    "#     print('tol: {}   score: {:5f}   auc: {:5f}   loss: {:5f}'.format(\r\n",
    "#         t,\r\n",
    "#         np.mean(k_val_scores), \r\n",
    "#         np.mean(k_val_aucs),\r\n",
    "#         np.mean(k_val_losses)))\r\n",
    "\r\n",
    "# print('best tol:', best_tol)\r\n",
    "\r\n",
    "# # plot accuracy vs layer size\r\n",
    "# plt.title('Model Accuracy vs Tolerance')\r\n",
    "# plt.xlabel('tolerance (1e-tol)')\r\n",
    "# plt.ylabel('accuracy')\r\n",
    "# plt.scatter(tols, train_scores, c='r', label='train')\r\n",
    "# plt.scatter(tols, val_scores, label='validation')\r\n",
    "# plt.xscale('log')\r\n",
    "# plt.legend()\r\n",
    "# plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "\r\n",
    "# # plot accuracy vs layer size\r\n",
    "# plt.title('Model Accuracy vs Tolerance')\r\n",
    "# plt.xlabel('tolerance (1e-tol)')\r\n",
    "# plt.ylabel('accuracy')\r\n",
    "# plt.scatter(tols, train_scores, c='r', label='train')\r\n",
    "# plt.scatter(tols, val_scores, label='validation')\r\n",
    "# plt.xscale('log')\r\n",
    "# plt.legend()\r\n",
    "# plt.show()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# # get details of best model\r\n",
    "# model = MLPClassifier(activation='relu', solver='adam', alpha=best_alpha, tol=best_tol)\r\n",
    "    \r\n",
    "# k_train_scores = list()\r\n",
    "# k_train_aucs = list()\r\n",
    "# k_train_losses = list()\r\n",
    "\r\n",
    "# k_val_scores = list()\r\n",
    "# k_val_aucs = list()\r\n",
    "# k_val_losses = list()\r\n",
    "\r\n",
    "# for train_index, val_index in kf.split(X):\r\n",
    "#     x_train, x_val = X[train_index], X[val_index]\r\n",
    "#     y_train, y_val = y[train_index], y[val_index]\r\n",
    "#     model.fit(x_train, y_train)\r\n",
    "    \r\n",
    "#     train_probas = model.predict_proba(x_train)[:,1]\r\n",
    "#     val_probas = model.predict_proba(x_val)[:,1]\r\n",
    "            \r\n",
    "#     k_train_scores.append(model.score(x_train, y_train))\r\n",
    "#     k_train_aucs.append(roc_auc_score(y_train, train_probas))\r\n",
    "#     k_train_losses.append(log_loss(y_train, train_probas))\r\n",
    "\r\n",
    "#     k_val_scores.append(model.score(x_val, y_val))\r\n",
    "#     k_val_aucs.append(roc_auc_score(y_val, val_probas))\r\n",
    "#     k_val_losses.append(log_loss(y_val, val_probas))\r\n",
    "\r\n",
    "# print('TRAIN -- auc: {:5f}   loss: {:5f}   score: {:5f}'.format(\r\n",
    "#     np.mean(k_train_aucs),\r\n",
    "#     np.mean(k_train_losses),\r\n",
    "#     np.mean(k_train_scores)))\r\n",
    "\r\n",
    "# print('VALID -- auc: {:5f}   loss: {:5f}   score: {:5f}'.format(\r\n",
    "#     np.mean(k_val_aucs),\r\n",
    "#     np.mean(k_val_losses),\r\n",
    "#     np.mean(k_val_scores)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decision Tree Classifier "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Simple Decision Tree"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "kf = KFold()\r\n",
    "model = tree.DecisionTreeClassifier()\r\n",
    "\r\n",
    "k_train_losses = list()\r\n",
    "k_train_scores = list()\r\n",
    "k_train_aucs = list()\r\n",
    "\r\n",
    "k_val_losses = list()\r\n",
    "k_val_scores = list()\r\n",
    "k_val_aucs = list()\r\n",
    "\r\n",
    "for train_index, val_index in kf.split(X):\r\n",
    "    x_train, x_val = X[train_index], X[val_index]\r\n",
    "    y_train, y_val = y[train_index], y[val_index]\r\n",
    "    model.fit(x_train, y_train)\r\n",
    "    \r\n",
    "    train_probas = model.predict_proba(x_train)[:,1]\r\n",
    "    val_probas = model.predict_proba(x_val)[:,1]\r\n",
    "\r\n",
    "    k_train_losses.append(log_loss(y_train, train_probas))\r\n",
    "    k_train_scores.append(model.score(x_train, y_train))\r\n",
    "    k_train_aucs.append(roc_auc_score(y_train, train_probas))\r\n",
    "    \r\n",
    "    k_val_losses.append(log_loss(y_val, val_probas))\r\n",
    "    k_val_scores.append(model.score(x_val, y_val))\r\n",
    "    k_val_aucs.append(roc_auc_score(y_val, val_probas))\r\n",
    "    \r\n",
    "    print('   TRAIN -- score: {:5f}   auc: {:5f}   loss: {:5f}'.format(\r\n",
    "        k_train_scores[-1],\r\n",
    "        k_train_aucs[-1],\r\n",
    "        k_train_losses[-1]))\r\n",
    "\r\n",
    "    print('   VALID -- score: {:5f}   auc: {:5f}   loss: {:5f}'.format(\r\n",
    "        k_val_scores[-1],\r\n",
    "        k_val_aucs[-1],\r\n",
    "        k_val_losses[-1]))\r\n",
    "\r\n",
    "print('TRAIN -- score: {:5f}   auc: {:5f}   loss: {:5f}'.format(\r\n",
    "    np.mean(k_train_scores),\r\n",
    "    np.mean(k_train_aucs),\r\n",
    "    np.mean(k_train_losses)))\r\n",
    "\r\n",
    "print('VALID -- score: {:5f}   auc: {:5f}   loss: {:5f}'.format(\r\n",
    "    np.mean(k_val_scores),\r\n",
    "    np.mean(k_val_aucs),\r\n",
    "    np.mean(k_val_losses)))\r\n",
    "\r\n",
    "tree.plot_tree(model)\r\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   TRAIN -- auc: 1.000000   loss: 0.000000   score: 1.000000\n",
      "   TEST --  auc: 0.687500   loss: 15.686711   score: 0.545833\n",
      "   TRAIN -- auc: 1.000000   loss: 0.000000   score: 1.000000\n",
      "   TEST --  auc: 0.625000   loss: 12.952141   score: 0.625000\n",
      "   TRAIN -- auc: 1.000000   loss: 0.000000   score: 1.000000\n",
      "   TEST --  auc: 0.662500   loss: 11.656965   score: 0.662500\n",
      "   TRAIN -- auc: 1.000000   loss: 0.000000   score: 1.000000\n",
      "   TEST --  auc: 0.645313   loss: 12.088753   score: 0.650000\n",
      "   TRAIN -- auc: 1.000000   loss: 0.000000   score: 1.000000\n",
      "   TEST --  auc: 0.686250   loss: 14.031401   score: 0.593750\n",
      "TRAIN -- score: 1.000000   auc: 1.000000   loss: 0.000000\n",
      "VALID -- score: 0.615417   auc: 0.661312   loss: 13.283194\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "# find best depth\r\n",
    "train_scores = list()\r\n",
    "train_aucs = list()\r\n",
    "\r\n",
    "val_scores = list()\r\n",
    "val_aucs = list()\r\n",
    "\r\n",
    "best_depth = 1\r\n",
    "best_score = 0\r\n",
    "\r\n",
    "kf = KFold(shuffle=True)\r\n",
    "for d in np.logspace(0, 10, 25):\r\n",
    "    model = RandomForestClassifier(max_depth=d)\r\n",
    "    \r\n",
    "    k_train_scores = list()\r\n",
    "    k_train_aucs = list()\r\n",
    "    \r\n",
    "    k_val_scores = list()\r\n",
    "    k_val_aucs = list()\r\n",
    "    \r\n",
    "    for train_index, val_index in kf.split(X):\r\n",
    "        x_train, x_val = X[train_index], X[val_index]\r\n",
    "        y_train, y_val = y[train_index], y[val_index]\r\n",
    "        model.fit(x_train, y_train)\r\n",
    "        \r\n",
    "        train_probas = model.predict_proba(x_train)[:,1]\r\n",
    "        val_probas = model.predict_proba(x_val)[:,1]\r\n",
    "                \r\n",
    "        k_train_scores.append(model.score(x_train, y_train))\r\n",
    "        k_train_aucs.append(roc_auc_score(y_train, train_probas))\r\n",
    "        \r\n",
    "        k_val_scores.append(model.score(x_val, y_val))\r\n",
    "        k_val_aucs.append(roc_auc_score(y_val, val_probas))\r\n",
    "\r\n",
    "    if np.mean(k_val_scores) > best_score:\r\n",
    "        best_depth = d\r\n",
    "        best_score = np.mean(k_val_scores)\r\n",
    "\r\n",
    "    train_scores.append(np.mean(k_train_scores))\r\n",
    "    train_aucs.append(np.mean(k_train_aucs))\r\n",
    "    \r\n",
    "    val_scores.append(np.mean(k_val_scores))\r\n",
    "    val_aucs.append(np.mean(k_val_aucs))\r\n",
    "    \r\n",
    "    print('depth: {:2f}   score: {:5f}   auc: {:5f}'.format(\r\n",
    "        d,\r\n",
    "        np.mean(k_val_scores), \r\n",
    "        np.mean(k_val_aucs)))\r\n",
    "\r\n",
    "print('best depth:', best_depth)\r\n",
    "\r\n",
    "# plot accuracy\r\n",
    "plt.title('Model Accuracy vs Maximum Depth')\r\n",
    "plt.xlabel('max depth')\r\n",
    "plt.ylabel('accuracy')\r\n",
    "plt.plot(np.logspace(0, 10, 25), train_scores, c='r', label='train')\r\n",
    "plt.plot(np.logspace(0, 10, 25), val_scores, label='validation')\r\n",
    "plt.xscale('log')\r\n",
    "plt.legend()\r\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "depth: 1.000000   score: 0.643750   auc: 0.761513\n",
      "depth: 2.610157   score: 0.661667   auc: 0.819245\n",
      "depth: 6.812921   score: 0.734583   auc: 0.845244\n",
      "depth: 17.782794   score: 0.776667   auc: 0.866404\n",
      "depth: 46.415888   score: 0.791667   auc: 0.873214\n",
      "depth: 121.152766   score: 0.791250   auc: 0.876837\n",
      "depth: 316.227766   score: 0.786667   auc: 0.873750\n",
      "depth: 825.404185   score: 0.787083   auc: 0.866581\n",
      "depth: 2154.434690   score: 0.774167   auc: 0.872345\n",
      "depth: 5623.413252   score: 0.790000   auc: 0.873649\n",
      "depth: 14677.992676   score: 0.785000   auc: 0.869476\n",
      "depth: 38311.868496   score: 0.790000   auc: 0.871507\n",
      "depth: 100000.000000   score: 0.780417   auc: 0.867265\n",
      "depth: 261015.721568   score: 0.792917   auc: 0.873249\n",
      "depth: 681292.069058   score: 0.783750   auc: 0.867196\n",
      "depth: 1778279.410039   score: 0.784167   auc: 0.868039\n",
      "depth: 4641588.833613   score: 0.785833   auc: 0.868507\n",
      "depth: 12115276.586286   score: 0.788333   auc: 0.872329\n",
      "depth: 31622776.601684   score: 0.779167   auc: 0.868149\n",
      "depth: 82540418.526802   score: 0.793333   auc: 0.877318\n",
      "depth: 215443469.003189   score: 0.777917   auc: 0.869843\n",
      "depth: 562341325.190349   score: 0.787083   auc: 0.870421\n",
      "depth: 1467799267.622074   score: 0.787500   auc: 0.867954\n",
      "depth: 3831186849.557293   score: 0.791667   auc: 0.869723\n",
      "depth: 10000000000.000000   score: 0.788750   auc: 0.871887\n",
      "best depth: 82540418.5268019\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEaCAYAAAAL7cBuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA6DUlEQVR4nO3deXhU5dn48e+dDZIASdiXgGHfVyMuuKBYBK1bpQhqrb4qVWurtlrRt+/P9m1t3aq2rwtia611RTRuZXNDcGGVHUHCloQ1YUkCSch2//44JziESTJJZjJL7s91zZU5y3PmPjOTuc/znHOeR1QVY4wxprqoYAdgjDEmNFmCMMYY45UlCGOMMV5ZgjDGGOOVJQhjjDFeWYIwxhjjlSUIcwIRSRMRFZEYH9a9QUS+aIq4TOOJyDkisjnYcYQKEdkhIhcGO45QZgkijLlf8FIRaV9t/mr3Rz4tSKF5xpIoIkdEZE6wYwllHon5m2rz27uf8Y7GvoaqLlbV/o3dTiC43+ViESkUkcMi8pWI3CoifvmNEpGXROSP/thWc2IJIvxtB6ZWTYjIUCA+eOGcZBJwDBgvIl2a8oV9qQWFoEQRGeIxfQ3OZ9wcXKqqrYFTgIeB+4B/BDek5s0SRPj7N3C9x/RPgZc9VxCRJBF5WURyRWSniPy26shMRKJF5HERyRORbcAlXsr+Q0T2iMguEfmjiETXI76fAjOAtcC11bZ9tnukeFhEskXkBnd+vIj8xY01X0S+cOeNFZGcats43kwgIr8Tkdki8oqIFAA3iMhoEfnafY09IvK0iMR5lB8sIh+JyEER2SciD4hIZxEpEpF2Huud6r5/sdVev6t75NvWY95I9/2MFZE+IvK5ux95IvJmHe/Xv933rMr1nPx5TheRre7R9kYRudJj2XMiMttj+hER+UQcJ7x/7nt3r4isFZGj7ufcSUTmutv+WERS3HV9ee/fct/7QhFZJyL9ROR+Ednvfr7j69h3AFQ1X1XfB64GflqVMEWkhftdzXI/qxkiEu8Zn/v55bmxXesum4bz3fuNOLXZDzxeboS7//ki8qaItPQlxubCEkT4WwK0EZGB7g/31cAr1db5PyAJ6AWch/Ojc6O77Bbgh8BIIB3niN/Tv4ByoI+7znjgZl8CE5EewFjgVfdxfbVlc93YOgAjgNXu4seBU4GzgLbAb4BKX14TuByYDSS7r1kB3A20B84ExgG3uzG0Bj4G5gFd3X38RFX3AguByR7bvQ54Q1XLPF9MVXcDXwNXecy+BpjtrvsHYAGQAqS6+1ubV4ApbuIeCLQGllZbZytwDs5n+nvgFfm+dvZrYJg454fOAW4Cfqo196lzFfADoB9wKc5n8gDO+xUF/LKOeD1dipPgUoBVwHx3G92A/wWer8e2UNVlQA7OvgI84sY5Auez6gb8P48ind24u+Ek2Zki0l9VZ+J8Fx5V1VaqeqlHmcnABKAnMAy4oT4xRjxVtUeYPoAdwIXAb4E/43zRPwJiAAXSgGicJp5BHuV+Bix0n38K3OqxbLxbNgbo5JaN91g+FfjMfX4D8EUt8f0WWO0+74rzYz3Snb4fyPBSJgooBoZ7WTYWyPH2HrjPfwcsquM9u6vqdd19WVXDelcDX7rPo4G9wOga1r0Z+NR9LkA2cK47/TIwE0itI640j/f9Y+AinGaW/3Y/4x21lF0NXO4xPRo4COwEptb0/rnv3bUe028Dz3lM/wJ4tx7v/Uceyy4FjgDR7nRrd/+Sa/sue5m/xH0PBDgK9PZYdiaw3SO+ciDRY/ks4H/c5y8Bf/Tymtd5TD8KzAjE/2q4PqwGERn+jXPUegPVmiNwjqjicH4squzEOcoC54c7u9qyKqcAscAet4nmMM5RYEcf47oe58gNdY60P+f75pPuOEfC1bUHWtawzBee+4LbzPGhiOx1m53+5L5GbTEAvAcMEpFeOEfY+eoc0XozGzhTRLoC5+L8EC52l/0G58dtmYhsEJH/8mEfXsb5LKdycm0QEblenAsRqj6TIR77hBvnNvd1Z9XxWvs8nhd7mW7lQ7w1bStPVSs8pqnn9sD5nh7EqWUmACs99nueO7/KIVU96jG9E+f7XZu9Hs+LGhBfRLMEEQFUdSfOicyLgXeqLc4DynB+7Kv0AHa5z/fg/FB6LquSjVODaK+qye6jjaoOrismETkL6Avc7/447wVOB6aKc/I4G+jtpWgeUFLDsqM4PxJVrxHNiT8Q4Pw4e3oO2AT0VdU2OM0n4rF/3l4HVS3B+XG9FvgJThL2SlUP4zQjTcZJ1K+re0iqqntV9RZV7YpTc3tWRPrUtC3X2zjngra5n+1xInIK8AJwB9BOVZOB9R77hIj8HGgB7MZJUP7gy3vvVyJyGk6C+ALne1EMDPb4LiapqucPeoqIJHpM98B5D+Dk74XxgSWIyHETcEG1IyjcI7hZwEMi0tr9gfkV3x+ZzgJ+KSKp7gnJ6R5l9+D88P1FRNqISJSI9BaR83yI56c4zV2DcNqMR+Ac6SYAE3FqFheKyGQRiRGRdiIyQlUrgReBJ9wTwNEicqaItAC+A1qKyCXinCz+Lc4PYW1aAwXAEREZANzmsexDoLOI3OWeAG0tIqd7LK86kr8ML0fy1byGU2O6yn0OgIj8WERS3clDOD9UFScX/577GV6A93M9ie42ct3t34jzvla9Xj/gjzjnTH6Cc2J2RB2x+6Ih732DuN+1HwJvAK+o6jr3e/EC8KSIdHTX6yYiF1Ur/nsRiXPPv/wQeMudvw/nHJypB0sQEUJVt6rqihoW/wLnCHAbztHYazg/wuD8080H1gDfcHIN5HqcJqqNOD9ws4FaL1d1rwSZDPyfewRd9diOe5WOqmbh1Hh+jdOEsBoY7m7iHmAdsNxd9ggQpar5OCeY/45TAzqKcxKzNvfgHNUXuvt6/CoiVS3EaT66FKepYQtwvsfyL3FOjn+jqjvqeJ33cWpM+1R1jcf804ClInLEXedO932olaquUNWTmr9UdSPwF5wT4/uAocCXcPyy3leAR1R1japuwakx/dtNsA3WwPe+vj4QkUKcmt1/A0/w/cUU4Fz2mgkscZsLPwY87+vYi/Md3Y1zAHKrqm5yl/0Dp8nwsIi86+e4I5a4NWFjjBci8inwmqr+PdixmJqJyFic2kZqHauaegjHG4mMaRJuG/gonEtnjWl2rInJGC9E5F84TRh3uU1RxjQ71sRkjDHGK6tBGGOM8coShDHGGK8i6iR1+/btNS0tLdhhGGNM2Fi5cmWeqnq96TGiEkRaWhorVtR0K4AxxpjqRGRnTcusickYY4xXliCMMcZ4ZQnCGGOMV5YgjDHGeBWwBCEiL7pDDa6vYbmIyN9EJNMd8m+Ux7IJIrLZXTbdW3ljjDGBFcgaxEs4I5zVZCJO75d9gWk4/fZX9TP/jLt8EM74AYMCGKcxxhgvAnaZq6ouEpG0Wla5HHjZHVhliYgku+PqpgGZqroNQETecNfdGKhYTR1U4ehRyMv7/lFaGuyojL+I1P6IijpxWrXmR2XlyfNM4LVoARdVHxqj8YJ5H0Q3ThweMsed522+5yAuJxCRaTg1EHr06FHTaqYmhw/DvHmQm3tiAqh6HDjg/D12LNiRGmNq0qkT7N1b93r1FMwEIV7maS3zvVLVmTiDwpOenm6HK/VRUABjx8Iad3wbEWjbFtq3h3btIC0N0tOd6apHu3bOIz4+mJEbf6mtNlBTzcDXmobnwwRWTGB+yoOZIHI4cSzkVJyRoOJqmG/8qbQUrroKNmyA2bPhvPMgJQWio4MdmTEmRATzMtf3gevdq5nOAPLdMZCXA31FpKeIxAFT3HWNv6jCLbfAxx/DCy84iaJ9e0sOxpgTBKwGISKvA2OB9iKSAzwIxAKo6gxgDs6YxJlAEe7Ys6paLiJ34IyTHA28qKobAhVns/Tgg/Dyy/D738MNNwQ7GmNMiIqoAYPS09PVOuurwwsvwLRpcNNNznNrHzamWRORlaqa7m2Z3UndnMyZA7fdBhMmwHPPWXIwxtTKEkRzsXIlTJ4Mw4fDW29BbGywIzLGhDhLEM3B9u1wySXOiej//AdatQp2RMaYMBBRAwYZLw4cgIkTnctaP/sMOncOdkTGmDBhCSKSFRfD5ZfDjh3w0UcwcGCwIzLGhBFLEJGqshJ+8hP48kuYNQvOOSfYERljwoydg4hUv/41vP02PPEE/PjHwY7GGBOGLEFEoiefhKeegjvvhLvvDnY0xpgwZQki0sye7dQerroK/vKXYEdjjAljliAiSWYmXHcdnHkm/Pvf1reSMaZRLEFEkjfecC5nffNN647bGNNoliAiSUYGnHEGpKYGOxJjTASwBBEpdu6Eb76BK68MdiTGmAhhCSJSvPuu8/eKK4IZhTEmgliCiBQZGTB4MPTtG+xIjDERwhJEJMjNhcWLrXnJGONXliAiwQcfOF1rWIIwxviRJYhIkJEBp5wCI0cGOxJjTASxBBHuCgudnlqvuMJGiDPG+FVAE4SITBCRzSKSKSLTvSxPEZEMEVkrIstEZIjHsh0isk5EVouIDTRdk3nz4Ngxa14yxvhdwLr7FpFo4BngB0AOsFxE3lfVjR6rPQCsVtUrRWSAu/44j+Xnq2peoGKMCBkZzkhxZ58d7EiMMREmkDWI0UCmqm5T1VLgDeDyausMAj4BUNVNQJqIdApgTJGltNQZQvSyy6zfJWOM3wUyQXQDsj2mc9x5ntYAPwIQkdHAKUBVPxEKLBCRlSIyraYXEZFpIrJCRFbk5ub6Lfiw8NlnUFBgzUvGmIAIZILwdsZUq00/DKSIyGrgF8AqoNxdNkZVRwETgZ+LyLneXkRVZ6pquqqmd+jQwT+Rh4uMDGjVCi68MNiRGGMiUCCHHM0BuntMpwK7PVdQ1QLgRgAREWC7+0BVd7t/94tIBk6T1aIAxhteKivhvfdg4kRo2TLY0RhjIlAgaxDLgb4i0lNE4oApwPueK4hIsrsM4GZgkaoWiEiiiLR210kExgPrAxhr+FmyBPbutb6XjDEBE7AahKqWi8gdwHwgGnhRVTeIyK3u8hnAQOBlEakANgI3ucU7ARlOpYIY4DVVnReoWMNSRgbExsIllwQ7EmNMhBLV6qcFwld6erquWNEMbplQdTrl69PHuQ/CGGMaSERWqmq6t2V2J3U4Wr8etm61q5eMMQFlCSIcZWQ43WpcXv22EmOM8R9LEOEoIwPOPBM6dw52JMaYCGYJItxs3w6rV1vzkjEm4CxBhJuqoUUtQRhjAswSRLjJyIChQ6F372BHYoyJcJYgwsn+/fDFF1Z7MMY0CUsQ4eT99517ICxBGGOagCWIcPLuu5CWBsOHBzsSY0wzYAkiXFQNLXrllTa0qDGmSViCCBdz5zoDBFnnfMaYJmIJIlxkZECHDjBmTLAjMcY0E5YgwsGxYza0qDGmyVmCCAeffuqcg7Crl4wxTcgSRDioGlp03LhgR2KMaUYsQYS6igpnaNGLL7ahRY0xTcoSRKj7+mvnDmprXjLGNDFLEKEuIwPi4pwahDHGNCFLEKFM1UkQ48ZBmzbBjsYY08wENEGIyAQR2SwimSIy3cvyFBHJEJG1IrJMRIb4WrZZWLvWGf/BmpeMMUEQsAQhItHAM8BEYBAwVUQGVVvtAWC1qg4Drgf+Wo+yka9qaNHLLgt2JMaYZiiQNYjRQKaqblPVUuANoPogyoOATwBUdROQJiKdfCwb+d5917lzulOnYEdijGmGApkgugHZHtM57jxPa4AfAYjIaOAUINXHsrjlponIChFZkZub66fQQ0BODqxZA5deGuxIjDHNVCAThLcuR7Xa9MNAioisBn4BrALKfSzrzFSdqarpqpreoUOHRoQbYubNc/7a1UvGmCCJCeC2c4DuHtOpwG7PFVS1ALgRQEQE2O4+EuoqG/HmzIHUVBg8ONiRGGOaqUDWIJYDfUWkp4jEAVOA9z1XEJFkdxnAzcAiN2nUWTailZbCxx/DxIk29oMxJmgCVoNQ1XIRuQOYD0QDL6rqBhG51V0+AxgIvCwiFcBG4KbaygYq1pDz1VdO53wTJwY7EmNMMxbIJiZUdQ4wp9q8GR7Pvwb6+lq22Zg7F2JjrXM+Y0xQ2Z3UoWjuXDj7bLt72hgTVJYgQk1ODqxbZ81LxpigswQRaqoub7UEYYwJMksQocYubzXGhAhLEKHELm81xoQQSxChpOryVrt72hgTAixBhBK7vNUYE0IsQYSSqstbW7cOdiTGGGMJImTY5a3GmBBjCSJU2OWtxpgQYwkiVNjlrcaYEGMJIhRUXd568cV2easxJmRYgggF1nurMSYEWYIIBXZ5qzEmBFmCCAV2easxJgRZggg2u7zVGBOiLEEEm13eaowJUT4lCBF5W0QuERFLKP5ml7caY0KUrz/4zwHXAFtE5GERGeBLIRGZICKbRSRTRKZ7WZ4kIh+IyBoR2SAiN3os2yEi60RktYis8DHO8GKXtxpjQphPCUJVP1bVa4FRwA7gIxH5SkRuFJFYb2VEJBp4BpgIDAKmisigaqv9HNioqsOBscBfRCTOY/n5qjpCVdPrs1Nhwy5vNcaEMJ+bjESkHXADcDOwCvgrTsL4qIYio4FMVd2mqqXAG8Dl1dZRoLWICNAKOAiU12cHwppd3mqMCWExvqwkIu8AA4B/A5eq6h530Zu1NP90A7I9pnOA06ut8zTwPrAbaA1craqV7jIFFoiIAs+r6kxfYg0rdnmrMSaE+ZQggKdV9VNvC2pp/vHWqK7Vpi8CVgMXAL1xmq4Wq2oBMEZVd4tIR3f+JlVddNKLiEwDpgH06NHDp50JCVWXtz76aLAjMcYYr3xtYhooIslVEyKSIiK311EmB+juMZ2KU1PwdCPwjjoyge04NRVUdbf7dz+QgdNkdRJVnamq6aqa3qFDBx93JwTY5a3GmBDna4K4RVUPV02o6iHgljrKLAf6ikhP98TzFJzmJE9ZwDgAEekE9Ae2iUiiiLR25ycC44H1PsYaHubMge7d7fJWY0zI8rWJKUpERFUVjl+hFFdbAVUtF5E7gPlANPCiqm4QkVvd5TOAPwAvicg6nCap+1Q1T0R6ARnOuWtigNdUdV4D9i80VV3eOnWqXd5qjAlZviaI+cAsEZmBcx7hVqDOH2xVnQPMqTZvhsfz3Ti1g+rltgHDfYwt/NjlrcaYMOBrgrgP+BlwG86R/gLg74EKKuLZ5a3GmDDgU4JwLz19zn2YxrLLW40xYcDXvpj6ishsEdkoItuqHoEOLiJZ763GmDDh61VM/8SpPZQD5wMv49w0Z+qr6vLWiy8ObhzGGFMHXxNEvKp+Aoiq7lTV3+Hc3Gbqq+ry1kHVu6UyxpjQ4utJ6hK3q+8t7qWru4COgQsrQtnlrcaYMOJrDeIuIAH4JXAqcB3w0wDFFLns8lZjTBipswbh3hQ3WVXvBY7gdI9hGsIubzXGhJE6axCqWgGc6nbJbRrDLm81xoQRX89BrALeE5G3gKNVM1X1nYBEFYms91ZjTJjxNUG0BQ5w4pVLCliC8JVd3mqMCTO+3klt5x0a65NPoFs3u7zVGBM2fB1R7p+cPNgPqvpffo8oUi1dCmedZZe3GmPChq9NTB96PG8JXMnJg/+YmuzfD9u3w89/HuxIjDHGZ742Mb3tOS0irwMfBySiSLR0qfP3jDOCG4cxxtSDrzfKVdcXCKMBoINsyRKIiYFRo4IdiTHG+MzXcxCFnHgOYi/OGBHGF0uXwvDhEB8f7EiMMcZnvjYx2Z1dDVVRAcuWwU9+EuxIjDGmXnwdD+JKEUnymE4WkSsCFlUk2bTJ6X/Jzj8YY8KMr+cgHlTV/KoJVT0MPFhXIRGZICKbRSRTRKZ7WZ4kIh+IyBoR2SAiN/paNmwsWeL8Pf304MZhjDH15GuC8LZerc1Tbid/zwATgUHAVBGpfpfYz4GNqjocGAv8RUTifCwbHpYuhZQU6Ns32JEYY0y9+JogVojIEyLSW0R6iciTwMo6yowGMlV1m6qWAm8Al1dbR4HWbkeArYCDOKPW+VI2PCxZ4tQe7AY5Y0yY8TVB/AIoBd4EZgHFOEf/tekGZHtM57jzPD0NDMS56W4dcKeqVvpYFgARmSYiK0RkRW5urm9701QKC2HDBjv/YIwJS75exXQUqO95AG+HzNW767gIWI3TCWBv4CMRWexj2arYZgIzAdLT072uEzQrVkBlpZ1/MMaEJV+vYvpIRJI9plNEZH4dxXKA7h7TqZzcPceNwDvqyAS2AwN8LBv6qk5Qjx4d3DiMMaYBfG1iau9euQSAqh6i7jGplwN9RaSniMQBU4D3q62TBYwDEJFOQH9gm49lQ9/SpdCvH7RtG+xITBgpLa/k4437yDpQFOxQwsLBo6U8Om8Tn38XYk3MEcDXzvoqRaSHqmYBiEgaNTT5VFHVchG5A5gPRAMvquoGEbnVXT4D+APwkoisw2lWuk9V89zXOKlsvfcumFSdGsRFFzXZS67NOczzn2/jWHklMVFCTLQQGx1FdJQQGy3ERHk8j45y1omKIik+hsHdkhjUpQ2JLXz9Shh/U1Xmrt/Lo/M2scNNDuf0bc+1p/dg3MBOxEY3tGecyKSqzF6Zw5/mfMuhojKeXbiVq9O789sfDqR1y9hgh+ez0vJKtuYe4ds9BXy7p4Csg0V0aN2C1JQEuqck0L1tPKkpCaQkxNLUA3uKat3N9iIyAaed/3N31rnANFWtq5mpSaWnp+uKFSuCHYZj505IS4NnnoHbbw/oS5WUVfDUx1uYuWgryQlxdG7TkvLKSsorlPJKpbyikjL3rzOtlFdWUlZx4mcfJdC7QyuGdktiaGoSQ7slMahrGxLiQj9p5BeV8ej8TeQdOcbIHimM7J7M0NQkv8ReXlHJpr2FrMo+zKqsQxwuKuOKkd2YMLgzcTH++dFetv0gf5rzLauzD9O/U2t+Oa4vmfuP8ObyLHbnl9ChdQsmp6cy5bQedG+b4JfXDGeZ+wt5IGM9y7Yf5NRTUnjw0kHMWbeXmYu20iUpnkcnDWNMn/bBDvMkeUeOHU8Em/YUsnFPAVtzjxz/X4yLiaJ7Sjx5R0rJLy47oWxiXDTd2yaQmuIkjO5tE+h+/Hl8g5OiiKxU1XSvy3xJEO5GOgLTcE4qtwT2q+qiBkUUICGVIN58E6ZMgZUrA9pJ34odB/nN22vZlnuUKad15/6LB5IU7/sXpaJSyTtyjHU5+azblc/6Xfms3ZVPbuExwEkafTq2Ykg3J2EMS01iUJck4uOij29DVY8nnrLKSircv+UVSkWlUlZRSXSU0KNtQkCOgBZvyeXet9aSd+QYXZPjyTroHH1HRwkDOrdmZI9kRnZPYWSPZHq2T6wzhv0FJXyTdZhV2YdYlXWYdTn5FJdVANC+VRwtYqLZdbiYDq1bMHV0D64Z3YPOSS0bFHvm/iM8Mm8TH23cR6c2Lfj1+P5cNSqV6CgnxopKZeHm/by+LItPN+1HgXP7dmDq6B6MG9gxJGoV5RWV7C88xp78Evbml7Anv9j5W+BMl5RVcPHQLvw4PZWOrRv2PlUpKavg6U8zeX7RVhLiYrh/4gAmp3cnyn2/vsk6xD2z1rAt7yjXndGD+ycO9FutuLi0gp0Hj55w4FX9e1914FX1vS+rUHYeOMrGPQV8u6eQvCPHjm+vc5uWDOjSmoFd2jiPzq3p2T6RGPczLSgpI+dgMdmHisg5VEz2wSJyDhWR7c4rKq04vq02LWNY+7uGtVY0OkGIyM3AnTgni1cDZwBfq+oFtZVraiGVIH71K3juOSgogFj/V3eLSst5bP5mXvpqB12T4nn4qqGc07eD37a/r6DkeNKoengmjYS4GMrcf5CKSt8OMs7o1ZbfXjKIId2S6l7ZB8WlFTw891v+9fVO+nRsxZOTRzA0NYkDR46xOvswq9wf+TXZ+Rw5Vg5AckIsI7p/nzAGdW3DzgNFrMo6xKrsw6zOOsyuw8UAxEYLg7smOQnGrZWkpsSjCp9vyeXlr3aw8LtcokS4aHAnrj8zjdN7tvUpCe4vLOGpj7fw5vJs4mOjuW1sb/5rTM8TEm91uw8X8+bybN5cns3eghI6tm7B1ad15+rTupOa4t9axbHyCvKLyjhUVMbhotIT/u5zf/idBFBMbuExqn8FWsZG0TUpns5JLSktr2TFzkPERAnjB3di6ugejOnd/viPuq8+/y6X/3l3PVkHi/jRyG48cMlA2rdqcdJ6JWUVPDZ/My9+uZ3uKQk8NmkYp/dq1+D34ts9Bby+LIuMb3ZR6H6P6iMuJop+nVoxoPP3iWBAlza0TYxrcEyqyqGiMjdpFHO0tJzJ6d3rLuiFPxLEOuA0YImqjhCRAcDvVfXqBkUUICGVIM46C6KjYfFiv2/6q8w87ntnLdkHi/npmafwmwkDmuTcgWfSOHKsnJhoOX4eIzZaiHb/xkQJ0dFRxEaJe84jiv2FJcz4fBuHikq5alQq917Un05tGn40uSb7MHfPWs223KPcOCaN+yYMoGWs9x/Xikpla+4Rvtl56HjS2LL/CNW/+t2S479PBj2SGdSlTY3brLLzwFFeXZrFm8uzyS8uo1+nVlx/ZhpXjuzm9TM5eqycmYu28cLibZSWV3LdGafwiwv60M7LD11NyisqWbg5l9eWZbFws1OrOK9fBy4d1pW4mKjjR681HeV6NjcWlZYf//E/XFTG4aIyDhWVnnB0Wl1iXDRdkuPpktSSzm1aOn+T3OkkZzop/sT28q25R3hjWRazV+ZwqKiMU9olMOW0Hkw6NZUOrWvf9/0FJfzvhxv5cO0eerVP5I9XDuGs3nU3Hy3bfpB7Z68h62ARN57Vk3sv6l9rAvZUXFrBh2t389qyLFZlHSYuJopLhnbhggEdaRETdfzcXox7bi8mWoj1do4vWujQqsXxWkEo8keCWK6qp4nIauB0VT0mIqtVdYR/Q22ckEkQpaXQpg3ccQc8/rjfNltYUsaf527itaVZpLVL4JGrGndk1NTyi8t49rNM/vnlDmKihVvP680t5/Ty+Z8WoKyikmc+y+T/Ps2kY+sWPP7j4Q1qay4sKWNtTj7f7imge9sERnZPpmMjElZxaQUfrNnNv77ewYbdBbRuEcNVp6Zy3Rmn0KdjK8oqKnlzeTZPfbyFvCPHuGRYF+4d35+09okNfk2AXcdrFVnsKzhWdwFX1Y9XfGw0KQlxJCfEkuz+TUmIIzk+luTEOFISYkmOd+cnOvMbczBSUlbB/A17eW1pFku3HyQ2Whg/qDPXnN6DM3u1O6FWUVGpvLp0J4/N28yxikruOL8PPzuvFy1ifP++FJWW8/DcTbz89U56tU/ksR8P59RTUmpcf/PeQl5bupN3Vu2isKSc3h0Sueb0U7hqVDeSExp+xB/K/JEgMnDuWbgL56a2Q0Csql7sxzgbLWQSxPLlzr0Pb70Fkyb5ZZOfbd7PA++sY19BCTef04u7L+xXrx/WULLzwFEenruJuev30iWpJb+Z0J/Lh3ers8lha+4RfvXmatbk5HPlyG787rLB9Trf0hRUlW+yDvPvr3fwn3V7KKtQxvRpx578ErblHmV0Wlvuv3gAI3vU/CPVEOUVlWzLO0qUQHTU90ev39funJpcjFura+qrYbzJ3H+E15dl8fY3ORwuKiOtXQJTRju1ir35Jfx3xjrW5ORzdp/2/OGKIfRsRDL9KjOPe2evZU9+Mbec6/z/VNUOS8oq+HDtHl5flsXKnYeIi4ni4iGdmTq6B6N9bDIMZ345Se2xsfOAJGCe209SyAiZBPH00/CLX0BWFnRvWLtglcNFpfzhw295+5sc+nZsxaOThvn9xyVYlm0/yB8+3Mi6XfkMT03if344iPS0k+8ZUVX+vWQnf5rzLS1jo3noiqFcMqxLECKun9zCY7y5PIvXl2XTqkUM917Un3EDO0b8D059lZRVMG/9Xl5blsUyt1ZRUam0TWzB//xwIJcN7+qX96ywpIw/zdnE68uy6NuxFfdc1J+vtx7gnW9yKCgpp1eHRK4Z3YOrRqWS0ojzA+HGrwkilIVMgrjuOvjsM8jJaVQnfZ9u2sd9b6/j4NFSbh/bmzsu6FOv6nU4qKxUMlbt4tH5m9hXcIxLhnZh+sQBxy/l3Jtfwr2z17B4Sx7n9evAo5OGNerchQltmfsLeXN5NlFRwu1j+wSkhvj5d7ncN3stewtKiIuOYsIQp4nL1wsMIo0liKbWty8MHQrvvNPgTeQcKuKCxz+nV4dEHv/xcL9d+ROqikrLef7zbTy/aCuVlXDj2Wn069ia//1wI6Xllfz3JQO59vQezfIf2PhffnEZX2XmcXqvdo26migS1JYgQv8OqHCTlweZmXDLLY3azBMLvkME/nnjaXRJivyxrBPiYrj7B/2YOroHj87fxPOfbwNgRPdknrx6RKPan42pLik+lolDQ7+ZMtgsQfjbsmXO30b04Lphdz4Zq3fxs3N7N4vk4KlzUkuemDyCG8/qycY9+Vw1KjWkLxE0JpJZgvC3JUsgKgrSvdbYfPLw3E0kxcdy29jefgwsvAxNdbr7MMYEjx2a+dvSpc75h8SGNYl8sSWPxVvyuOP8wJygM8YYX1mC8KfKSidBNHAEucpK5c9zv6Vbcjw/OfMUPwdnjDH1YwnCn777DvLzG3z+4YO1u9mwu4B7LuoXcZezGmPCjyUIf6oaQa4BNYhj5U4HY4O6tOHy4V6H3zbGmCZlCcKfli6FpCTo37/eRV9ZkkXOoWKmTxxQ714ujTEmECxB+NOSJU4fTFH1e1sLSsp4+tMtnN2nPef281+X3cYY0xiWIPzl6FFYt65B5x9mLNzKoaIypk8cEIDAjDGmYSxB+MvKlVBRUe/zD3vzS3jxy+1cPqJrxHenYYwJLwFNECIyQUQ2i0imiEz3svxeEVntPtaLSIWItHWX7RCRde6yEOhgqQ5Llzp/R4+uV7EnP/qOykq4Z3z9z1sYY0wgBexOahGJBp4BfgDkAMtF5H1V3Vi1jqo+Bjzmrn8pcLeqHvTYzPmqmheoGP1qyRLo3Rs6+H4OYcu+Qt5amc0NZ/W0geiNMSEnkDWI0UCmqm5zx414A7i8lvWnAq8HMJ7AWrq03ucfHpm3icS4GO64oE+AgjLGmIYLZILoBmR7TOe4804iIgnABOBtj9kKLBCRlSIyraYXEZFpIrJCRFbk5ub6IewGyMmBXbvqdf5h2faDfPztfm4d27vZdzdsjAlNgUwQ3i7mr2nwiUuBL6s1L41R1VHARODnInKut4KqOlNV01U1vUM9mnf8qur8g481CFWnS43ObVryX2N6BjAwY4xpuEAmiBzAc7zNVGB3DetOoVrzkqrudv/uBzJwmqxC05Il0KIFjBjh0+rz1u9lVdZh7v5B37AdV9oYE/kCmSCWA31FpKeIxOEkgferryQiScB5wHse8xJFpHXVc2A8sD6AsTbO0qUwciTE1d1UVFZRyaPzN9O3YyuuGpXaBMEZY0zDBCxBqGo5cAcwH/gWmKWqG0TkVhG51WPVK4EFqnrUY14n4AsRWQMsA/6jqvMCFWujlJXBihU+n394Y3k22/OOct+EATYQjjEmpAV0wCBVnQPMqTZvRrXpl4CXqs3bBgwPZGx+s349FBf7dP7h6LFy/vrxFkantWXcwI5NEJwxxjScHcI2Vj16cH1h8Tbyjhxj+sUDELEO+Ywxoc0SRGMtXQodO8IptQ/wk1t4jJmLtjFxSGdG9UhpouCMMabhLEE01pIlTu2hjhrBC4u3cay8knsvsi41jDHhwRJEYxw6BJs313n+QVX5z9o9jO3XgV4dWjVRcMYY0ziWIBpj2TLnbx3nHzbsLmDX4WIuGty5CYIyxhj/sATRGEuXOk1L6em1rrZg4z6iBLtyyRgTVixBNMaSJTB4MLRpU+tqCzbsJT2tLe1atWiiwIwxpvEsQTSUqk89uGYdKGLT3kLGD+rURIEZY4x/WIJoqMxMOHiwzvMPCzbuBWD8IDv/YIwJL5YgGsrHHlwXbNjHgM6t6dHOBgQyxoQXSxANtWQJtGoFgwbVuErekWOs2HnQrl4yxoQlSxANtXQpnHYaRNfcXfcn3+6jUmH8YDv/YIwJP5YgGqK4GFavrvv8w4Z9dEuOZ1CX2q9yMsaYUGQJoiE++ADKy2s9/3D0WDmLM/MYP7iTdcxnjAlLliDqa/FiuOEGZ4Cg8eNrXG3Rd7mUllfa+QdjTNiyBFEfq1bBD38IPXrA/PkQH1/jqvM37CUlIZb0U6znVmNMeLIE4avNm+GiiyA5GT76CDp0qHHVsopKPtm0n3EDO9moccaYsGW/Xr7IyoIf/MDpd+mjj6B791pXX7rtIIUl5Xb3tDEmrAV0yNGIsH+/kxwKCmDhQujXr84iCzbupWVsFOf0rbmWYYwxoS6gNQgRmSAim0UkU0Sme1l+r4isdh/rRaRCRNr6UrZJ5OfDhAmQnQ3/+Q+MGFFnkcpKZcGGfZzXrwPxcTXfI2GMMaEuYAlCRKKBZ4CJwCBgqoiccNuxqj6mqiNUdQRwP/C5qh70pWzAFRU5J6TXr4d33oExY3wqtm5XPnsLSqzvJWNM2AtkDWI0kKmq21S1FHgDuLyW9acCrzewrH+VlsKkSfDll/DKK04twkcLNu4lOkq4YICN/WCMCW+BTBDdgGyP6Rx33klEJAGYALzdgLLTRGSFiKzIzc1tdNBUVMD118PcufD88zB5cr2KL9iwj9FpbUlJjGt8LMYYE0SBTBDebh/WGta9FPhSVQ/Wt6yqzlTVdFVN71DLpac+UYXbb4c334RHH4VbbqlX8W25R9iy/wgXWd9LxpgIEMgEkQN4Xg+aCuyuYd0pfN+8VN+y/nP//TBzpvP33nvrXXzBxn0A/MDunjbGRIBAXua6HOgrIj2BXThJ4JrqK4lIEnAecF19y/rVI484j9tug4ceatAmFmzYy5BubeiWXPMd1sYY35SVlZGTk0NJSUmwQ4kILVu2JDU1ldjYWJ/LBCxBqGq5iNwBzAeigRdVdYOI3Ooun+GueiWwQFWP1lU2ULHy/PMwfTpMnQpPP+3cEFdP+wtKWJV9mLsvrPs+CWNM3XJycmjdujVpaWnW4WUjqSoHDhwgJyeHnj17+lwuoDfKqeocYE61eTOqTb8EvORL2YA4cADuuw8uuQT+9S+Ialir20ff7kMV65zPGD8pKSmx5OAnIkK7du2o74U8did1u3ZOD619+kA9ql7VLdiwj1PaJdCvUys/BmdM82bJwX8a8l5aX0wAQ4fW2jNrXQpLyvhqax7jB9nYD8aYyGEJwg8Wbs6lrEIZb81LxkSMw4cP8+yzz9a73MUXX8zhw4f9H1AQWILwg/kb9tK+VRyjetjYD8ZEipoSREVFRa3l5syZQ3JycoCialp2DqKRjpVXsHBzLj8c1oXoKGteMiZSTJ8+na1btzJixAhiY2Np1aoVXbp0YfXq1WzcuJErrriC7OxsSkpKuPPOO5k2bRoAaWlprFixgiNHjjBx4kTOPvtsvvrqK7p168Z7771HfCOas5uaJYhG+nrrAY4cK2e83T1tTODcdResXu3fbY4YAU89VePihx9+mPXr17N69WoWLlzIJZdcwvr1649fJvriiy/Stm1biouLOe2007jqqqto167dCdvYsmULr7/+Oi+88AKTJ0/m7bff5rrrrvP2ciHJEkQjLdi4j8S4aM7q3T7YoRhjAmj06NEn3EPwt7/9jYyMDACys7PZsmXLSQmiZ8+ejHCHCTj11FPZsWNHU4XrF5YgGqGyUvlo4z7G9u9Iy1gb+8GYgKnlSL+pJCYmHn++cOFCPv74Y77++msSEhIYO3as1zu+W7Rocfx5dHQ0xcXFTRKrv9hJ6kZYlX2Y3MJj1rxkTARq3bo1hYWFXpfl5+eTkpJCQkICmzZtYsmSJU0cXdOwGkQjLNi4l5goYWx/G/vBmEjTrl07xowZw5AhQ4iPj6dTp+8PBCdMmMCMGTMYNmwY/fv354wzzghipIFjCaKBVJ2hRc/s3Y6k+IbfgW2MCV2vvfaa1/ktWrRg7ty5XpdVnWdo374969evPz7/nnvu8Xt8gWZNTA2Uuf8I2/OO2s1xxpiIZQmigY6P/TDQzj8YYyKTJYgGWrBhL8O7J9M5qWWwQzHGmICwBNEA2/OOsiYnn/GDrPZgjIlcliDq6auteUx67isS46K5bHjXYIdjjDEBYwnCR6rKzEVb+ck/lpGcEMt7d4yhe9uEYIdljDEBYwnCB0eOlXPHa6v405xNjB/UiffuOJs+HVsHOyxjTAhp1coZLGz37t1MmjTJ6zpjx45lxYoVtW7nqaeeoqio6Ph0MLsPD2iCEJEJIrJZRDJFZHoN64wVkdUiskFEPveYv0NE1rnLan9HAyhz/xGueOZL5q7fw/0TB/DstaNo1cJuHzHGeNe1a1dmz57d4PLVE0Qwuw8PWIIQkWjgGWAiMAiYKiKDqq2TDDwLXKaqg4EfV9vM+ao6QlXTAxVnbeat38MVz3zJoaOlvHLT6fzsvN42YpwxzcR99913wngQv/vd7/j973/PuHHjGDVqFEOHDuW99947qdyOHTsYMmQIAMXFxUyZMoVhw4Zx9dVXn9AX02233UZ6ejqDBw/mwQcfBJwOAHfv3s3555/P+eefDzjdh+fl5QHwxBNPMGTIEIYMGcJTbv9UO3bsYODAgdxyyy0MHjyY8ePH+63Pp0AeCo8GMlV1G4CIvAFcDmz0WOca4B1VzQJQ1f0BjMdn5RWVPLZgM89/vo3h3ZN57tpRdE0Onz7cjYk0v/9gAxt3F/h1m4O6tuHBSwfXuHzKlCncdddd3H777QDMmjWLefPmcffdd9OmTRvy8vI444wzuOyyy2o8cHzuuedISEhg7dq1rF27llGjRh1f9tBDD9G2bVsqKioYN24ca9eu5Ze//CVPPPEEn332Ge3bn9hD9MqVK/nnP//J0qVLUVVOP/10zjvvPFJSUgLWrXggm5i6Adke0znuPE/9gBQRWSgiK0Xkeo9lCixw508LYJwnOHDkGNe/uIznP9/GNaf3YNbPzrDkYEwzNHLkSPbv38/u3btZs2YNKSkpdOnShQceeIBhw4Zx4YUXsmvXLvbt21fjNhYtWnT8h3rYsGEMGzbs+LJZs2YxatQoRo4cyYYNG9i4cWNNmwHgiy++4MorryQxMZFWrVrxox/9iMWLFwOB61Y8kDUIbylVvbz+qcA4IB74WkSWqOp3wBhV3S0iHYGPRGSTqi466UWc5DENoEePHo0KeHX2YW57ZSUHjpby6KRhTE7v3qjtGWP8o7Yj/UCaNGkSs2fPZu/evUyZMoVXX32V3NxcVq5cSWxsLGlpaV67+fbkrXaxfft2Hn/8cZYvX05KSgo33HBDndtRrf7z+b1AdSseyBpEDuD5C5sK7PayzjxVPaqqecAiYDiAqu52/+4HMnCarE6iqjNVNV1V0zt06NCgQFWV15ZmMXnG10SJ8M5tZ1lyMMYwZcoU3njjDWbPns2kSZPIz8+nY8eOxMbG8tlnn7Fz585ay5977rm8+uqrAKxfv561a9cCUFBQQGJiIklJSezbt++Ejv9q6mb83HPP5d1336WoqIijR4+SkZHBOeec48e9PVkgaxDLgb4i0hPYBUzBOefg6T3gaRGJAeKA04EnRSQRiFLVQvf5eOB/AxFkSVkF/++99cxakcO5/Trw16tHkJIYF4iXMsaEmcGDB1NYWEi3bt3o0qUL1157LZdeeinp6emMGDGCAQMG1Fr+tttu48Ybb2TYsGGMGDGC0aOd49zhw4czcuRIBg8eTK9evRgzZszxMtOmTWPixIl06dKFzz777Pj8UaNGccMNNxzfxs0338zIkSMDOkqd1FZtafTGRS4GngKigRdV9SERuRVAVWe469wL3AhUAn9X1adEpBdOrQGcJPaaqj5U1+ulp6drXdcYV1dUWs6Pnv2K8YM6ceeF/YiOsquUjAkF3377LQMHDgx2GBHF23sqIitrulI0oBf0q+ocYE61eTOqTT8GPFZt3jbcpqZAS4iL4d2fj7EhQ40xphq7kxosORhjjBeWIIwxISuQTeDNTUPeS0sQxpiQ1LJlSw4cOGBJwg9UlQMHDtCyZf3Gr7FOhYwxISk1NZWcnBxyc3ODHUpEaNmyJampqfUqYwnCGBOSYmNj6dmzZ7DDaNasickYY4xXliCMMcZ4ZQnCGGOMVwG9k7qpiUguUHvnKKGnPZAX7CCamO1z82D7HB5OUVWvHdlFVIIIRyKyIlgDIgWL7XPzYPsc/qyJyRhjjFeWIIwxxnhlCSL4ZgY7gCCwfW4ebJ/DnJ2DMMYY45XVIIwxxnhlCcIYY4xXliCMMcZ4ZQkixIhILxH5h4jMDnYsTUVEBorIDBGZLSK3BTuepiAiY0VksbvfY4MdT6CJyDnuvv5dRL4KdjxNQUQGicgsEXlORCYFO56GsATRBETkRRHZLyLrq82fICKbRSRTRKaDM9yqqt4UnEj9p577/K2q3gpMBsL2JqP67DOgwBGgJZDT1LH6Qz0/48XuZ/wh8K9gxOsP9fyMJwL/p6q3Adc3ebD+oKr2CPADOBcYBaz3mBcNbAV6AXHAGmCQx/LZwY67KfcZuAz4Crgm2LE3xT4DUe7yTsCrwY69KT5jd/ksoE2wY2+iz7gj8AzwGPBlsGNvyMNqEE1AVRcBB6vNHg1kqlNjKAXeAC5v8uACpL77rKrvq+pZwLVNG6n/1GefVbXSXX4IaNGEYfpNfT9jEekB5KtqQdNG6j/1/Iz3q+rPgemEX/9MgA0YFEzdgGyP6RzgdBFpBzwEjBSR+1X1z0GJLjBq2uexwI9wfijnNH1YAVXTPv8IuAhIBp4OQlyB4nV/3ec3Af9s8ogCr6bPOA14AEjEqUWEHUsQwSNe5qmqHgBubepgmkhN+7wQWNi0oTSZmvb5HeCdpg6mCXjdXwBVfbCJY2kqNX3GO4BpTRyLX1kTU/DkAN09plOB3UGKpanYPkf+Pje3/YUI3mdLEMGzHOgrIj1FJA6YArwf5JgCzfY58ve5ue0vRPA+W4JoAiLyOvA10F9EckTkJlUtB+4A5gPfArNUdUMw4/Qn2+fI3+fmtr/Q/PbZOuszxhjjldUgjDHGeGUJwhhjjFeWIIwxxnhlCcIYY4xXliCMMcZ4ZQnCGGOMV5YgjGlCIrJQRBrUpbmIXCEig/yxLWN8YQnCmPBxBU430sY0CUsQptkSkTQR2eSOcrZeRF4VkQtF5EsR2SIio931RovIVyKyyv3b353/KxF50X0+1N1GQrXXiBeRN0RkrYi8CcR7LBsvIl+LyDci8paItHLn7xCRR0RkmfvoIyJn4YyZ8ZiIrBaR3u5mfuyu852InBP4d800J5YgTHPXB/grMAwYAFwDnA3cg9NVM8Am4FxVHQn8P+BP7vyngD4iciVON9Y/U9Wiatu/DShS1WE43bifCiAi7YHfAheq6ihgBfArj3IFqjoapyvwp1T1K5z+fe5V1RGqutVdL8Zd7y4gUntLNUFi3X2b5m67qq4DEJENwCeqqiKyDkhz10kC/iUifXG6ro4FUNVKEbkBWAs8r6pfetn+ucDf3PXXishad/4ZOM1FX4oIOCORfe1R7nWPv0/WEn9Vl+ErPeI1xi8sQZjm7pjH80qP6Uq+///4A/CZql7pDgKz0KNMX5yxpbvW8hreOjwT4CNVnepDmdo6TKuKtwL7fzZ+Zk1MxtQtCdjlPr+haqaIJOE0T50LtBORSV7KLsIdRlVEhuA0ZQEsAcaISB93WYKI9PMod7XH36qaRSHQurE7Y4yvLEEYU7dHgT+LyJc4A9RXeRJ4VlW/wxlO82ER6Vit7HNAK7dp6TfAMgBVzcVJNq+7y5bgnAOp0kJElgJ3Ane7894A7nVPlvfGmACz7r6NCTEisgNIV9WwHOjeRA6rQRhjjPHKahDGGGO8shqEMcYYryxBGGOM8coShDHGGK8sQRhjjPHKEoQxxhivLEEYY4zx6v8DhmibV8J1X/0AAAAASUVORK5CYII="
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# find best number of leaves\r\n",
    "train_scores = list()\r\n",
    "train_aucs = list()\r\n",
    "\r\n",
    "val_scores = list()\r\n",
    "val_aucs = list()\r\n",
    "\r\n",
    "best_max_leaves = 1\r\n",
    "best_score = 0\r\n",
    "\r\n",
    "kf = KFold(shuffle=True)\r\n",
    "for n in np.logspace(1, 4, 25):\r\n",
    "    model = RandomForestClassifier(max_leaf_nodes=int(n))\r\n",
    "    \r\n",
    "    k_train_scores = list()\r\n",
    "    k_train_aucs = list()\r\n",
    "    \r\n",
    "    k_val_scores = list()\r\n",
    "    k_val_aucs = list()\r\n",
    "    \r\n",
    "    for train_index, val_index in kf.split(X):\r\n",
    "        x_train, x_val = X[train_index], X[val_index]\r\n",
    "        y_train, y_val = y[train_index], y[val_index]\r\n",
    "        model.fit(x_train, y_train)\r\n",
    "        \r\n",
    "        train_probas = model.predict_proba(x_train)[:,1]\r\n",
    "        val_probas = model.predict_proba(x_val)[:,1]\r\n",
    "                \r\n",
    "        k_train_scores.append(model.score(x_train, y_train))\r\n",
    "        k_train_aucs.append(roc_auc_score(y_train, train_probas))\r\n",
    "        \r\n",
    "        k_val_scores.append(model.score(x_val, y_val))\r\n",
    "        k_val_aucs.append(roc_auc_score(y_val, val_probas))\r\n",
    "\r\n",
    "    if np.mean(k_val_scores) > best_score:\r\n",
    "        best_max_leaves = n\r\n",
    "        best_score = np.mean(k_val_scores)\r\n",
    "\r\n",
    "    train_scores.append(np.mean(k_train_scores))\r\n",
    "    train_aucs.append(np.mean(k_train_aucs))\r\n",
    "    \r\n",
    "    val_scores.append(np.mean(k_val_scores))\r\n",
    "    val_aucs.append(np.mean(k_val_aucs))\r\n",
    "    \r\n",
    "    print('leaves: {:2f}   score: {:5f}   auc: {:5f}'.format(\r\n",
    "        n,\r\n",
    "        np.mean(k_val_scores), \r\n",
    "        np.mean(k_val_aucs)))\r\n",
    "\r\n",
    "print('best depth:', best_depth)\r\n",
    "\r\n",
    "# plot accuracy\r\n",
    "plt.title('Model Accuracy vs Maximum Leaf Nodes')\r\n",
    "plt.xlabel('maximum leaf count')\r\n",
    "plt.ylabel('accuracy')\r\n",
    "plt.plot(np.logspace(1, 4, 25), train_scores, c='r', label='train')\r\n",
    "plt.plot(np.logspace(1, 4, 25), val_scores, label='validation')\r\n",
    "plt.xscale('log')\r\n",
    "plt.legend()\r\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "leaves: 10.000000   score: 0.764583   auc: 0.859142\n",
      "leaves: 13.335214   score: 0.780417   auc: 0.859106\n",
      "leaves: 17.782794   score: 0.769583   auc: 0.857200\n",
      "leaves: 23.713737   score: 0.782500   auc: 0.865271\n",
      "leaves: 31.622777   score: 0.773750   auc: 0.861724\n",
      "leaves: 42.169650   score: 0.783333   auc: 0.866246\n",
      "leaves: 56.234133   score: 0.792083   auc: 0.870039\n",
      "leaves: 74.989421   score: 0.795833   auc: 0.874228\n",
      "leaves: 100.000000   score: 0.800417   auc: 0.872481\n",
      "leaves: 133.352143   score: 0.794583   auc: 0.873886\n",
      "leaves: 177.827941   score: 0.797917   auc: 0.876149\n",
      "leaves: 237.137371   score: 0.798750   auc: 0.880829\n",
      "leaves: 316.227766   score: 0.790000   auc: 0.877296\n",
      "leaves: 421.696503   score: 0.805000   auc: 0.881182\n",
      "leaves: 562.341325   score: 0.787500   auc: 0.870716\n",
      "leaves: 749.894209   score: 0.792917   auc: 0.871069\n",
      "leaves: 1000.000000   score: 0.788333   auc: 0.870885\n",
      "leaves: 1333.521432   score: 0.781250   auc: 0.869086\n",
      "leaves: 1778.279410   score: 0.780000   auc: 0.865306\n",
      "leaves: 2371.373706   score: 0.788333   auc: 0.868082\n",
      "leaves: 3162.277660   score: 0.782500   auc: 0.868884\n",
      "leaves: 4216.965034   score: 0.784167   auc: 0.871116\n",
      "leaves: 5623.413252   score: 0.780417   auc: 0.869547\n",
      "leaves: 7498.942093   score: 0.777917   auc: 0.868797\n",
      "leaves: 10000.000000   score: 0.788333   auc: 0.876230\n",
      "best depth: 316.22776601683796\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "# find best number of estimators\r\n",
    "train_scores = list()\r\n",
    "train_aucs = list()\r\n",
    "\r\n",
    "val_scores = list()\r\n",
    "val_aucs = list()\r\n",
    "\r\n",
    "best_num_estimators = 1\r\n",
    "best_score = 0\r\n",
    "\r\n",
    "kf = KFold(shuffle=True)\r\n",
    "for n in [int(i) for i in np.logspace(0, 3, 25)]:\r\n",
    "    model = RandomForestClassifier(n_estimators=n)\r\n",
    "    \r\n",
    "    k_train_scores = list()\r\n",
    "    k_train_aucs = list()\r\n",
    "    \r\n",
    "    k_val_scores = list()\r\n",
    "    k_val_aucs = list()\r\n",
    "    \r\n",
    "    for train_index, val_index in kf.split(X):\r\n",
    "        x_train, x_val = X[train_index], X[val_index]\r\n",
    "        y_train, y_val = y[train_index], y[val_index]\r\n",
    "        model.fit(x_train, y_train)\r\n",
    "        \r\n",
    "        train_probas = model.predict_proba(x_train)[:,1]\r\n",
    "        val_probas = model.predict_proba(x_val)[:,1]\r\n",
    "                \r\n",
    "        k_train_scores.append(model.score(x_train, y_train))\r\n",
    "        k_train_aucs.append(roc_auc_score(y_train, train_probas))\r\n",
    "        \r\n",
    "        k_val_scores.append(model.score(x_val, y_val))\r\n",
    "        k_val_aucs.append(roc_auc_score(y_val, val_probas))\r\n",
    "\r\n",
    "    if np.mean(k_val_scores) > best_score:\r\n",
    "        best_num_estimators = n\r\n",
    "        best_score = np.mean(k_val_scores)\r\n",
    "\r\n",
    "    train_scores.append(np.mean(k_train_scores))\r\n",
    "    train_aucs.append(np.mean(k_train_aucs))\r\n",
    "    \r\n",
    "    val_scores.append(np.mean(k_val_scores))\r\n",
    "    val_aucs.append(np.mean(k_val_aucs))\r\n",
    "    \r\n",
    "    print('estimators: {:1f}   score: {:5f}   auc: {:5f}'.format(\r\n",
    "        n,\r\n",
    "        np.mean(k_val_scores), \r\n",
    "        np.mean(k_val_aucs)))\r\n",
    "\r\n",
    "print('best estimators:', best_num_estimators)\r\n",
    "\r\n",
    "# plot accuracy\r\n",
    "plt.title('Model Accuracy vs Number of Estimators')\r\n",
    "plt.xlabel('number of estimators')\r\n",
    "plt.ylabel('accuracy')\r\n",
    "plt.plot([int(i) for i in np.logspace(0, 3, 25)], train_scores, c='r', label='train')\r\n",
    "plt.plot([int(i) for i in np.logspace(0, 3, 25)], val_scores, label='validation')\r\n",
    "plt.xscale('log')\r\n",
    "plt.legend()\r\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "estimators: 1.000000   score: 0.677500   auc: 0.676481\n",
      "estimators: 1.000000   score: 0.677917   auc: 0.677725\n",
      "estimators: 1.000000   score: 0.674167   auc: 0.674361\n",
      "estimators: 2.000000   score: 0.672083   auc: 0.721951\n",
      "estimators: 3.000000   score: 0.701250   auc: 0.750349\n",
      "estimators: 4.000000   score: 0.715833   auc: 0.775735\n",
      "estimators: 5.000000   score: 0.712917   auc: 0.780488\n",
      "estimators: 7.000000   score: 0.745833   auc: 0.812294\n",
      "estimators: 10.000000   score: 0.753333   auc: 0.827816\n",
      "estimators: 13.000000   score: 0.752083   auc: 0.836176\n",
      "estimators: 17.000000   score: 0.772500   auc: 0.850722\n",
      "estimators: 23.000000   score: 0.769167   auc: 0.847285\n",
      "estimators: 31.000000   score: 0.772500   auc: 0.859855\n",
      "estimators: 42.000000   score: 0.779583   auc: 0.862941\n",
      "estimators: 56.000000   score: 0.774583   auc: 0.860318\n",
      "estimators: 74.000000   score: 0.781250   auc: 0.871819\n",
      "estimators: 100.000000   score: 0.778333   auc: 0.869617\n",
      "estimators: 133.000000   score: 0.788333   auc: 0.870789\n",
      "estimators: 177.000000   score: 0.784167   auc: 0.871966\n",
      "estimators: 237.000000   score: 0.786250   auc: 0.873060\n",
      "estimators: 316.000000   score: 0.790833   auc: 0.873212\n",
      "estimators: 421.000000   score: 0.790833   auc: 0.874247\n",
      "estimators: 562.000000   score: 0.791667   auc: 0.876244\n",
      "estimators: 749.000000   score: 0.793333   auc: 0.874562\n",
      "estimators: 1000.000000   score: 0.784583   auc: 0.872909\n",
      "best estimators: 749\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "parameters = {\r\n",
    "    'n_estimators': [int(i) for i in np.logspace(0, 3, 25)],\r\n",
    "    'max_depth': [int(i) for i in np.logspace(0, 10, 25)],\r\n",
    "    'max_leaf_nodes': [int(i) for i in np.logspace(1, 4, 25)]\r\n",
    "}\r\n",
    "\r\n",
    "rfc = RandomizedSearchCV(RandomForestClassifier(), parameters, n_iter=50)\r\n",
    "best = rfc.fit(X, y)\r\n",
    "\r\n",
    "\r\n",
    "pd.DataFrame(best.cv_results_)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        1.516970      0.078142         0.033700        0.000951   \n",
       "1        0.365622      0.005070         0.007580        0.000799   \n",
       "2        0.111997      0.006661         0.003191        0.000406   \n",
       "3        0.076789      0.002590         0.007380        0.001833   \n",
       "4        0.058643      0.002130         0.002400        0.000483   \n",
       "5        2.353892      0.049307         0.035175        0.001628   \n",
       "6       15.724759      0.232795         0.218210        0.006097   \n",
       "7        1.027979      0.035712         0.090957        0.001458   \n",
       "8        0.670344      0.015385         0.012560        0.001190   \n",
       "9        0.044476      0.002073         0.001602        0.000493   \n",
       "10       6.167136      0.138523         0.103516        0.001708   \n",
       "11       0.185902      0.004828         0.006987        0.000892   \n",
       "12       0.030911      0.001252         0.001596        0.000488   \n",
       "13       0.207657      0.004388         0.009168        0.000747   \n",
       "14       0.640687      0.013773         0.009976        0.000904   \n",
       "15       0.395524      0.009542         0.024322        0.002565   \n",
       "16       0.853027      0.005978         0.014361        0.000476   \n",
       "17       0.030312      0.003713         0.001601        0.000483   \n",
       "18       0.095977      0.003802         0.006574        0.000789   \n",
       "19       0.132854      0.004303         0.004379        0.001011   \n",
       "20       1.778518      0.035256         0.074081        0.002721   \n",
       "21       1.260022      0.062462         0.019756        0.003236   \n",
       "22       3.129973      0.107828         0.073212        0.001626   \n",
       "23       0.029926      0.002093         0.001790        0.000736   \n",
       "24       0.071198      0.001358         0.004398        0.000494   \n",
       "25       0.027936      0.001984         0.002188        0.000385   \n",
       "26       0.024927      0.000906         0.001797        0.000399   \n",
       "27       0.101134      0.003242         0.004585        0.000792   \n",
       "28      16.216621      3.282145         0.301296        0.099986   \n",
       "29       0.054855      0.004365         0.003776        0.001309   \n",
       "30       1.012553      0.016588         0.021728        0.002907   \n",
       "31       0.794494      0.058113         0.015489        0.001413   \n",
       "32       0.032843      0.002704         0.004144        0.000863   \n",
       "33       0.045388      0.005253         0.003016        0.001082   \n",
       "34       9.239404      0.273541         0.168344        0.014260   \n",
       "35       0.041093      0.007367         0.002591        0.000488   \n",
       "36       3.320301      0.169988         0.057417        0.004084   \n",
       "37       2.688577      0.092790         0.081517        0.008635   \n",
       "38       0.260935      0.031021         0.006592        0.001273   \n",
       "39       0.061075      0.006621         0.006368        0.002218   \n",
       "40       0.075061      0.012681         0.004359        0.001693   \n",
       "41       3.295517      0.099139         0.089693        0.015303   \n",
       "42       6.815371      0.804340         0.146335        0.038070   \n",
       "43       0.022570      0.002305         0.001806        0.000404   \n",
       "44       1.496077      0.047288         0.084998        0.007270   \n",
       "45       0.824405      0.049831         0.036208        0.001414   \n",
       "46       1.400075      0.068779         0.066956        0.006003   \n",
       "47       0.008381      0.000860         0.001416        0.000514   \n",
       "48       1.487110      0.055955         0.049656        0.005039   \n",
       "49       2.984641      0.389697         0.055363        0.014500   \n",
       "\n",
       "   param_n_estimators param_max_leaf_nodes param_max_depth  \\\n",
       "0                 133                   74            2154   \n",
       "1                  23                  177             121   \n",
       "2                  10                   74          261015   \n",
       "3                  42                 5623               1   \n",
       "4                   2                 3162          100000   \n",
       "5                 100                10000             121   \n",
       "6                 562                 2371         4641588   \n",
       "7                 562                 5623               1   \n",
       "8                  31                  421             825   \n",
       "9                   2                  421       215443469   \n",
       "10                316                  316            2154   \n",
       "11                 31                   23             825   \n",
       "12                  1                 4216            5623   \n",
       "13                 42                   17        31622776   \n",
       "14                 23                 5623      3831186849   \n",
       "15                133                  749               6   \n",
       "16                 31                 5623        31622776   \n",
       "17                  1                  749      3831186849   \n",
       "18                 31                   13               6   \n",
       "19                 10                  100          681292   \n",
       "20                421                   13      3831186849   \n",
       "21                 42                 7498        31622776   \n",
       "22                316                   56         1778279   \n",
       "23                  1                 7498     10000000000   \n",
       "24                 23                 1333               6   \n",
       "25                  4                   31             316   \n",
       "26                  1                  421            2154   \n",
       "27                 17                  562              17   \n",
       "28                421                  749      3831186849   \n",
       "29                  1                 3162       562341325   \n",
       "30                 23                  562        82540418   \n",
       "31                 17                 4216         4641588   \n",
       "32                  5                 1778               6   \n",
       "33                  1                  562            2154   \n",
       "34                316                  316        31622776   \n",
       "35                  1                 2371     10000000000   \n",
       "36                 74                10000           14677   \n",
       "37                177                   56           38311   \n",
       "38                  5                 1333      1467799267   \n",
       "39                  3                   56              46   \n",
       "40                  1                 4216          261015   \n",
       "41                237                   56             825   \n",
       "42                421                  100       562341325   \n",
       "43                  2                   42             825   \n",
       "44                421                  100               6   \n",
       "45                177                   13             121   \n",
       "46                316                   13              17   \n",
       "47                  2                 4216               6   \n",
       "48                237                  316              17   \n",
       "49                133                  316        12115276   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'n_estimators': 133, 'max_leaf_nodes': 74, 'm...           0.785417   \n",
       "1   {'n_estimators': 23, 'max_leaf_nodes': 177, 'm...           0.814583   \n",
       "2   {'n_estimators': 10, 'max_leaf_nodes': 74, 'ma...           0.793750   \n",
       "3   {'n_estimators': 42, 'max_leaf_nodes': 5623, '...           0.547917   \n",
       "4   {'n_estimators': 2, 'max_leaf_nodes': 3162, 'm...           0.741667   \n",
       "5   {'n_estimators': 100, 'max_leaf_nodes': 10000,...           0.808333   \n",
       "6   {'n_estimators': 562, 'max_leaf_nodes': 2371, ...           0.806250   \n",
       "7   {'n_estimators': 562, 'max_leaf_nodes': 5623, ...           0.777083   \n",
       "8   {'n_estimators': 31, 'max_leaf_nodes': 421, 'm...           0.800000   \n",
       "9   {'n_estimators': 2, 'max_leaf_nodes': 421, 'ma...           0.702083   \n",
       "10  {'n_estimators': 316, 'max_leaf_nodes': 316, '...           0.818750   \n",
       "11  {'n_estimators': 31, 'max_leaf_nodes': 23, 'ma...           0.752083   \n",
       "12  {'n_estimators': 1, 'max_leaf_nodes': 4216, 'm...           0.622917   \n",
       "13  {'n_estimators': 42, 'max_leaf_nodes': 17, 'ma...           0.766667   \n",
       "14  {'n_estimators': 23, 'max_leaf_nodes': 5623, '...           0.772917   \n",
       "15  {'n_estimators': 133, 'max_leaf_nodes': 749, '...           0.760417   \n",
       "16  {'n_estimators': 31, 'max_leaf_nodes': 5623, '...           0.775000   \n",
       "17  {'n_estimators': 1, 'max_leaf_nodes': 749, 'ma...           0.693750   \n",
       "18  {'n_estimators': 31, 'max_leaf_nodes': 13, 'ma...           0.747917   \n",
       "19  {'n_estimators': 10, 'max_leaf_nodes': 100, 'm...           0.766667   \n",
       "20  {'n_estimators': 421, 'max_leaf_nodes': 13, 'm...           0.789583   \n",
       "21  {'n_estimators': 42, 'max_leaf_nodes': 7498, '...           0.779167   \n",
       "22  {'n_estimators': 316, 'max_leaf_nodes': 56, 'm...           0.791667   \n",
       "23  {'n_estimators': 1, 'max_leaf_nodes': 7498, 'm...           0.687500   \n",
       "24  {'n_estimators': 23, 'max_leaf_nodes': 1333, '...           0.716667   \n",
       "25  {'n_estimators': 4, 'max_leaf_nodes': 31, 'max...           0.714583   \n",
       "26  {'n_estimators': 1, 'max_leaf_nodes': 421, 'ma...           0.706250   \n",
       "27  {'n_estimators': 17, 'max_leaf_nodes': 562, 'm...           0.793750   \n",
       "28  {'n_estimators': 421, 'max_leaf_nodes': 749, '...           0.804167   \n",
       "29  {'n_estimators': 1, 'max_leaf_nodes': 3162, 'm...           0.645833   \n",
       "30  {'n_estimators': 23, 'max_leaf_nodes': 562, 'm...           0.812500   \n",
       "31  {'n_estimators': 17, 'max_leaf_nodes': 4216, '...           0.758333   \n",
       "32  {'n_estimators': 5, 'max_leaf_nodes': 1778, 'm...           0.570833   \n",
       "33  {'n_estimators': 1, 'max_leaf_nodes': 562, 'ma...           0.695833   \n",
       "34  {'n_estimators': 316, 'max_leaf_nodes': 316, '...           0.818750   \n",
       "35  {'n_estimators': 1, 'max_leaf_nodes': 2371, 'm...           0.735417   \n",
       "36  {'n_estimators': 74, 'max_leaf_nodes': 10000, ...           0.816667   \n",
       "37  {'n_estimators': 177, 'max_leaf_nodes': 56, 'm...           0.779167   \n",
       "38  {'n_estimators': 5, 'max_leaf_nodes': 1333, 'm...           0.737500   \n",
       "39  {'n_estimators': 3, 'max_leaf_nodes': 56, 'max...           0.745833   \n",
       "40  {'n_estimators': 1, 'max_leaf_nodes': 4216, 'm...           0.650000   \n",
       "41  {'n_estimators': 237, 'max_leaf_nodes': 56, 'm...           0.785417   \n",
       "42  {'n_estimators': 421, 'max_leaf_nodes': 100, '...           0.812500   \n",
       "43  {'n_estimators': 2, 'max_leaf_nodes': 42, 'max...           0.687500   \n",
       "44  {'n_estimators': 421, 'max_leaf_nodes': 100, '...           0.787500   \n",
       "45  {'n_estimators': 177, 'max_leaf_nodes': 13, 'm...           0.779167   \n",
       "46  {'n_estimators': 316, 'max_leaf_nodes': 13, 'm...           0.787500   \n",
       "47  {'n_estimators': 2, 'max_leaf_nodes': 4216, 'm...           0.564583   \n",
       "48  {'n_estimators': 237, 'max_leaf_nodes': 316, '...           0.783333   \n",
       "49  {'n_estimators': 133, 'max_leaf_nodes': 316, '...           0.812500   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.793750           0.768750           0.756250   \n",
       "1            0.785417           0.766667           0.733333   \n",
       "2            0.733333           0.710417           0.745833   \n",
       "3            0.625000           0.622917           0.625000   \n",
       "4            0.629167           0.589583           0.631250   \n",
       "5            0.779167           0.772917           0.772917   \n",
       "6            0.772917           0.795833           0.760417   \n",
       "7            0.772917           0.710417           0.756250   \n",
       "8            0.772917           0.760417           0.750000   \n",
       "9            0.670833           0.662500           0.658333   \n",
       "10           0.781250           0.787500           0.760417   \n",
       "11           0.781250           0.747917           0.756250   \n",
       "12           0.639583           0.637500           0.612500   \n",
       "13           0.777083           0.756250           0.733333   \n",
       "14           0.768750           0.735417           0.747917   \n",
       "15           0.770833           0.739583           0.743750   \n",
       "16           0.772917           0.750000           0.731250   \n",
       "17           0.625000           0.604167           0.677083   \n",
       "18           0.720833           0.695833           0.700000   \n",
       "19           0.756250           0.745833           0.743750   \n",
       "20           0.787500           0.762500           0.754167   \n",
       "21           0.766667           0.768750           0.745833   \n",
       "22           0.793750           0.766667           0.760417   \n",
       "23           0.608333           0.664583           0.650000   \n",
       "24           0.675000           0.687500           0.652083   \n",
       "25           0.681250           0.633333           0.691667   \n",
       "26           0.687500           0.612500           0.620833   \n",
       "27           0.745833           0.687500           0.702083   \n",
       "28           0.783333           0.795833           0.752083   \n",
       "29           0.635417           0.641667           0.660417   \n",
       "30           0.787500           0.735417           0.766667   \n",
       "31           0.779167           0.731250           0.727083   \n",
       "32           0.608333           0.570833           0.554167   \n",
       "33           0.691667           0.672917           0.689583   \n",
       "34           0.791667           0.797917           0.764583   \n",
       "35           0.656250           0.635417           0.639583   \n",
       "36           0.750000           0.770833           0.754167   \n",
       "37           0.793750           0.758333           0.764583   \n",
       "38           0.693750           0.697917           0.706250   \n",
       "39           0.693750           0.689583           0.656250   \n",
       "40           0.695833           0.683333           0.672917   \n",
       "41           0.797917           0.768750           0.752083   \n",
       "42           0.797917           0.781250           0.747917   \n",
       "43           0.689583           0.583333           0.631250   \n",
       "44           0.783333           0.731250           0.756250   \n",
       "45           0.793750           0.737500           0.747917   \n",
       "46           0.787500           0.756250           0.762500   \n",
       "47           0.645833           0.533333           0.518750   \n",
       "48           0.808333           0.758333           0.745833   \n",
       "49           0.793750           0.783333           0.764583   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.814583         0.783750        0.020173                9  \n",
       "1            0.804167         0.780833        0.028849               11  \n",
       "2            0.766667         0.750000        0.028474               30  \n",
       "3            0.679167         0.620000        0.041845               48  \n",
       "4            0.652083         0.648750        0.050666               44  \n",
       "5            0.804167         0.787500        0.015534                5  \n",
       "6            0.802083         0.787500        0.017776                5  \n",
       "7            0.800000         0.763333        0.029919               24  \n",
       "8            0.787500         0.774167        0.017999               18  \n",
       "9            0.660417         0.670833        0.016191               39  \n",
       "10           0.812500         0.792083        0.021303                3  \n",
       "11           0.775000         0.762500        0.013176               26  \n",
       "12           0.702083         0.642917        0.031197               46  \n",
       "13           0.793750         0.765417        0.020267               22  \n",
       "14           0.787500         0.762500        0.018540               25  \n",
       "15           0.781250         0.759167        0.015789               27  \n",
       "16           0.793750         0.764583        0.021691               23  \n",
       "17           0.641667         0.648333        0.032951               45  \n",
       "18           0.727083         0.718333        0.018975               32  \n",
       "19           0.766667         0.755833        0.009807               28  \n",
       "20           0.797917         0.778333        0.016905               15  \n",
       "21           0.791667         0.770417        0.015161               21  \n",
       "22           0.810417         0.784583        0.018475                8  \n",
       "23           0.681250         0.658333        0.028229               42  \n",
       "24           0.747917         0.695833        0.033359               34  \n",
       "25           0.714583         0.687083        0.029855               37  \n",
       "26           0.681250         0.661667        0.037745               41  \n",
       "27           0.752083         0.736250        0.037928               31  \n",
       "28           0.800000         0.787083        0.018838                7  \n",
       "29           0.622917         0.641250        0.012318               47  \n",
       "30           0.789583         0.778333        0.025907               15  \n",
       "31           0.758333         0.750833        0.019302               29  \n",
       "32           0.583333         0.577500        0.017989               49  \n",
       "33           0.700000         0.690000        0.009261               36  \n",
       "34           0.818750         0.798333        0.020087                1  \n",
       "35           0.675000         0.668333        0.036334               40  \n",
       "36           0.791667         0.776667        0.024805               17  \n",
       "37           0.808333         0.780833        0.018418               11  \n",
       "38           0.741667         0.715417        0.020181               33  \n",
       "39           0.683333         0.693750        0.029137               35  \n",
       "40           0.704167         0.681250        0.018911               38  \n",
       "41           0.812500         0.783333        0.021246               10  \n",
       "42           0.814583         0.790833        0.024566                4  \n",
       "43           0.670833         0.652500        0.040427               43  \n",
       "44           0.797917         0.771250        0.024274               20  \n",
       "45           0.808333         0.773333        0.026855               19  \n",
       "46           0.802083         0.779167        0.017129               14  \n",
       "47           0.541667         0.560833        0.045019               50  \n",
       "48           0.808333         0.780833        0.025495               11  \n",
       "49           0.810417         0.792917        0.017805                2  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_max_leaf_nodes</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.516970</td>\n",
       "      <td>0.078142</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>133</td>\n",
       "      <td>74</td>\n",
       "      <td>2154</td>\n",
       "      <td>{'n_estimators': 133, 'max_leaf_nodes': 74, 'm...</td>\n",
       "      <td>0.785417</td>\n",
       "      <td>0.793750</td>\n",
       "      <td>0.768750</td>\n",
       "      <td>0.756250</td>\n",
       "      <td>0.814583</td>\n",
       "      <td>0.783750</td>\n",
       "      <td>0.020173</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.365622</td>\n",
       "      <td>0.005070</td>\n",
       "      <td>0.007580</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>23</td>\n",
       "      <td>177</td>\n",
       "      <td>121</td>\n",
       "      <td>{'n_estimators': 23, 'max_leaf_nodes': 177, 'm...</td>\n",
       "      <td>0.814583</td>\n",
       "      <td>0.785417</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.804167</td>\n",
       "      <td>0.780833</td>\n",
       "      <td>0.028849</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.111997</td>\n",
       "      <td>0.006661</td>\n",
       "      <td>0.003191</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>10</td>\n",
       "      <td>74</td>\n",
       "      <td>261015</td>\n",
       "      <td>{'n_estimators': 10, 'max_leaf_nodes': 74, 'ma...</td>\n",
       "      <td>0.793750</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.710417</td>\n",
       "      <td>0.745833</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.028474</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.076789</td>\n",
       "      <td>0.002590</td>\n",
       "      <td>0.007380</td>\n",
       "      <td>0.001833</td>\n",
       "      <td>42</td>\n",
       "      <td>5623</td>\n",
       "      <td>1</td>\n",
       "      <td>{'n_estimators': 42, 'max_leaf_nodes': 5623, '...</td>\n",
       "      <td>0.547917</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.622917</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.679167</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.041845</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.058643</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>2</td>\n",
       "      <td>3162</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'n_estimators': 2, 'max_leaf_nodes': 3162, 'm...</td>\n",
       "      <td>0.741667</td>\n",
       "      <td>0.629167</td>\n",
       "      <td>0.589583</td>\n",
       "      <td>0.631250</td>\n",
       "      <td>0.652083</td>\n",
       "      <td>0.648750</td>\n",
       "      <td>0.050666</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.353892</td>\n",
       "      <td>0.049307</td>\n",
       "      <td>0.035175</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>100</td>\n",
       "      <td>10000</td>\n",
       "      <td>121</td>\n",
       "      <td>{'n_estimators': 100, 'max_leaf_nodes': 10000,...</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.779167</td>\n",
       "      <td>0.772917</td>\n",
       "      <td>0.772917</td>\n",
       "      <td>0.804167</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.015534</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15.724759</td>\n",
       "      <td>0.232795</td>\n",
       "      <td>0.218210</td>\n",
       "      <td>0.006097</td>\n",
       "      <td>562</td>\n",
       "      <td>2371</td>\n",
       "      <td>4641588</td>\n",
       "      <td>{'n_estimators': 562, 'max_leaf_nodes': 2371, ...</td>\n",
       "      <td>0.806250</td>\n",
       "      <td>0.772917</td>\n",
       "      <td>0.795833</td>\n",
       "      <td>0.760417</td>\n",
       "      <td>0.802083</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.017776</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.027979</td>\n",
       "      <td>0.035712</td>\n",
       "      <td>0.090957</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>562</td>\n",
       "      <td>5623</td>\n",
       "      <td>1</td>\n",
       "      <td>{'n_estimators': 562, 'max_leaf_nodes': 5623, ...</td>\n",
       "      <td>0.777083</td>\n",
       "      <td>0.772917</td>\n",
       "      <td>0.710417</td>\n",
       "      <td>0.756250</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.763333</td>\n",
       "      <td>0.029919</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.670344</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>0.012560</td>\n",
       "      <td>0.001190</td>\n",
       "      <td>31</td>\n",
       "      <td>421</td>\n",
       "      <td>825</td>\n",
       "      <td>{'n_estimators': 31, 'max_leaf_nodes': 421, 'm...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.772917</td>\n",
       "      <td>0.760417</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.774167</td>\n",
       "      <td>0.017999</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.044476</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>0.001602</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>2</td>\n",
       "      <td>421</td>\n",
       "      <td>215443469</td>\n",
       "      <td>{'n_estimators': 2, 'max_leaf_nodes': 421, 'ma...</td>\n",
       "      <td>0.702083</td>\n",
       "      <td>0.670833</td>\n",
       "      <td>0.662500</td>\n",
       "      <td>0.658333</td>\n",
       "      <td>0.660417</td>\n",
       "      <td>0.670833</td>\n",
       "      <td>0.016191</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.167136</td>\n",
       "      <td>0.138523</td>\n",
       "      <td>0.103516</td>\n",
       "      <td>0.001708</td>\n",
       "      <td>316</td>\n",
       "      <td>316</td>\n",
       "      <td>2154</td>\n",
       "      <td>{'n_estimators': 316, 'max_leaf_nodes': 316, '...</td>\n",
       "      <td>0.818750</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.760417</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.792083</td>\n",
       "      <td>0.021303</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.185902</td>\n",
       "      <td>0.004828</td>\n",
       "      <td>0.006987</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>825</td>\n",
       "      <td>{'n_estimators': 31, 'max_leaf_nodes': 23, 'ma...</td>\n",
       "      <td>0.752083</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.747917</td>\n",
       "      <td>0.756250</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.013176</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.030911</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>1</td>\n",
       "      <td>4216</td>\n",
       "      <td>5623</td>\n",
       "      <td>{'n_estimators': 1, 'max_leaf_nodes': 4216, 'm...</td>\n",
       "      <td>0.622917</td>\n",
       "      <td>0.639583</td>\n",
       "      <td>0.637500</td>\n",
       "      <td>0.612500</td>\n",
       "      <td>0.702083</td>\n",
       "      <td>0.642917</td>\n",
       "      <td>0.031197</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.207657</td>\n",
       "      <td>0.004388</td>\n",
       "      <td>0.009168</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>42</td>\n",
       "      <td>17</td>\n",
       "      <td>31622776</td>\n",
       "      <td>{'n_estimators': 42, 'max_leaf_nodes': 17, 'ma...</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.777083</td>\n",
       "      <td>0.756250</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.793750</td>\n",
       "      <td>0.765417</td>\n",
       "      <td>0.020267</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.640687</td>\n",
       "      <td>0.013773</td>\n",
       "      <td>0.009976</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>23</td>\n",
       "      <td>5623</td>\n",
       "      <td>3831186849</td>\n",
       "      <td>{'n_estimators': 23, 'max_leaf_nodes': 5623, '...</td>\n",
       "      <td>0.772917</td>\n",
       "      <td>0.768750</td>\n",
       "      <td>0.735417</td>\n",
       "      <td>0.747917</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.018540</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.395524</td>\n",
       "      <td>0.009542</td>\n",
       "      <td>0.024322</td>\n",
       "      <td>0.002565</td>\n",
       "      <td>133</td>\n",
       "      <td>749</td>\n",
       "      <td>6</td>\n",
       "      <td>{'n_estimators': 133, 'max_leaf_nodes': 749, '...</td>\n",
       "      <td>0.760417</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.739583</td>\n",
       "      <td>0.743750</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.759167</td>\n",
       "      <td>0.015789</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.853027</td>\n",
       "      <td>0.005978</td>\n",
       "      <td>0.014361</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>31</td>\n",
       "      <td>5623</td>\n",
       "      <td>31622776</td>\n",
       "      <td>{'n_estimators': 31, 'max_leaf_nodes': 5623, '...</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.772917</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.731250</td>\n",
       "      <td>0.793750</td>\n",
       "      <td>0.764583</td>\n",
       "      <td>0.021691</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.030312</td>\n",
       "      <td>0.003713</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>1</td>\n",
       "      <td>749</td>\n",
       "      <td>3831186849</td>\n",
       "      <td>{'n_estimators': 1, 'max_leaf_nodes': 749, 'ma...</td>\n",
       "      <td>0.693750</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0.677083</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>0.648333</td>\n",
       "      <td>0.032951</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.095977</td>\n",
       "      <td>0.003802</td>\n",
       "      <td>0.006574</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>31</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>{'n_estimators': 31, 'max_leaf_nodes': 13, 'ma...</td>\n",
       "      <td>0.747917</td>\n",
       "      <td>0.720833</td>\n",
       "      <td>0.695833</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.727083</td>\n",
       "      <td>0.718333</td>\n",
       "      <td>0.018975</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.132854</td>\n",
       "      <td>0.004303</td>\n",
       "      <td>0.004379</td>\n",
       "      <td>0.001011</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>681292</td>\n",
       "      <td>{'n_estimators': 10, 'max_leaf_nodes': 100, 'm...</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.756250</td>\n",
       "      <td>0.745833</td>\n",
       "      <td>0.743750</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.755833</td>\n",
       "      <td>0.009807</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.778518</td>\n",
       "      <td>0.035256</td>\n",
       "      <td>0.074081</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>421</td>\n",
       "      <td>13</td>\n",
       "      <td>3831186849</td>\n",
       "      <td>{'n_estimators': 421, 'max_leaf_nodes': 13, 'm...</td>\n",
       "      <td>0.789583</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.754167</td>\n",
       "      <td>0.797917</td>\n",
       "      <td>0.778333</td>\n",
       "      <td>0.016905</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.260022</td>\n",
       "      <td>0.062462</td>\n",
       "      <td>0.019756</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>42</td>\n",
       "      <td>7498</td>\n",
       "      <td>31622776</td>\n",
       "      <td>{'n_estimators': 42, 'max_leaf_nodes': 7498, '...</td>\n",
       "      <td>0.779167</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.768750</td>\n",
       "      <td>0.745833</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.770417</td>\n",
       "      <td>0.015161</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.129973</td>\n",
       "      <td>0.107828</td>\n",
       "      <td>0.073212</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>316</td>\n",
       "      <td>56</td>\n",
       "      <td>1778279</td>\n",
       "      <td>{'n_estimators': 316, 'max_leaf_nodes': 56, 'm...</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.793750</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.760417</td>\n",
       "      <td>0.810417</td>\n",
       "      <td>0.784583</td>\n",
       "      <td>0.018475</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.029926</td>\n",
       "      <td>0.002093</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>1</td>\n",
       "      <td>7498</td>\n",
       "      <td>10000000000</td>\n",
       "      <td>{'n_estimators': 1, 'max_leaf_nodes': 7498, 'm...</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.608333</td>\n",
       "      <td>0.664583</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.681250</td>\n",
       "      <td>0.658333</td>\n",
       "      <td>0.028229</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.071198</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>0.004398</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>23</td>\n",
       "      <td>1333</td>\n",
       "      <td>6</td>\n",
       "      <td>{'n_estimators': 23, 'max_leaf_nodes': 1333, '...</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.652083</td>\n",
       "      <td>0.747917</td>\n",
       "      <td>0.695833</td>\n",
       "      <td>0.033359</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.027936</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>316</td>\n",
       "      <td>{'n_estimators': 4, 'max_leaf_nodes': 31, 'max...</td>\n",
       "      <td>0.714583</td>\n",
       "      <td>0.681250</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.691667</td>\n",
       "      <td>0.714583</td>\n",
       "      <td>0.687083</td>\n",
       "      <td>0.029855</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.024927</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.001797</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>1</td>\n",
       "      <td>421</td>\n",
       "      <td>2154</td>\n",
       "      <td>{'n_estimators': 1, 'max_leaf_nodes': 421, 'ma...</td>\n",
       "      <td>0.706250</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.612500</td>\n",
       "      <td>0.620833</td>\n",
       "      <td>0.681250</td>\n",
       "      <td>0.661667</td>\n",
       "      <td>0.037745</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.101134</td>\n",
       "      <td>0.003242</td>\n",
       "      <td>0.004585</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>17</td>\n",
       "      <td>562</td>\n",
       "      <td>17</td>\n",
       "      <td>{'n_estimators': 17, 'max_leaf_nodes': 562, 'm...</td>\n",
       "      <td>0.793750</td>\n",
       "      <td>0.745833</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.702083</td>\n",
       "      <td>0.752083</td>\n",
       "      <td>0.736250</td>\n",
       "      <td>0.037928</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>16.216621</td>\n",
       "      <td>3.282145</td>\n",
       "      <td>0.301296</td>\n",
       "      <td>0.099986</td>\n",
       "      <td>421</td>\n",
       "      <td>749</td>\n",
       "      <td>3831186849</td>\n",
       "      <td>{'n_estimators': 421, 'max_leaf_nodes': 749, '...</td>\n",
       "      <td>0.804167</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.795833</td>\n",
       "      <td>0.752083</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.787083</td>\n",
       "      <td>0.018838</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.054855</td>\n",
       "      <td>0.004365</td>\n",
       "      <td>0.003776</td>\n",
       "      <td>0.001309</td>\n",
       "      <td>1</td>\n",
       "      <td>3162</td>\n",
       "      <td>562341325</td>\n",
       "      <td>{'n_estimators': 1, 'max_leaf_nodes': 3162, 'm...</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.635417</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>0.660417</td>\n",
       "      <td>0.622917</td>\n",
       "      <td>0.641250</td>\n",
       "      <td>0.012318</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.012553</td>\n",
       "      <td>0.016588</td>\n",
       "      <td>0.021728</td>\n",
       "      <td>0.002907</td>\n",
       "      <td>23</td>\n",
       "      <td>562</td>\n",
       "      <td>82540418</td>\n",
       "      <td>{'n_estimators': 23, 'max_leaf_nodes': 562, 'm...</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.735417</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.789583</td>\n",
       "      <td>0.778333</td>\n",
       "      <td>0.025907</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.794494</td>\n",
       "      <td>0.058113</td>\n",
       "      <td>0.015489</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>17</td>\n",
       "      <td>4216</td>\n",
       "      <td>4641588</td>\n",
       "      <td>{'n_estimators': 17, 'max_leaf_nodes': 4216, '...</td>\n",
       "      <td>0.758333</td>\n",
       "      <td>0.779167</td>\n",
       "      <td>0.731250</td>\n",
       "      <td>0.727083</td>\n",
       "      <td>0.758333</td>\n",
       "      <td>0.750833</td>\n",
       "      <td>0.019302</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.032843</td>\n",
       "      <td>0.002704</td>\n",
       "      <td>0.004144</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>5</td>\n",
       "      <td>1778</td>\n",
       "      <td>6</td>\n",
       "      <td>{'n_estimators': 5, 'max_leaf_nodes': 1778, 'm...</td>\n",
       "      <td>0.570833</td>\n",
       "      <td>0.608333</td>\n",
       "      <td>0.570833</td>\n",
       "      <td>0.554167</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.577500</td>\n",
       "      <td>0.017989</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.045388</td>\n",
       "      <td>0.005253</td>\n",
       "      <td>0.003016</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>1</td>\n",
       "      <td>562</td>\n",
       "      <td>2154</td>\n",
       "      <td>{'n_estimators': 1, 'max_leaf_nodes': 562, 'ma...</td>\n",
       "      <td>0.695833</td>\n",
       "      <td>0.691667</td>\n",
       "      <td>0.672917</td>\n",
       "      <td>0.689583</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.009261</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>9.239404</td>\n",
       "      <td>0.273541</td>\n",
       "      <td>0.168344</td>\n",
       "      <td>0.014260</td>\n",
       "      <td>316</td>\n",
       "      <td>316</td>\n",
       "      <td>31622776</td>\n",
       "      <td>{'n_estimators': 316, 'max_leaf_nodes': 316, '...</td>\n",
       "      <td>0.818750</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.797917</td>\n",
       "      <td>0.764583</td>\n",
       "      <td>0.818750</td>\n",
       "      <td>0.798333</td>\n",
       "      <td>0.020087</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.041093</td>\n",
       "      <td>0.007367</td>\n",
       "      <td>0.002591</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>1</td>\n",
       "      <td>2371</td>\n",
       "      <td>10000000000</td>\n",
       "      <td>{'n_estimators': 1, 'max_leaf_nodes': 2371, 'm...</td>\n",
       "      <td>0.735417</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.635417</td>\n",
       "      <td>0.639583</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.668333</td>\n",
       "      <td>0.036334</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>3.320301</td>\n",
       "      <td>0.169988</td>\n",
       "      <td>0.057417</td>\n",
       "      <td>0.004084</td>\n",
       "      <td>74</td>\n",
       "      <td>10000</td>\n",
       "      <td>14677</td>\n",
       "      <td>{'n_estimators': 74, 'max_leaf_nodes': 10000, ...</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.754167</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.776667</td>\n",
       "      <td>0.024805</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2.688577</td>\n",
       "      <td>0.092790</td>\n",
       "      <td>0.081517</td>\n",
       "      <td>0.008635</td>\n",
       "      <td>177</td>\n",
       "      <td>56</td>\n",
       "      <td>38311</td>\n",
       "      <td>{'n_estimators': 177, 'max_leaf_nodes': 56, 'm...</td>\n",
       "      <td>0.779167</td>\n",
       "      <td>0.793750</td>\n",
       "      <td>0.758333</td>\n",
       "      <td>0.764583</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.780833</td>\n",
       "      <td>0.018418</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.260935</td>\n",
       "      <td>0.031021</td>\n",
       "      <td>0.006592</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>5</td>\n",
       "      <td>1333</td>\n",
       "      <td>1467799267</td>\n",
       "      <td>{'n_estimators': 5, 'max_leaf_nodes': 1333, 'm...</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>0.693750</td>\n",
       "      <td>0.697917</td>\n",
       "      <td>0.706250</td>\n",
       "      <td>0.741667</td>\n",
       "      <td>0.715417</td>\n",
       "      <td>0.020181</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.061075</td>\n",
       "      <td>0.006621</td>\n",
       "      <td>0.006368</td>\n",
       "      <td>0.002218</td>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>46</td>\n",
       "      <td>{'n_estimators': 3, 'max_leaf_nodes': 56, 'max...</td>\n",
       "      <td>0.745833</td>\n",
       "      <td>0.693750</td>\n",
       "      <td>0.689583</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.693750</td>\n",
       "      <td>0.029137</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.075061</td>\n",
       "      <td>0.012681</td>\n",
       "      <td>0.004359</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>1</td>\n",
       "      <td>4216</td>\n",
       "      <td>261015</td>\n",
       "      <td>{'n_estimators': 1, 'max_leaf_nodes': 4216, 'm...</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.695833</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.672917</td>\n",
       "      <td>0.704167</td>\n",
       "      <td>0.681250</td>\n",
       "      <td>0.018911</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>3.295517</td>\n",
       "      <td>0.099139</td>\n",
       "      <td>0.089693</td>\n",
       "      <td>0.015303</td>\n",
       "      <td>237</td>\n",
       "      <td>56</td>\n",
       "      <td>825</td>\n",
       "      <td>{'n_estimators': 237, 'max_leaf_nodes': 56, 'm...</td>\n",
       "      <td>0.785417</td>\n",
       "      <td>0.797917</td>\n",
       "      <td>0.768750</td>\n",
       "      <td>0.752083</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.021246</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>6.815371</td>\n",
       "      <td>0.804340</td>\n",
       "      <td>0.146335</td>\n",
       "      <td>0.038070</td>\n",
       "      <td>421</td>\n",
       "      <td>100</td>\n",
       "      <td>562341325</td>\n",
       "      <td>{'n_estimators': 421, 'max_leaf_nodes': 100, '...</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.797917</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.747917</td>\n",
       "      <td>0.814583</td>\n",
       "      <td>0.790833</td>\n",
       "      <td>0.024566</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.022570</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.001806</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>825</td>\n",
       "      <td>{'n_estimators': 2, 'max_leaf_nodes': 42, 'max...</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.689583</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.631250</td>\n",
       "      <td>0.670833</td>\n",
       "      <td>0.652500</td>\n",
       "      <td>0.040427</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.496077</td>\n",
       "      <td>0.047288</td>\n",
       "      <td>0.084998</td>\n",
       "      <td>0.007270</td>\n",
       "      <td>421</td>\n",
       "      <td>100</td>\n",
       "      <td>6</td>\n",
       "      <td>{'n_estimators': 421, 'max_leaf_nodes': 100, '...</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.731250</td>\n",
       "      <td>0.756250</td>\n",
       "      <td>0.797917</td>\n",
       "      <td>0.771250</td>\n",
       "      <td>0.024274</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.824405</td>\n",
       "      <td>0.049831</td>\n",
       "      <td>0.036208</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>177</td>\n",
       "      <td>13</td>\n",
       "      <td>121</td>\n",
       "      <td>{'n_estimators': 177, 'max_leaf_nodes': 13, 'm...</td>\n",
       "      <td>0.779167</td>\n",
       "      <td>0.793750</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>0.747917</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.773333</td>\n",
       "      <td>0.026855</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.400075</td>\n",
       "      <td>0.068779</td>\n",
       "      <td>0.066956</td>\n",
       "      <td>0.006003</td>\n",
       "      <td>316</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>{'n_estimators': 316, 'max_leaf_nodes': 13, 'm...</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.756250</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.802083</td>\n",
       "      <td>0.779167</td>\n",
       "      <td>0.017129</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.008381</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>2</td>\n",
       "      <td>4216</td>\n",
       "      <td>6</td>\n",
       "      <td>{'n_estimators': 2, 'max_leaf_nodes': 4216, 'm...</td>\n",
       "      <td>0.564583</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.518750</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.560833</td>\n",
       "      <td>0.045019</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.487110</td>\n",
       "      <td>0.055955</td>\n",
       "      <td>0.049656</td>\n",
       "      <td>0.005039</td>\n",
       "      <td>237</td>\n",
       "      <td>316</td>\n",
       "      <td>17</td>\n",
       "      <td>{'n_estimators': 237, 'max_leaf_nodes': 316, '...</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.758333</td>\n",
       "      <td>0.745833</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.780833</td>\n",
       "      <td>0.025495</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2.984641</td>\n",
       "      <td>0.389697</td>\n",
       "      <td>0.055363</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>133</td>\n",
       "      <td>316</td>\n",
       "      <td>12115276</td>\n",
       "      <td>{'n_estimators': 133, 'max_leaf_nodes': 316, '...</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.793750</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.764583</td>\n",
       "      <td>0.810417</td>\n",
       "      <td>0.792917</td>\n",
       "      <td>0.017805</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "parameters = {\r\n",
    "    'n_estimators': [int(i) for i in np.logspace(0, 3, 25)],\r\n",
    "    # 'max_depth': [int(i) for i in np.logspace(0, 10, 25)],\r\n",
    "    'max_leaf_nodes': [int(i) for i in np.logspace(1, 4, 25)]\r\n",
    "}\r\n",
    "\r\n",
    "rfc = RandomizedSearchCV(RandomForestClassifier(), parameters, n_iter=50)\r\n",
    "best = rfc.fit(X, y)\r\n",
    "\r\n",
    "\r\n",
    "pd.DataFrame(best.cv_results_)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.031916      0.005048         0.001794        0.000398   \n",
       "1        0.327523      0.014278         0.006776        0.000745   \n",
       "2        0.160560      0.036406         0.003591        0.001198   \n",
       "3        1.208300      0.101814         0.017175        0.000406   \n",
       "4        0.084938      0.002992         0.002336        0.000681   \n",
       "5        0.677978      0.045590         0.010774        0.001470   \n",
       "6        0.137641      0.004227         0.003191        0.000745   \n",
       "7        0.976967      0.020233         0.024458        0.002360   \n",
       "8        0.016361      0.001503         0.002189        0.000966   \n",
       "9        0.136361      0.005858         0.003917        0.001151   \n",
       "10      20.938192      0.271128         0.288577        0.009172   \n",
       "11       0.843431      0.099172         0.015463        0.005354   \n",
       "12       1.276864      0.159248         0.019053        0.005946   \n",
       "13       2.335064      0.082362         0.058042        0.005471   \n",
       "14       0.477716      0.008504         0.008377        0.001191   \n",
       "15       0.182313      0.006771         0.007579        0.001006   \n",
       "16       0.029322      0.002224         0.001398        0.000798   \n",
       "17       0.191476      0.003415         0.004613        0.001022   \n",
       "18       0.020144      0.001599         0.001404        0.000496   \n",
       "19       0.879840      0.016104         0.042679        0.001721   \n",
       "20       5.069089      0.265948         0.073004        0.009127   \n",
       "21       1.988981      0.026115         0.043850        0.001256   \n",
       "22       0.498219      0.011052         0.027131        0.003741   \n",
       "23      20.954413      0.294147         0.308625        0.003893   \n",
       "24       5.994192      0.165842         0.192905        0.004233   \n",
       "25       0.839131      0.015968         0.015724        0.000939   \n",
       "26       8.942290      0.225588         0.125470        0.003235   \n",
       "27       0.276402      0.005885         0.005580        0.001347   \n",
       "28       0.030757      0.002055         0.001994        0.000001   \n",
       "29       0.751719      0.014813         0.013963        0.000312   \n",
       "30       0.061533      0.003848         0.001997        0.000001   \n",
       "31       0.368219      0.010202         0.005993        0.001245   \n",
       "32       1.794066      0.057227         0.031669        0.001654   \n",
       "33       0.083705      0.002698         0.004320        0.000662   \n",
       "34       0.013677      0.000622         0.001795        0.000399   \n",
       "35       3.084326      0.144991         0.061212        0.002976   \n",
       "36       8.805029      0.152405         0.124273        0.004244   \n",
       "37       0.023857      0.001993         0.001596        0.000489   \n",
       "38       0.482233      0.010504         0.007501        0.000666   \n",
       "39       0.051673      0.001159         0.002979        0.000880   \n",
       "40       0.023531      0.001852         0.001797        0.000399   \n",
       "41       0.192662      0.005009         0.003797        0.001159   \n",
       "42       0.163950      0.005904         0.005927        0.000989   \n",
       "43       0.115152      0.003488         0.006267        0.001592   \n",
       "44       1.212058      0.062743         0.017644        0.001104   \n",
       "45       0.324026      0.006603         0.006170        0.000967   \n",
       "46       0.361921      0.005748         0.006730        0.001731   \n",
       "47       0.228941      0.009506         0.004985        0.000008   \n",
       "48       0.065743      0.003154         0.005444        0.001333   \n",
       "49       6.819818      0.291031         0.098564        0.013243   \n",
       "\n",
       "   param_n_estimators param_max_leaf_nodes  \\\n",
       "0                   1                 7498   \n",
       "1                  23                  133   \n",
       "2                   5                 1778   \n",
       "3                  42                 5623   \n",
       "4                   3                10000   \n",
       "5                  23                 5623   \n",
       "6                   5                 4216   \n",
       "7                 100                   56   \n",
       "8                   4                   10   \n",
       "9                   5                 1000   \n",
       "10                749                 1778   \n",
       "11                 31                 1778   \n",
       "12                 42                 1000   \n",
       "13                237                   56   \n",
       "14                 17                 7498   \n",
       "15                 31                   23   \n",
       "16                  1                 2371   \n",
       "17                  7                 5623   \n",
       "18                  1                  237   \n",
       "19                237                   10   \n",
       "20                177                 5623   \n",
       "21                177                   74   \n",
       "22                133                   10   \n",
       "23                749                 1000   \n",
       "24               1000                   23   \n",
       "25                 42                  316   \n",
       "26                316                 1333   \n",
       "27                 10                  749   \n",
       "28                  2                  133   \n",
       "29                 42                  237   \n",
       "30                  4                  133   \n",
       "31                 13                 3162   \n",
       "32                100                  237   \n",
       "33                 17                   17   \n",
       "34                  2                   23   \n",
       "35                237                  100   \n",
       "36                316                  749   \n",
       "37                  3                   31   \n",
       "38                 17                 4216   \n",
       "39                  7                   31   \n",
       "40                  4                   17   \n",
       "41                  7                  749   \n",
       "42                 23                   31   \n",
       "43                 23                   17   \n",
       "44                 42                10000   \n",
       "45                 13                  562   \n",
       "46                 13                 1000   \n",
       "47                 13                  237   \n",
       "48                 17                   10   \n",
       "49                237                 1778   \n",
       "\n",
       "                                           params  split0_test_score  \\\n",
       "0     {'n_estimators': 1, 'max_leaf_nodes': 7498}           0.714583   \n",
       "1     {'n_estimators': 23, 'max_leaf_nodes': 133}           0.812500   \n",
       "2     {'n_estimators': 5, 'max_leaf_nodes': 1778}           0.729167   \n",
       "3    {'n_estimators': 42, 'max_leaf_nodes': 5623}           0.785417   \n",
       "4    {'n_estimators': 3, 'max_leaf_nodes': 10000}           0.710417   \n",
       "5    {'n_estimators': 23, 'max_leaf_nodes': 5623}           0.791667   \n",
       "6     {'n_estimators': 5, 'max_leaf_nodes': 4216}           0.700000   \n",
       "7     {'n_estimators': 100, 'max_leaf_nodes': 56}           0.804167   \n",
       "8       {'n_estimators': 4, 'max_leaf_nodes': 10}           0.606250   \n",
       "9     {'n_estimators': 5, 'max_leaf_nodes': 1000}           0.722917   \n",
       "10  {'n_estimators': 749, 'max_leaf_nodes': 1778}           0.800000   \n",
       "11   {'n_estimators': 31, 'max_leaf_nodes': 1778}           0.777083   \n",
       "12   {'n_estimators': 42, 'max_leaf_nodes': 1000}           0.766667   \n",
       "13    {'n_estimators': 237, 'max_leaf_nodes': 56}           0.787500   \n",
       "14   {'n_estimators': 17, 'max_leaf_nodes': 7498}           0.770833   \n",
       "15     {'n_estimators': 31, 'max_leaf_nodes': 23}           0.764583   \n",
       "16    {'n_estimators': 1, 'max_leaf_nodes': 2371}           0.706250   \n",
       "17    {'n_estimators': 7, 'max_leaf_nodes': 5623}           0.791667   \n",
       "18     {'n_estimators': 1, 'max_leaf_nodes': 237}           0.700000   \n",
       "19    {'n_estimators': 237, 'max_leaf_nodes': 10}           0.795833   \n",
       "20  {'n_estimators': 177, 'max_leaf_nodes': 5623}           0.812500   \n",
       "21    {'n_estimators': 177, 'max_leaf_nodes': 74}           0.806250   \n",
       "22    {'n_estimators': 133, 'max_leaf_nodes': 10}           0.772917   \n",
       "23  {'n_estimators': 749, 'max_leaf_nodes': 1000}           0.810417   \n",
       "24   {'n_estimators': 1000, 'max_leaf_nodes': 23}           0.797917   \n",
       "25    {'n_estimators': 42, 'max_leaf_nodes': 316}           0.785417   \n",
       "26  {'n_estimators': 316, 'max_leaf_nodes': 1333}           0.800000   \n",
       "27    {'n_estimators': 10, 'max_leaf_nodes': 749}           0.777083   \n",
       "28     {'n_estimators': 2, 'max_leaf_nodes': 133}           0.704167   \n",
       "29    {'n_estimators': 42, 'max_leaf_nodes': 237}           0.822917   \n",
       "30     {'n_estimators': 4, 'max_leaf_nodes': 133}           0.737500   \n",
       "31   {'n_estimators': 13, 'max_leaf_nodes': 3162}           0.743750   \n",
       "32   {'n_estimators': 100, 'max_leaf_nodes': 237}           0.808333   \n",
       "33     {'n_estimators': 17, 'max_leaf_nodes': 17}           0.733333   \n",
       "34      {'n_estimators': 2, 'max_leaf_nodes': 23}           0.654167   \n",
       "35   {'n_estimators': 237, 'max_leaf_nodes': 100}           0.804167   \n",
       "36   {'n_estimators': 316, 'max_leaf_nodes': 749}           0.800000   \n",
       "37      {'n_estimators': 3, 'max_leaf_nodes': 31}           0.710417   \n",
       "38   {'n_estimators': 17, 'max_leaf_nodes': 4216}           0.781250   \n",
       "39      {'n_estimators': 7, 'max_leaf_nodes': 31}           0.702083   \n",
       "40      {'n_estimators': 4, 'max_leaf_nodes': 17}           0.662500   \n",
       "41     {'n_estimators': 7, 'max_leaf_nodes': 749}           0.727083   \n",
       "42     {'n_estimators': 23, 'max_leaf_nodes': 31}           0.787500   \n",
       "43     {'n_estimators': 23, 'max_leaf_nodes': 17}           0.772917   \n",
       "44  {'n_estimators': 42, 'max_leaf_nodes': 10000}           0.800000   \n",
       "45    {'n_estimators': 13, 'max_leaf_nodes': 562}           0.772917   \n",
       "46   {'n_estimators': 13, 'max_leaf_nodes': 1000}           0.766667   \n",
       "47    {'n_estimators': 13, 'max_leaf_nodes': 237}           0.758333   \n",
       "48     {'n_estimators': 17, 'max_leaf_nodes': 10}           0.708333   \n",
       "49  {'n_estimators': 237, 'max_leaf_nodes': 1778}           0.797917   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.616667           0.656250           0.652083   \n",
       "1            0.764583           0.754167           0.764583   \n",
       "2            0.693750           0.670833           0.693750   \n",
       "3            0.766667           0.764583           0.733333   \n",
       "4            0.672917           0.654167           0.689583   \n",
       "5            0.775000           0.716667           0.760417   \n",
       "6            0.739583           0.704167           0.718750   \n",
       "7            0.791667           0.754167           0.747917   \n",
       "8            0.564583           0.600000           0.581250   \n",
       "9            0.695833           0.702083           0.722917   \n",
       "10           0.772917           0.789583           0.762500   \n",
       "11           0.758333           0.772917           0.733333   \n",
       "12           0.777083           0.762500           0.754167   \n",
       "13           0.795833           0.770833           0.762500   \n",
       "14           0.762500           0.754167           0.725000   \n",
       "15           0.777083           0.739583           0.752083   \n",
       "16           0.612500           0.614583           0.597917   \n",
       "17           0.722917           0.708333           0.706250   \n",
       "18           0.712500           0.635417           0.639583   \n",
       "19           0.766667           0.754167           0.747917   \n",
       "20           0.772917           0.783333           0.766667   \n",
       "21           0.795833           0.768750           0.752083   \n",
       "22           0.783333           0.729167           0.750000   \n",
       "23           0.766667           0.785417           0.762500   \n",
       "24           0.791667           0.764583           0.754167   \n",
       "25           0.793750           0.768750           0.760417   \n",
       "26           0.772917           0.791667           0.756250   \n",
       "27           0.768750           0.714583           0.716667   \n",
       "28           0.737500           0.639583           0.708333   \n",
       "29           0.779167           0.783333           0.770833   \n",
       "30           0.754167           0.697917           0.716667   \n",
       "31           0.756250           0.760417           0.752083   \n",
       "32           0.772917           0.779167           0.756250   \n",
       "33           0.731250           0.704167           0.697917   \n",
       "34           0.631250           0.631250           0.604167   \n",
       "35           0.785417           0.775000           0.754167   \n",
       "36           0.777083           0.785417           0.772917   \n",
       "37           0.631250           0.668750           0.612500   \n",
       "38           0.758333           0.747917           0.714583   \n",
       "39           0.750000           0.710417           0.689583   \n",
       "40           0.639583           0.633333           0.627083   \n",
       "41           0.731250           0.679167           0.722917   \n",
       "42           0.783333           0.756250           0.733333   \n",
       "43           0.770833           0.737500           0.716667   \n",
       "44           0.777083           0.770833           0.770833   \n",
       "45           0.745833           0.754167           0.722917   \n",
       "46           0.731250           0.741667           0.727083   \n",
       "47           0.752083           0.766667           0.750000   \n",
       "48           0.729167           0.687500           0.683333   \n",
       "49           0.775000           0.789583           0.760417   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.670833         0.662083        0.031716               45  \n",
       "1            0.789583         0.777083        0.021205               16  \n",
       "2            0.704167         0.698333        0.018884               41  \n",
       "3            0.779167         0.765833        0.017999               21  \n",
       "4            0.702083         0.685833        0.020259               43  \n",
       "5            0.783333         0.765417        0.026471               22  \n",
       "6            0.687500         0.710000        0.017844               39  \n",
       "7            0.802083         0.780000        0.024102               14  \n",
       "8            0.608333         0.592083        0.016739               50  \n",
       "9            0.733333         0.715417        0.014105               38  \n",
       "10           0.804167         0.785833        0.015888                4  \n",
       "11           0.766667         0.761667        0.015512               25  \n",
       "12           0.775000         0.767083        0.008375               19  \n",
       "13           0.808333         0.785000        0.016583                7  \n",
       "14           0.758333         0.754167        0.015590               29  \n",
       "15           0.793750         0.765417        0.018893               22  \n",
       "16           0.637500         0.633750        0.038401               49  \n",
       "17           0.768750         0.739583        0.034435               33  \n",
       "18           0.670833         0.671667        0.031030               44  \n",
       "19           0.793750         0.771667        0.019834               17  \n",
       "20           0.793750         0.785833        0.016213                6  \n",
       "21           0.806250         0.785833        0.021747                4  \n",
       "22           0.797917         0.766667        0.024403               20  \n",
       "23           0.797917         0.784583        0.018191                8  \n",
       "24           0.797917         0.781250        0.018305               13  \n",
       "25           0.789583         0.779583        0.012802               15  \n",
       "26           0.791667         0.782500        0.015844               11  \n",
       "27           0.768750         0.749167        0.027563               31  \n",
       "28           0.681250         0.694167        0.032633               42  \n",
       "29           0.802083         0.791667        0.018680                1  \n",
       "30           0.772917         0.735833        0.026530               34  \n",
       "31           0.756250         0.753750        0.005652               30  \n",
       "32           0.797917         0.782917        0.018418                9  \n",
       "33           0.764583         0.726250        0.023812               35  \n",
       "34           0.675000         0.639167        0.023914               48  \n",
       "35           0.814583         0.786667        0.021352                2  \n",
       "36           0.795833         0.786250        0.010425                3  \n",
       "37           0.600000         0.644583        0.040281               46  \n",
       "38           0.775000         0.755417        0.023592               28  \n",
       "39           0.737500         0.717917        0.022461               37  \n",
       "40           0.647917         0.642083        0.012318               47  \n",
       "41           0.735417         0.719167        0.020429               36  \n",
       "42           0.779167         0.767917        0.020395               18  \n",
       "43           0.779167         0.755417        0.024210               27  \n",
       "44           0.791667         0.782083        0.011756               12  \n",
       "45           0.785417         0.756250        0.021691               26  \n",
       "46           0.758333         0.745000        0.015287               32  \n",
       "47           0.795833         0.764583        0.016667               24  \n",
       "48           0.683333         0.698333        0.017989               40  \n",
       "49           0.791667         0.782917        0.013527               10  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_max_leaf_nodes</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.031916</td>\n",
       "      <td>0.005048</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>1</td>\n",
       "      <td>7498</td>\n",
       "      <td>{'n_estimators': 1, 'max_leaf_nodes': 7498}</td>\n",
       "      <td>0.714583</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.652083</td>\n",
       "      <td>0.670833</td>\n",
       "      <td>0.662083</td>\n",
       "      <td>0.031716</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.327523</td>\n",
       "      <td>0.014278</td>\n",
       "      <td>0.006776</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>23</td>\n",
       "      <td>133</td>\n",
       "      <td>{'n_estimators': 23, 'max_leaf_nodes': 133}</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.764583</td>\n",
       "      <td>0.754167</td>\n",
       "      <td>0.764583</td>\n",
       "      <td>0.789583</td>\n",
       "      <td>0.777083</td>\n",
       "      <td>0.021205</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.160560</td>\n",
       "      <td>0.036406</td>\n",
       "      <td>0.003591</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>5</td>\n",
       "      <td>1778</td>\n",
       "      <td>{'n_estimators': 5, 'max_leaf_nodes': 1778}</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.693750</td>\n",
       "      <td>0.670833</td>\n",
       "      <td>0.693750</td>\n",
       "      <td>0.704167</td>\n",
       "      <td>0.698333</td>\n",
       "      <td>0.018884</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.208300</td>\n",
       "      <td>0.101814</td>\n",
       "      <td>0.017175</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>42</td>\n",
       "      <td>5623</td>\n",
       "      <td>{'n_estimators': 42, 'max_leaf_nodes': 5623}</td>\n",
       "      <td>0.785417</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.764583</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.779167</td>\n",
       "      <td>0.765833</td>\n",
       "      <td>0.017999</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.084938</td>\n",
       "      <td>0.002992</td>\n",
       "      <td>0.002336</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>3</td>\n",
       "      <td>10000</td>\n",
       "      <td>{'n_estimators': 3, 'max_leaf_nodes': 10000}</td>\n",
       "      <td>0.710417</td>\n",
       "      <td>0.672917</td>\n",
       "      <td>0.654167</td>\n",
       "      <td>0.689583</td>\n",
       "      <td>0.702083</td>\n",
       "      <td>0.685833</td>\n",
       "      <td>0.020259</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.677978</td>\n",
       "      <td>0.045590</td>\n",
       "      <td>0.010774</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>23</td>\n",
       "      <td>5623</td>\n",
       "      <td>{'n_estimators': 23, 'max_leaf_nodes': 5623}</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.760417</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.765417</td>\n",
       "      <td>0.026471</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.137641</td>\n",
       "      <td>0.004227</td>\n",
       "      <td>0.003191</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>5</td>\n",
       "      <td>4216</td>\n",
       "      <td>{'n_estimators': 5, 'max_leaf_nodes': 4216}</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.739583</td>\n",
       "      <td>0.704167</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.017844</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.976967</td>\n",
       "      <td>0.020233</td>\n",
       "      <td>0.024458</td>\n",
       "      <td>0.002360</td>\n",
       "      <td>100</td>\n",
       "      <td>56</td>\n",
       "      <td>{'n_estimators': 100, 'max_leaf_nodes': 56}</td>\n",
       "      <td>0.804167</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.754167</td>\n",
       "      <td>0.747917</td>\n",
       "      <td>0.802083</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.024102</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.016361</td>\n",
       "      <td>0.001503</td>\n",
       "      <td>0.002189</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_estimators': 4, 'max_leaf_nodes': 10}</td>\n",
       "      <td>0.606250</td>\n",
       "      <td>0.564583</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.581250</td>\n",
       "      <td>0.608333</td>\n",
       "      <td>0.592083</td>\n",
       "      <td>0.016739</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.136361</td>\n",
       "      <td>0.005858</td>\n",
       "      <td>0.003917</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'n_estimators': 5, 'max_leaf_nodes': 1000}</td>\n",
       "      <td>0.722917</td>\n",
       "      <td>0.695833</td>\n",
       "      <td>0.702083</td>\n",
       "      <td>0.722917</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.715417</td>\n",
       "      <td>0.014105</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20.938192</td>\n",
       "      <td>0.271128</td>\n",
       "      <td>0.288577</td>\n",
       "      <td>0.009172</td>\n",
       "      <td>749</td>\n",
       "      <td>1778</td>\n",
       "      <td>{'n_estimators': 749, 'max_leaf_nodes': 1778}</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.772917</td>\n",
       "      <td>0.789583</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.804167</td>\n",
       "      <td>0.785833</td>\n",
       "      <td>0.015888</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.843431</td>\n",
       "      <td>0.099172</td>\n",
       "      <td>0.015463</td>\n",
       "      <td>0.005354</td>\n",
       "      <td>31</td>\n",
       "      <td>1778</td>\n",
       "      <td>{'n_estimators': 31, 'max_leaf_nodes': 1778}</td>\n",
       "      <td>0.777083</td>\n",
       "      <td>0.758333</td>\n",
       "      <td>0.772917</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.761667</td>\n",
       "      <td>0.015512</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.276864</td>\n",
       "      <td>0.159248</td>\n",
       "      <td>0.019053</td>\n",
       "      <td>0.005946</td>\n",
       "      <td>42</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'n_estimators': 42, 'max_leaf_nodes': 1000}</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.777083</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.754167</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.767083</td>\n",
       "      <td>0.008375</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.335064</td>\n",
       "      <td>0.082362</td>\n",
       "      <td>0.058042</td>\n",
       "      <td>0.005471</td>\n",
       "      <td>237</td>\n",
       "      <td>56</td>\n",
       "      <td>{'n_estimators': 237, 'max_leaf_nodes': 56}</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.795833</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>0.016583</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.477716</td>\n",
       "      <td>0.008504</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>17</td>\n",
       "      <td>7498</td>\n",
       "      <td>{'n_estimators': 17, 'max_leaf_nodes': 7498}</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.754167</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.758333</td>\n",
       "      <td>0.754167</td>\n",
       "      <td>0.015590</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.182313</td>\n",
       "      <td>0.006771</td>\n",
       "      <td>0.007579</td>\n",
       "      <td>0.001006</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>{'n_estimators': 31, 'max_leaf_nodes': 23}</td>\n",
       "      <td>0.764583</td>\n",
       "      <td>0.777083</td>\n",
       "      <td>0.739583</td>\n",
       "      <td>0.752083</td>\n",
       "      <td>0.793750</td>\n",
       "      <td>0.765417</td>\n",
       "      <td>0.018893</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.029322</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>1</td>\n",
       "      <td>2371</td>\n",
       "      <td>{'n_estimators': 1, 'max_leaf_nodes': 2371}</td>\n",
       "      <td>0.706250</td>\n",
       "      <td>0.612500</td>\n",
       "      <td>0.614583</td>\n",
       "      <td>0.597917</td>\n",
       "      <td>0.637500</td>\n",
       "      <td>0.633750</td>\n",
       "      <td>0.038401</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.191476</td>\n",
       "      <td>0.003415</td>\n",
       "      <td>0.004613</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>7</td>\n",
       "      <td>5623</td>\n",
       "      <td>{'n_estimators': 7, 'max_leaf_nodes': 5623}</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.722917</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.706250</td>\n",
       "      <td>0.768750</td>\n",
       "      <td>0.739583</td>\n",
       "      <td>0.034435</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.020144</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>1</td>\n",
       "      <td>237</td>\n",
       "      <td>{'n_estimators': 1, 'max_leaf_nodes': 237}</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.712500</td>\n",
       "      <td>0.635417</td>\n",
       "      <td>0.639583</td>\n",
       "      <td>0.670833</td>\n",
       "      <td>0.671667</td>\n",
       "      <td>0.031030</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.879840</td>\n",
       "      <td>0.016104</td>\n",
       "      <td>0.042679</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>237</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_estimators': 237, 'max_leaf_nodes': 10}</td>\n",
       "      <td>0.795833</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.754167</td>\n",
       "      <td>0.747917</td>\n",
       "      <td>0.793750</td>\n",
       "      <td>0.771667</td>\n",
       "      <td>0.019834</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.069089</td>\n",
       "      <td>0.265948</td>\n",
       "      <td>0.073004</td>\n",
       "      <td>0.009127</td>\n",
       "      <td>177</td>\n",
       "      <td>5623</td>\n",
       "      <td>{'n_estimators': 177, 'max_leaf_nodes': 5623}</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.772917</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.793750</td>\n",
       "      <td>0.785833</td>\n",
       "      <td>0.016213</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.988981</td>\n",
       "      <td>0.026115</td>\n",
       "      <td>0.043850</td>\n",
       "      <td>0.001256</td>\n",
       "      <td>177</td>\n",
       "      <td>74</td>\n",
       "      <td>{'n_estimators': 177, 'max_leaf_nodes': 74}</td>\n",
       "      <td>0.806250</td>\n",
       "      <td>0.795833</td>\n",
       "      <td>0.768750</td>\n",
       "      <td>0.752083</td>\n",
       "      <td>0.806250</td>\n",
       "      <td>0.785833</td>\n",
       "      <td>0.021747</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.498219</td>\n",
       "      <td>0.011052</td>\n",
       "      <td>0.027131</td>\n",
       "      <td>0.003741</td>\n",
       "      <td>133</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_estimators': 133, 'max_leaf_nodes': 10}</td>\n",
       "      <td>0.772917</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.797917</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.024403</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>20.954413</td>\n",
       "      <td>0.294147</td>\n",
       "      <td>0.308625</td>\n",
       "      <td>0.003893</td>\n",
       "      <td>749</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'n_estimators': 749, 'max_leaf_nodes': 1000}</td>\n",
       "      <td>0.810417</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.785417</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.797917</td>\n",
       "      <td>0.784583</td>\n",
       "      <td>0.018191</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5.994192</td>\n",
       "      <td>0.165842</td>\n",
       "      <td>0.192905</td>\n",
       "      <td>0.004233</td>\n",
       "      <td>1000</td>\n",
       "      <td>23</td>\n",
       "      <td>{'n_estimators': 1000, 'max_leaf_nodes': 23}</td>\n",
       "      <td>0.797917</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.764583</td>\n",
       "      <td>0.754167</td>\n",
       "      <td>0.797917</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.018305</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.839131</td>\n",
       "      <td>0.015968</td>\n",
       "      <td>0.015724</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>42</td>\n",
       "      <td>316</td>\n",
       "      <td>{'n_estimators': 42, 'max_leaf_nodes': 316}</td>\n",
       "      <td>0.785417</td>\n",
       "      <td>0.793750</td>\n",
       "      <td>0.768750</td>\n",
       "      <td>0.760417</td>\n",
       "      <td>0.789583</td>\n",
       "      <td>0.779583</td>\n",
       "      <td>0.012802</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8.942290</td>\n",
       "      <td>0.225588</td>\n",
       "      <td>0.125470</td>\n",
       "      <td>0.003235</td>\n",
       "      <td>316</td>\n",
       "      <td>1333</td>\n",
       "      <td>{'n_estimators': 316, 'max_leaf_nodes': 1333}</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.772917</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.756250</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.782500</td>\n",
       "      <td>0.015844</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.276402</td>\n",
       "      <td>0.005885</td>\n",
       "      <td>0.005580</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>10</td>\n",
       "      <td>749</td>\n",
       "      <td>{'n_estimators': 10, 'max_leaf_nodes': 749}</td>\n",
       "      <td>0.777083</td>\n",
       "      <td>0.768750</td>\n",
       "      <td>0.714583</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.768750</td>\n",
       "      <td>0.749167</td>\n",
       "      <td>0.027563</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.030757</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>0.001994</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>2</td>\n",
       "      <td>133</td>\n",
       "      <td>{'n_estimators': 2, 'max_leaf_nodes': 133}</td>\n",
       "      <td>0.704167</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>0.639583</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.681250</td>\n",
       "      <td>0.694167</td>\n",
       "      <td>0.032633</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.751719</td>\n",
       "      <td>0.014813</td>\n",
       "      <td>0.013963</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>42</td>\n",
       "      <td>237</td>\n",
       "      <td>{'n_estimators': 42, 'max_leaf_nodes': 237}</td>\n",
       "      <td>0.822917</td>\n",
       "      <td>0.779167</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.802083</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.018680</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.061533</td>\n",
       "      <td>0.003848</td>\n",
       "      <td>0.001997</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>4</td>\n",
       "      <td>133</td>\n",
       "      <td>{'n_estimators': 4, 'max_leaf_nodes': 133}</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>0.754167</td>\n",
       "      <td>0.697917</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.772917</td>\n",
       "      <td>0.735833</td>\n",
       "      <td>0.026530</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.368219</td>\n",
       "      <td>0.010202</td>\n",
       "      <td>0.005993</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>13</td>\n",
       "      <td>3162</td>\n",
       "      <td>{'n_estimators': 13, 'max_leaf_nodes': 3162}</td>\n",
       "      <td>0.743750</td>\n",
       "      <td>0.756250</td>\n",
       "      <td>0.760417</td>\n",
       "      <td>0.752083</td>\n",
       "      <td>0.756250</td>\n",
       "      <td>0.753750</td>\n",
       "      <td>0.005652</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.794066</td>\n",
       "      <td>0.057227</td>\n",
       "      <td>0.031669</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>100</td>\n",
       "      <td>237</td>\n",
       "      <td>{'n_estimators': 100, 'max_leaf_nodes': 237}</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.772917</td>\n",
       "      <td>0.779167</td>\n",
       "      <td>0.756250</td>\n",
       "      <td>0.797917</td>\n",
       "      <td>0.782917</td>\n",
       "      <td>0.018418</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.083705</td>\n",
       "      <td>0.002698</td>\n",
       "      <td>0.004320</td>\n",
       "      <td>0.000662</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>{'n_estimators': 17, 'max_leaf_nodes': 17}</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.731250</td>\n",
       "      <td>0.704167</td>\n",
       "      <td>0.697917</td>\n",
       "      <td>0.764583</td>\n",
       "      <td>0.726250</td>\n",
       "      <td>0.023812</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.013677</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.001795</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>{'n_estimators': 2, 'max_leaf_nodes': 23}</td>\n",
       "      <td>0.654167</td>\n",
       "      <td>0.631250</td>\n",
       "      <td>0.631250</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.639167</td>\n",
       "      <td>0.023914</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3.084326</td>\n",
       "      <td>0.144991</td>\n",
       "      <td>0.061212</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>237</td>\n",
       "      <td>100</td>\n",
       "      <td>{'n_estimators': 237, 'max_leaf_nodes': 100}</td>\n",
       "      <td>0.804167</td>\n",
       "      <td>0.785417</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.754167</td>\n",
       "      <td>0.814583</td>\n",
       "      <td>0.786667</td>\n",
       "      <td>0.021352</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>8.805029</td>\n",
       "      <td>0.152405</td>\n",
       "      <td>0.124273</td>\n",
       "      <td>0.004244</td>\n",
       "      <td>316</td>\n",
       "      <td>749</td>\n",
       "      <td>{'n_estimators': 316, 'max_leaf_nodes': 749}</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.777083</td>\n",
       "      <td>0.785417</td>\n",
       "      <td>0.772917</td>\n",
       "      <td>0.795833</td>\n",
       "      <td>0.786250</td>\n",
       "      <td>0.010425</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.023857</td>\n",
       "      <td>0.001993</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>{'n_estimators': 3, 'max_leaf_nodes': 31}</td>\n",
       "      <td>0.710417</td>\n",
       "      <td>0.631250</td>\n",
       "      <td>0.668750</td>\n",
       "      <td>0.612500</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.644583</td>\n",
       "      <td>0.040281</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.482233</td>\n",
       "      <td>0.010504</td>\n",
       "      <td>0.007501</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>17</td>\n",
       "      <td>4216</td>\n",
       "      <td>{'n_estimators': 17, 'max_leaf_nodes': 4216}</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.758333</td>\n",
       "      <td>0.747917</td>\n",
       "      <td>0.714583</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.755417</td>\n",
       "      <td>0.023592</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.051673</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.002979</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>{'n_estimators': 7, 'max_leaf_nodes': 31}</td>\n",
       "      <td>0.702083</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.710417</td>\n",
       "      <td>0.689583</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>0.717917</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.023531</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>0.001797</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>{'n_estimators': 4, 'max_leaf_nodes': 17}</td>\n",
       "      <td>0.662500</td>\n",
       "      <td>0.639583</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.627083</td>\n",
       "      <td>0.647917</td>\n",
       "      <td>0.642083</td>\n",
       "      <td>0.012318</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.192662</td>\n",
       "      <td>0.005009</td>\n",
       "      <td>0.003797</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>7</td>\n",
       "      <td>749</td>\n",
       "      <td>{'n_estimators': 7, 'max_leaf_nodes': 749}</td>\n",
       "      <td>0.727083</td>\n",
       "      <td>0.731250</td>\n",
       "      <td>0.679167</td>\n",
       "      <td>0.722917</td>\n",
       "      <td>0.735417</td>\n",
       "      <td>0.719167</td>\n",
       "      <td>0.020429</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.163950</td>\n",
       "      <td>0.005904</td>\n",
       "      <td>0.005927</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>23</td>\n",
       "      <td>31</td>\n",
       "      <td>{'n_estimators': 23, 'max_leaf_nodes': 31}</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.756250</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.779167</td>\n",
       "      <td>0.767917</td>\n",
       "      <td>0.020395</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.115152</td>\n",
       "      <td>0.003488</td>\n",
       "      <td>0.006267</td>\n",
       "      <td>0.001592</td>\n",
       "      <td>23</td>\n",
       "      <td>17</td>\n",
       "      <td>{'n_estimators': 23, 'max_leaf_nodes': 17}</td>\n",
       "      <td>0.772917</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.779167</td>\n",
       "      <td>0.755417</td>\n",
       "      <td>0.024210</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.212058</td>\n",
       "      <td>0.062743</td>\n",
       "      <td>0.017644</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>42</td>\n",
       "      <td>10000</td>\n",
       "      <td>{'n_estimators': 42, 'max_leaf_nodes': 10000}</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.777083</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.782083</td>\n",
       "      <td>0.011756</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.324026</td>\n",
       "      <td>0.006603</td>\n",
       "      <td>0.006170</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>13</td>\n",
       "      <td>562</td>\n",
       "      <td>{'n_estimators': 13, 'max_leaf_nodes': 562}</td>\n",
       "      <td>0.772917</td>\n",
       "      <td>0.745833</td>\n",
       "      <td>0.754167</td>\n",
       "      <td>0.722917</td>\n",
       "      <td>0.785417</td>\n",
       "      <td>0.756250</td>\n",
       "      <td>0.021691</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.361921</td>\n",
       "      <td>0.005748</td>\n",
       "      <td>0.006730</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>13</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'n_estimators': 13, 'max_leaf_nodes': 1000}</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.731250</td>\n",
       "      <td>0.741667</td>\n",
       "      <td>0.727083</td>\n",
       "      <td>0.758333</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.015287</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.228941</td>\n",
       "      <td>0.009506</td>\n",
       "      <td>0.004985</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>13</td>\n",
       "      <td>237</td>\n",
       "      <td>{'n_estimators': 13, 'max_leaf_nodes': 237}</td>\n",
       "      <td>0.758333</td>\n",
       "      <td>0.752083</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.795833</td>\n",
       "      <td>0.764583</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.065743</td>\n",
       "      <td>0.003154</td>\n",
       "      <td>0.005444</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_estimators': 17, 'max_leaf_nodes': 10}</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.698333</td>\n",
       "      <td>0.017989</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>6.819818</td>\n",
       "      <td>0.291031</td>\n",
       "      <td>0.098564</td>\n",
       "      <td>0.013243</td>\n",
       "      <td>237</td>\n",
       "      <td>1778</td>\n",
       "      <td>{'n_estimators': 237, 'max_leaf_nodes': 1778}</td>\n",
       "      <td>0.797917</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.789583</td>\n",
       "      <td>0.760417</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.782917</td>\n",
       "      <td>0.013527</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "kf = KFold(shuffle=True)\r\n",
    "model = RandomForestClassifier(max_leaf_nodes=600)\r\n",
    "\r\n",
    "k_train_losses = list()\r\n",
    "k_train_scores = list()\r\n",
    "k_train_aucs = list()\r\n",
    "\r\n",
    "k_val_losses = list()\r\n",
    "k_val_scores = list()\r\n",
    "k_val_aucs = list()\r\n",
    "\r\n",
    "for train_index, val_index in kf.split(X):\r\n",
    "    x_train, x_val = X[train_index], X[val_index]\r\n",
    "    y_train, y_val = y[train_index], y[val_index]\r\n",
    "    model.fit(x_train, y_train)\r\n",
    "    \r\n",
    "    train_probas = model.predict_proba(x_train)[:,1]\r\n",
    "    val_probas = model.predict_proba(x_val)[:,1]\r\n",
    "\r\n",
    "    k_train_losses.append(log_loss(y_train, train_probas))\r\n",
    "    k_train_scores.append(model.score(x_train, y_train))\r\n",
    "    k_train_aucs.append(roc_auc_score(y_train, train_probas))\r\n",
    "    \r\n",
    "    k_val_losses.append(log_loss(y_val, val_probas))\r\n",
    "    k_val_scores.append(model.score(x_val, y_val))\r\n",
    "    k_val_aucs.append(roc_auc_score(y_val, val_probas))\r\n",
    "    \r\n",
    "    print('   TRAIN -- score: {:5f}   auc: {:5f}   loss: {:5f}'.format(\r\n",
    "        k_train_scores[-1],\r\n",
    "        k_train_aucs[-1],\r\n",
    "        k_train_losses[-1]))\r\n",
    "\r\n",
    "    print('   VALID -- score: {:5f}   auc: {:5f}   loss: {:5f}'.format(\r\n",
    "        k_val_scores[-1],\r\n",
    "        k_val_aucs[-1],\r\n",
    "        k_val_losses[-1]))\r\n",
    "\r\n",
    "print('TRAIN -- score: {:5f}   auc: {:5f}   loss: {:5f}'.format(\r\n",
    "    np.mean(k_train_scores),\r\n",
    "    np.mean(k_train_aucs),\r\n",
    "    np.mean(k_train_losses)))\r\n",
    "\r\n",
    "print('VALID -- score: {:5f}   auc: {:5f}   loss: {:5f}'.format(\r\n",
    "    np.mean(k_val_scores),\r\n",
    "    np.mean(k_val_aucs),\r\n",
    "    np.mean(k_val_losses)))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   TRAIN -- score: 0.997917   auc: 0.999822   loss: 0.159861\n",
      "   VALID -- score: 0.804167   auc: 0.874826   loss: 0.476400\n",
      "   TRAIN -- score: 0.997917   auc: 0.999973   loss: 0.153043\n",
      "   VALID -- score: 0.797917   auc: 0.871830   loss: 0.465388\n",
      "   TRAIN -- score: 0.998437   auc: 0.999980   loss: 0.151009\n",
      "   VALID -- score: 0.783333   auc: 0.856992   loss: 0.487946\n",
      "   TRAIN -- score: 0.996354   auc: 0.999937   loss: 0.156958\n",
      "   VALID -- score: 0.808333   auc: 0.890050   loss: 0.444357\n",
      "   TRAIN -- score: 0.997917   auc: 0.999952   loss: 0.154605\n",
      "   VALID -- score: 0.797917   auc: 0.874182   loss: 0.469402\n",
      "TRAIN -- score: 0.997708   auc: 0.999933   loss: 0.155095\n",
      "VALID -- score: 0.798333   auc: 0.873576   loss: 0.468699\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "np.std([0.804167, 0.797917, 0.783333, 0.808333, 0.797917])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.008477991120542656"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('ml_env': conda)"
  },
  "interpreter": {
   "hash": "5488065bb2d6ca6a9de5ec6734160292494259bd2c5abc9e8432ae79ff9e5079"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}